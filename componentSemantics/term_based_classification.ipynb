{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiset\n",
    "\n",
    "from utils import load_embeddings, load_projects, load_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "projects, imapping = load_projects('.', 'java-projects - java-projects (1).csv')\n",
    "labels = {k:v for k,v in zip(projects['names'], projects['labels_id'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = \"resources/java/stopwords.txt\"\n",
    "stopwords = load_stopwords(path)\n",
    "path = \"resources/en/stopwords.txt\"\n",
    "stopwords.update(load_stopwords(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "terms_path = '../data/embeddings/terms-count/'\n",
    "mapping = {v:k for k,v in imapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import  Word\n",
    "\n",
    "category_terms_count = defaultdict(lambda: Counter())\n",
    "category_terms_occ = defaultdict(lambda: Counter())\n",
    "text = []\n",
    "lab = []\n",
    "for project in labels:\n",
    "    category = mapping[labels[project]]\n",
    "    try:\n",
    "        terms_count = load_embeddings(os.path.join(terms_path, f\"{project}.vec\"))\n",
    "        terms = []\n",
    "        for x, y in terms_count.items():\n",
    "            lemma = Word(x).lemmatize()\n",
    "            if lemma not in stopwords and len(x) > 1 and x not in stopwords:\n",
    "                tokens = [lemma] * int(y[0])\n",
    "                terms.extend(tokens)\n",
    "        #terms = [[Word(x).lemmatize()] * int(y[0]) for x, y in terms_count.items() if Word(x).lemmatize() not in stopwords and len(x) > 1]\n",
    "        text.append(\" \".join(terms))\n",
    "        lab.append(category)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.DataFrame({'text': text, 'label': lab})\n",
    "df = df[~df['label'].isin(['NA', 'Miscellaneous'])]\n",
    "\n",
    "y = df['label'].to_frame()\n",
    "X = df['text']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,stratify=y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "category_terms_occ = defaultdict(lambda: multiset.Multiset())\n",
    "\n",
    "for label in set(y['label'].tolist()):\n",
    "    corpus = X_train[y_train['label']==label].tolist()\n",
    "\n",
    "    for sample in corpus:\n",
    "        terms = sample.split(\" \")\n",
    "        category_terms_occ[label].update(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all = 0\n",
    "true = 0\n",
    "rsults = {}\n",
    "tl = []\n",
    "predl = []\n",
    "for label in set(y['label'].tolist()):\n",
    "    corpus = X_test[y_test['label']==label].tolist()\n",
    "\n",
    "    for i, sample in enumerate(corpus):\n",
    "        terms = multiset.Multiset(sample.split(\" \"))\n",
    "        scores = {}\n",
    "        for l in y['label'].tolist():\n",
    "            cat_terms = category_terms_occ[l]\n",
    "            scores[l] = len(terms.intersection(cat_terms)) / len(terms.union(cat_terms))\n",
    "\n",
    "        all += 1\n",
    "        gt = y_test['label'].tolist()[i]\n",
    "        pred = max(scores.items(), key=operator.itemgetter(1))[0]\n",
    "        tl.append(gt)\n",
    "        predl.append(pred)\n",
    "        if gt == pred:\n",
    "            true += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(true/all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confusion_matrix(tl, predl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "X_t_mat = vectorizer.transform(X_train)\n",
    "X_s_mat = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neigh.fit(X_t_mat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "predictions = neigh.predict(X_s_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    n_jobs=1,\n",
    "    memory_limit=140072,\n",
    "    per_run_time_limit=30,\n",
    "    tmp_folder='/tmp/autosklearn_parallel_1_example_tmp',\n",
    "    output_folder='/tmp/autosklearn_parallel_1_example_out',\n",
    "    ensemble_size=1,\n",
    "    include_preprocessors=[\"no_preprocessing\"],\n",
    "    exclude_estimators=['liblinear_svc', 'libsvm_svc', 'mlp']\n",
    ")\n",
    "automl.fit(X_t_mat, y_train)\n",
    "y_hat = automl.predict(X_s_mat)\n",
    "print(\"Accuracy score\", classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(\n",
    "    generations=5,\n",
    "    population_size=50,\n",
    "    verbosity=2, random_state=42)\n",
    "tpot.fit(X_t_mat.todense(), y_train)\n",
    "print(tpot.score(X_s_mat.todense(), y_test))\n",
    "tpot.export('tpot_terms_pipeline.py')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}