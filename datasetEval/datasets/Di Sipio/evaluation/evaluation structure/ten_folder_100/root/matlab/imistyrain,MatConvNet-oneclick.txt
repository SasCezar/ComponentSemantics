b"# MatConvNet tutorial\xef\xbc\x9aTrain your own data\n##  \xe7\x94\xa8[MatConvNet](https://github.com/vlfeat/matconvnet)\xe8\xae\xad\xe7\xbb\x83\xe8\x87\xaa\xe5\xb7\xb1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n\n## \xe5\xae\x89\xe8\xa3\x85\xe5\x92\x8c\xe7\xbc\x96\xe8\xaf\x91MatConvNet(Build the [library](https://github.com/vlfeat/matconvnet) with CUDA)\n\n\tgit clone https://github.com/vlfeat/matconvnet\n\tcd matconvnet\n\t%create a new file called compileGPU.m and save its contents as:\n\taddpath matlab\n\tvl_compilenn('enableGpu', true, ...\n               'cudaRoot', 'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0', ...\n               'cudaMethod', 'nvcc');%,...\n\t%                'enableCudnn', 'true',...\n\t%                'cudnnRoot','E:\\MachineLearning\\DeepLearning\\CuDNN\\CUDNNv4') ;\n\t%\n\n\t%then setup the mex environment\n\t%please select VS2015 or greater\n\tmex -setup c\n\tmex -setup cpp\n\t%finally compile it\n\tcompileGPU\n\n## \xe5\x87\x86\xe5\xa4\x87\xe6\x95\xb0\xe6\x8d\xaePrepare data\n\n\xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe4\xbb\x8eEasyPR\xe8\x8e\xb7\xe5\x8f\x96\xe4\xba\x86\xe8\xbd\xa6\xe7\x89\x8c\xe6\x95\xb0\xe6\x8d\xae(\xe8\xa7\xa3\xe5\x8e\x8b[data.zip](data.zip)\xe5\x8d\xb3\xe5\x8f\xaf),0-9\xe5\x85\xb110\xe7\xb1\xbb\xe5\xad\x97\xe7\xac\xa6,\xe6\xaf\x8f\xe7\xb1\xbb\xe5\xad\x97\xe7\xac\xa6\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x90\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b,\xe5\xa6\x82\xe4\xb8\x8b\xe5\x9b\xbe\xe6\x89\x80\xe7\xa4\xba\xef\xbc\x9a\n\n![](https://i.imgur.com/j3zJ0YL.jpg)\n\n\xe4\xbb\xa3\xe7\xa0\x81\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xe4\xbd\x8d\xe4\xba\x8ecnn_plate_setup_data.m\xef\xbc\x8c\xe8\xaf\xb7\xe8\x87\xaa\xe8\xa1\x8c\xe8\xb0\x83\xe8\x8a\x82\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa4\xa7\xe5\xb0\x8f\n\n\tinputSize =[20,20,1];\n\n\xe6\x95\xb0\xe6\x8d\xae\xe5\xad\x98\xe6\x94\xbe\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe5\x9c\xa8startup.m\n\n\tdatadir='data';\n\n## \xe7\xbc\x96\xe5\x86\x99\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84Setup the net structure\n\n\xe5\x8f\x82\xe8\x80\x83cnn_plate_init.m\xe7\xbc\x96\xe5\x86\x99\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe6\x9e\x84\xe5\xbb\xba\xe4\xba\x863\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x92\x8c\xe6\xb1\xa0\xe5\x8c\x96\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xbaReLU.\n\n\tf=1/100 ;\n\tnet.layers = {};\n\tnet.layers{end+1} = struct('type', 'conv', ...\n\t                           'weights', {{f*randn(3,3,1,20, 'single'), zeros(1, 20, 'single')}}, ...\n\t                           'stride', 1, ...\n\t                           'pad', 0) ;\n\tnet.layers{end+1} = struct('type', 'pool', ...\n\t                           'method', 'max', ...\n\t                           'pool', [2 2], ...\n\t                           'stride', 2, ...\n\t                           'pad', 0) ;\n\tnet.layers{end+1} = struct('type', 'relu') ;\n\tnet.layers{end+1} = struct('type', 'conv', ...\n\t                           'weights', {{f*randn(3,3,20,100, 'single'),zeros(1,100,'single')}}, ...\n\t                           'stride', 1, ...\n\t                           'pad', 0) ;\n\tnet.layers{end+1} = struct('type', 'pool', ...\n\t                           'method', 'max', ...\n\t                           'pool', [2 2], ...\n\t                           'stride', 2, ...\n\t                           'pad', 0) ;\n\tnet.layers{end+1} = struct('type', 'relu') ;\n\tnet.layers{end+1} = struct('type', 'conv', ...\n\t   'weights', {{f*randn(3,3,100,65, 'single'),zeros(1,65,'single')}}, ...\n\t   'stride', 1, ...\n\t   'pad', 0) ;\n\tnet.layers{end+1} = struct('type', 'softmaxloss') ;\n\t\n\t% Meta parameters\n\tnet.meta.inputSize = [20 20 1] ;\n\tnet.meta.trainOpts.learningRate = logspace(-3, -5, 100);\n\tnet.meta.trainOpts.numEpochs = 50 ;\n\tnet.meta.trainOpts.batchSize = 1000 ;\n\t\n\t% Fill in defaul values\n\tnet = vl_simplenn_tidy(net) ;\n\n## \xe8\xae\xad\xe7\xbb\x83Train\n\n\xe8\xbf\x90\xe8\xa1\x8ccnn_plate.m\xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c,\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe7\x9a\x84\xe6\x9b\xb2\xe7\xba\xbf\xe5\xa6\x82\xe4\xb8\x8b\xe5\x9b\xbe\xe6\x89\x80\xe7\xa4\xba,\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x87\xba\xe5\xbe\x88\xe5\xbf\xab\xe5\xb0\xb1\xe5\x88\xb0\xe8\xbe\xbe99%\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87.\n\n![](https://i.imgur.com/4MFOZY8.jpg)\n\n## \xe6\xb5\x8b\xe8\xaf\x95Demo\n\ndemo.m\xe5\xb1\x95\xe7\xa4\xba\xe4\xba\x86\xe5\xa6\x82\xe4\xbd\x95\xe4\xbd\xbf\xe7\x94\xa8\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n\n![](https://i.imgur.com/iaDjqV1.jpg)\n\n\nNote:\xe8\xae\xb0\xe5\xbe\x97\xe4\xbf\xae\xe6\x94\xb9netpath\xe4\xb8\xba\xe8\x87\xaa\xe5\xb7\xb1\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x93\x9f.\n\n## \xe5\x8f\x82\xe8\x80\x83Reference\n\n[caffe\xe4\xb8\x80\xe9\x94\xae\xe5\xbc\x8f\xe9\x9b\x86\xe6\x88\x90\xe5\xbc\x80\xe5\x8f\x91\xe7\x8e\xaf\xe5\xa2\x83](https://github.com/imistyrain/caffe-oneclick)\n\n[mxnet\xe8\xae\xad\xe7\xbb\x83\xe8\x87\xaa\xe5\xb7\xb1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae](https://github.com/imistyrain/mxnet-oneclick)"