b"# Dual-Path Convolutional Image-Text Embedding\n\n[[Paper]](https://arxiv.org/abs/1711.05535) [[Slide]](http://zdzheng.xyz/ZhedongZheng_CA_Talk_DualPath.pdf)\n\nThis repository contains the code for our paper [Dual-Path Convolutional Image-Text Embedding](https://arxiv.org/abs/1711.05535). Thank you for your kindly attention. \n\n![](http://zdzheng.xyz/images/fulls/ConvVSE.jpg)\n\n![](https://github.com/layumi/Image-Text-Embedding/blob/master/CUHK-show.jpg)\n\n**What's New**: We updated the paper to the second version, adding more illustration about the mechanism of the proposed instance loss.\n\n# Install Matconvnet\nI have included my Matconvnet in this repo, so you do not need to download it again.You just need to uncomment and modify some lines in gpu_compile.m and run it in Matlab. Try it~ (The code does not support cudnn 6.0. You may just turn off the Enablecudnn or try cudnn5.1)\n\nIf you fail in compilation, you may refer to http://www.vlfeat.org/matconvnet/install/\n\n# Prepocess Datasets\n1. Extract wrod2vec weights. Follow the instruction in `./word2vector_matlab`;\n\n2. Prepocess the dataset. Follow the instruction in `./dataset`. You can choose one dataset to run.\nThree datasets need different prepocessing. I write the instruction for [Flickr30k](https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/Flickr30k-prepare), [MSCOCO](https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/MSCOCO-prepare) and [CUHK-PEDES](https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/CUHK-PEDES-prepare).\n\n3. Download the model pre-trained on ImageNet. And put the model into './data'.\n```\n(bash) wget http://www.vlfeat.org/matconvnet/models/imagenet-resnet-50-dag.mat\n```\nAlternatively, you may try [VGG16](http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-16.mat) or [VGG19](http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat). \n\nYou may have a different split with me. (Sorry, this is my fault. I used a random split.) Just for a backup, this is the [dictionary archive](https://drive.google.com/open?id=1Yp6B5GKhgQTD9bsmvmVkvxt-SnmHHjVA) used in the paper.\n\n# Trained Model\nYou may download the three trained models from [GoogleDrive](https://drive.google.com/open?id=1QxIdJd3oQIJSVVlAxaIZquOoLQahMrWH).\n\n# Train\n* For Flickr30k, run `train_flickr_word2_1_pool.m` for **Stage I** training.\n\nRun `train_flickr_word_Rankloss_shift_hard` for **Stage II** training.\n\n* For\xc2\xa0MSCOCO, run `train_coco_word2_1_pool.m` for **Stage I** training.\n\nRun `train_coco_Rankloss_shift_hard.m` for **Stage II** training.\n\n* For CUHK-PEDES, run `train_cuhk_word2_1_pool.m` for **Stage I** training.\n\nRun `train_cuhk_word_Rankloss_shift` for **Stage II** training.\n\n# Test\nSelect one model and have fun!\n\n* For Flickr30k, run `test/extract_pic_feature_word2_plus_52.m` and to extract the feature from image and text. Note that you need to change the model path in the code. \n\n* For MSCOCO, run `test_coco/extract_pic_feature_word2_plus.m` and to extract the feature from image and text. Note that you need to change the model path in the code. \n\n* For CUHK-PEDES, run `test_cuhk/extract_pic_feature_word2_plus_52.m` and to extract the feature from image and text. Note that you need to change the model path in the code. \n\n\n### CheckList\n- [x] Get word2vec weight\n\n- [x] Data Preparation (Flickr30k)\n- [x] Train on Flickr30k\n- [x] Test on Flickr30k\n\n- [x] Data Preparation (MSCOCO)\n- [x] Train on MSCOCO\n- [x] Test on MSCOCO\n\n- [x] Data Preparation (CUHK-PEDES)\n- [x] Train on CUHK-PEDES\n- [x] Test on CUHK-PEDES\n\n- [ ] Run the code on another machine \n"