b'_Update: see [last paragraph](#some-updates) for some important status update._\n\n# py-swirld\n\nJust fooling around the _Swirlds_ byzantine consensus algorithm by Leemon Baird\n([whitepaper](https://www.swirlds.com/downloads/SWIRLDS-TR-2016-01.pdf)\navailable) in python. _Swirlds_ is an algorithm constructing a strongly\nconsistent and partition tolerant, peer-to-peer append-only log.\n\nIt seems to work as intended to me but don\'t take it for granted!\n\n\n## Dependencies\n\n- python3 (obviously!)\n- [pysodium](https://pypi.python.org/pypi/pysodium) for the crypto\n- [bokeh](http://bokeh.pydata.org/en/latest/) for the analysis and interactive\n  visualization\n\n## Usage / High-level explainations\n\nI don\'t think this is any useful to you if you don\'t plan to understand how the\nalgorithm works so you should read the whitepaper first. After that, the\nimplementation is _quite_ straightforward: the code is divided in the same\nfunctions as presented in the paper:\n\n- The main loop (which is a coroutine to enable step by step evaluation and\n  avoid threads).\n- `sync(<remote-node-id>, <payload-to-embed>)` which queries the remote node\n  and updates local data.\n- `divide_rounds` which sets round number and witnessness for the new\n  transactions.\n- `decide_fame` which does the voting stuff.\n- `find_order` which update the final transactions list according to new\n  election results.\n\nEverything is packed into a `Node` class which is initialized with it\'s signing\nkeypair and with a dictionary mapping a node ID (it\'s public key) to some mean\nto query data from him (the `ask_sync` method). Note that for simplicity, a\nnode is included in it\'s own mapping.\n\nYou can fiddle directly with that code or try out my nice interactive\nvisualizations to see how the network evolves in real time with:\n\n```shell\nbokeh serve --show viz.py --args <number of nodes>\n```\n\nThis will start locally the specified number of nodes and by pressing the\n_play_ button it will start choosing one at random every few miliseconds and do\na mainloop step. The color indicates the round number (it\'s just a random\ncolor, the only thing is that transactions with the same round have the same\ncolor).\n\n## Algorithm details\n\nActually, I didn\'t implement the algorithm completely straitforward with full\ngraph traversals everywhere and big loops over all nodes. The main specificity\nI introduced is a mapping I named `can_see`. It is updated along the round\nnumber in `divide_rounds` and stores for each transaction a dictionnary that\nmaps to each node the id of the latest (highest) transaction from that node\nthis transaction can see (if there is one). It is easily updated by a\nrecurrence relation and enables to quickly seable and strongly seable\ntransactions.\n\nWith nn and nt respectively the number of nodes and the number of transactions,\nthis datastructure adds up O(nn\\*nt) space and enables to compute the set of\nwitnesses a transaction can strongly see in O(nn^2).\n\n## IPFS\n\nA variant lives in the `ipfs` branch. This variant uses [IPFS](http://ipfs.io/)\nas a backend to store the hashgraph. Indeed a swirlds _hashgraph_ is just the\nsame as an IPFS _merkle DAG_. This enables global deduplication of the\nhashgraph (bandwith and computation efficient syncing between members). The\nsyncing process is just about getting the head of the remote member. As the\nhead of a member is stored in an IPNS record, this code is currently very slow,\nbut a lot of work is currently going on on the IPFS side to improve IPNS (_cf_\n[IPRS](https://github.com/ipfs/go-iprs)).\n\n## Work In Progress\n\n- The interactive visualization is still rather crude.\n- There is no strong fork detection when syncing.\n- There is no real networking (the _communication_ is really just a method\n  call). This should not be complicated to implement, but I will have to bring\n  in threads, locks and stuff like that. I am actually thinking about embedding\n  the hashgraph in [IPFS](http://ipfs.io/) objects as it fits perfectly. This\n  would enable to just drop any crypto and network operation as IPFS already\n  takes care of it well.\n\n## Some updates\n\n_AKA why swirlds isn\'t *that* much interesting_\n\nFollowing [this issue](https://github.com/Lapin0t/py-swirld/issues/1), I want\nto stress some things (also explaining why I stopped to be interested in\nswirlds).\n\nThere are two flaws in this protocol which are somewhat related. The first one\nis that the protocol cannot scale very well (ie have sublinear complexity in\nthe number of nodes for message handling) and the second one is that it doesn\'t\nhandle open-membership.\n\nThese issues may not be relevant if you want a distributed db in a medium-sized\nclosed organisation having some external centralized auth system. But this is\nimportant if you want to make an internet-sized distributed db which has\nabsolutely no owner and no centralized registration service.\n\nI\'m gonna start with the second issue. To have a distributed db with\nopen-membership you *must* have protocol handling open-membership, you can\'t\nmake some construction with a second distributed db for the current stake\nrepartition with swirlds because there should then be a fixed set of "stake\nvalidators". Some solutions proposed on swirlds.com are:\n\n- an invite system where one gives a share of it\'s stake to people he\'s\n  inviting, not trivial to do and the one starting the network has full power\n  until he gives some away\n- rely on some external mean (associating with a bitcoin wallet etc), that\'s\n  aweful because this should be something replacing it\n- use PoW for the stake, hurray, we just got back to a blockchain\n\nThey are all wacky because there is no mean to transform a closed membership\nprotocol into an open one.\n\nThe first issue gives a hint why it is probably not interesting to use any hack\nor external mean to turn swirlds into an open-membership protocol: just try to\nthink of an efficient algorithm for the voting part (`divide_rounds`,\n`decide_fame`, ...), you must in one way or another iterate through all members\n(and probably also through some part of their respective transaction history)\nso each transaction takes a time *at least* linear in the number of nodes. Sure\nyou can sync only with a few people, maybe O(log n) or even O(1) using a\ncarefully choosen De Bruijn graph but you will still need to maintain the full\nhashgraph and iterate through witness transactions (and there is one for each\nnode), there\'s no escaping from the O(n) time and space lower bound and linear\ncomplexity is bad(tm). Any decent distributed database that wants internet\nscale should be having time and space bounds at most Omega(log n) for incoming\nevents processing.\n\nSo bottom line: don\'t star this repo, this algorithm is bad ;).\n\n### Conclusion\n\nWhy do you actually need strong consensus? (tldr: you don\'t)\n\n1. Probably you want to create a new cryptocurrency, but crypto-currencies are\n   mostly shitty in the sense that the 2 groups interested in them are (1)\n   speculators which are people how\'s job is to scam other people (yep, their\n   job is mostly to buy things lower than what people think it\'s worth and\n   resell it higher, having no interaction at all with the underlying physical\n   object of their transactions), they only care about how much money will be\n   in their pocket at the end. I don\'t think it has any sense to design an\n   economic system where you can get money without creating anything, physical\n   or intellectual) (2) right-libertarian who love capitalism: nope guys, the\n   market isn\'t automagically stabilizing, it strives to make rich people\n   richer and poor people poorer, which is quite the oposite of convergence.\n   So at the end, i have nothing special against *crypto*-currencies per se,\n   it\'s just that it has attracted quite a lot of attention from uninteresting\n   people that have uninteresting and very conservative ideas.\n\n2. Maybe you\'re writing this awesome service that is fully distributed (p2p)\n   and you need some structure to handle shared mutable data. You should think\n   twice about what real guarantees you need, because high chance are that you\n   can overcome weak consensus. There are loads of distributed datastructures\n   that take advantage of providing lower guarantees but still be on point for\n   a particular use.\n\n   - DHT will be great for a cache and if you\'re adventurous you can go read\n     some papers about distributed skip-lists and skip-graph.\n   - A lot of abstract datastructures (mostly sets and counters) have CRDT\n     implementations which you can use. You can then plug an anti-entropy (or\n     epidemic) protocol to synchronize the states in an eventually consistent\n     fashion.\n   - Networking is hard, if you need something like decentralized pub/sub, you\n     should take a look at the Matrix protocol. It\'s highly unstable and I have\n     some criticism on their protocol, but they\'re trying to solve this\n     problem.\n\n3. Oh so you do need strong consensus with open membership because you want to\n   create a replacement protocol for naming things on the internet? Somewhat a\n   mix between DNS and SSL-PKI? and distributed keybase? Sure, that\'s a good\n   goal, but at first, you shouldn\'t rely on unique global identifiers that are\n   not self-authentifying (so you shouldn\'t need strong consistency for that\n   part). That said, strong consistency might be useful in this case. You\n   should probably take a look at SCP (stellar consensus protocol), this one\n   should be good.\n\n### Post-Scriptum\n\nPlease don\'t ever use or mention blockchain protocols. If you encounter someone\nwho does, please repeat him the following (or just go away from him, chances\nare he falls into one of the two categories of first conclusion bullet and he\nwill be much harder to deal with).\n\nBlockchains are ledgers, not generic databases. Sure you can build a\ngeneric database on top of a ledger, but i\'m not sure we would like a world\nwhere every database would be a dictionary mapping unique IDs to integers.\n\nPoW blockchain\'s security claims rely on:\n\n- The protocol\'s own inefficiency. The only slow part is block creation and\n  making it fast would render the protocol trivially insecure.\n- The fact that users rational. Maybe we are maybe not, the problem is that is\n  just a fancy way of saying they assume users want to maximise the number of\n  currency-tokens they have which is *not* a valid assumption: adversaries will\n  surely not behave in a manner that maximises their tokens, they only want to\n  crash your system. Instead you should target a byzantine-fault tolerant\n  protocol.\n'