b'# zi2zi: Master Chinese Calligraphy with Conditional Adversarial Networks\n\n<p align="center">\n  <img src="assets/intro.gif" alt="animation", style="width: 350px;"/>\n</p>\n\n## Introduction\nLearning eastern asian language typefaces with GAN. zi2zi(\xe5\xad\x97\xe5\x88\xb0\xe5\xad\x97, meaning from character to character) is an application and extension of the recent popular [pix2pix](https://github.com/phillipi/pix2pix) model to Chinese characters.\n\nDetails could be found in this [**blog post**](https://kaonashi-tyc.github.io/2017/04/06/zi2zi.html).\n\n## Network Structure\n### Original Model\n![alt network](assets/network.png)\n\nThe network structure is based off pix2pix with the addition of category embedding and two other losses, category loss and constant loss, from [AC-GAN](https://arxiv.org/abs/1610.09585) and [DTN](https://arxiv.org/abs/1611.02200) respectively.\n\n### Updated Model with Label Shuffling\n\n![alt network](assets/network_v2.png)\n\nAfter sufficient training, **d_loss** will drop to near zero, and the model\'s performance plateaued. **Label Shuffling** mitigate this problem by presenting new challenges to the model. \n\nSpecifically, within a given minibatch, for the same set of source characters, we generate two sets of target characters: one with correct embedding labels, the other with the shuffled labels. The shuffled set likely will not have the corresponding target images to compute **L1\\_Loss**, but can be used as a good source for all other losses, forcing the model to further generalize beyond the limited set of provided examples. Empirically, label shuffling improves the model\'s generalization on unseen data with better details, and decrease the required number of characters.\n\nYou can enable label shuffling by setting **flip_labels=1** option in **train.py** script. It is recommended that you enable this after **d_loss** flatlines around zero, for further tuning.\n\n## Gallery\n### Compare with Ground Truth\n\n<p align="center">\n<img src="assets/compare3.png" alt="compare" width="600"/>\n</p>\n\n### Brush Writing Fonts\n<p align="center">\n<img src="assets/cj_mix.png" alt="brush"  width="600"/>\n</p>\n\n### Cursive Script (Requested by SNS audience)\n<p align="center">\n<img src="assets/cursive.png" alt="cursive"  width="600"/>\n</p>\n\n\n### Mingchao Style (\xe5\xae\x8b\xe4\xbd\x93/\xe6\x98\x8e\xe6\x9c\x9d\xe4\xbd\x93)\n<p align="center">\n<img src="assets/mingchao4.png" alt="gaussian"  width="600"/>\n</p>\n\n### Korean\n<p align="center">\n<img src="assets/kr_mix_v2.png" alt="korean"  width="600"/>\n</p>\n\n### Interpolation\n<p align="center">\n  <img src="assets/transition.png" alt="animation",  width="600"/>\n</p>\n\n### Animation\n<p align="center">\n  <img src="assets/poem.gif" alt="animation",  width="250"/>\n  <img src="assets/ko_wiki.gif" alt="animation", width="250"/>\n</p>\n\n<p align="center">\n  <img src="assets/reddit_bonus_humor_easter_egg.gif" alt="easter egg"  width="300"/>\n</p>\n\n\n## How to Use\n### Step Zero\nDownload tons of fonts as you please\n### Requirement\n* Python 2.7\n* CUDA\n* cudnn\n* Tensorflow >= 1.0.1\n* Pillow(PIL)\n* numpy >= 1.12.1\n* scipy >= 0.18.1\n* imageio\n\n### Preprocess\nTo avoid IO bottleneck, preprocessing is necessary to pickle your data into binary and persist in memory during training.\n\nFirst run the below command to get the font images:\n\n```sh\npython font2img.py --src_font=src.ttf\n                   --dst_font=tgt.otf\n                   --charset=CN \n                   --sample_count=1000\n                   --sample_dir=dir\n                   --label=0\n                   --filter=1\n                   --shuffle=1\n```\nFour default charsets are offered: CN, CN_T(traditional), JP, KR. You can also point it to a one line file, it will generate the images of the characters in it. Note, **filter** option is highly recommended, it will pre sample some characters and filter all the images that have the same hash, usually indicating that character is missing. **label** indicating index in the category embeddings that this font associated with, default to 0.\n\nAfter obtaining all images, run **package.py** to pickle the images and their corresponding labels into binary format:\n\n```sh\npython package.py --dir=image_directories\n                  --save_dir=binary_save_directory\n                  --split_ratio=[0,1]\n```\n\nAfter running this, you will find two objects **train.obj** and **val.obj** under the save_dir for training and validation, respectively.\n\n### Experiment Layout\n```sh\nexperiment/\n\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 data\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train.obj\n    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 val.obj\n```\nCreate a **experiment** directory under the root of the project, and a data directory within it to place the two binaries. Assuming a directory layout enforce bettet data isolation, especially if you have multiple experiments running.\n### Train\nTo start training run the following command\n\n```sh\npython train.py --experiment_dir=experiment \n                --experiment_id=0\n                --batch_size=16 \n                --lr=0.001\n                --epoch=40 \n                --sample_steps=50 \n                --schedule=20 \n                --L1_penalty=100 \n                --Lconst_penalty=15\n```\n**schedule** here means in between how many epochs, the learning rate will decay by half. The train command will create **sample,logs,checkpoint** directory under **experiment_dir** if non-existed, where you can check and manage the progress of your training.\n\n### Infer and Interpolate\nAfter training is done, run the below command to infer test data:\n\n```sh\npython infer.py --model_dir=checkpoint_dir/ \n                --batch_size=16 \n                --source_obj=binary_obj_path \n                --embedding_ids=label[s] of the font, separate by comma\n                --save_dir=save_dir/\n```\n\nAlso you can do interpolation with this command:\n\n```sh\npython infer.py --model_dir= checkpoint_dir/ \n                --batch_size=10\n                --source_obj=obj_path \n                --embedding_ids=label[s] of the font, separate by comma\n                --save_dir=frames/ \n                --output_gif=gif_path \n                --interpolate=1 \n                --steps=10\n                --uroboros=1\n```\n\nIt will run through all the pairs of fonts specified in embedding_ids and interpolate the number of steps as specified. \n\n### Pretrained Model\nPretained model can be downloaded [here](https://drive.google.com/open?id=0Bz6mX0EGe2ZuNEFSNWpTQkxPM2c) which is trained with 27 fonts, only generator is saved to reduce the model size. You can use encoder in the this pretrained model to accelerate the training process.\n## Acknowledgements\nCode derived and rehashed from:\n\n* [pix2pix-tensorflow](https://github.com/yenchenlin/pix2pix-tensorflow) by [yenchenlin](https://github.com/yenchenlin)\n* [Domain Transfer Network](https://github.com/yunjey/domain-transfer-network) by [yunjey](https://github.com/yunjey)\n* [ac-gan](https://github.com/buriburisuri/ac-gan) by [buriburisuri](https://github.com/buriburisuri)\n* [dc-gan](https://github.com/carpedm20/DCGAN-tensorflow) by [carpedm20](https://github.com/carpedm20)\n* [origianl pix2pix torch code](https://github.com/phillipi/pix2pix) by [phillipi](https://github.com/phillipi)\n\n## License\nApache 2.0\n\n'