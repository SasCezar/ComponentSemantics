b'# Keras Visualization Toolkit\n[![Build Status](https://travis-ci.org/raghakot/keras-vis.svg?branch=master)](https://travis-ci.org/raghakot/keras-vis)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/raghakot/keras-vis/blob/master/LICENSE)\n[![Slack](https://img.shields.io/badge/slack-discussion-E01563.svg)](https://keras-vis.herokuapp.com/)\n\nkeras-vis is a high-level toolkit for visualizing and debugging your trained keras neural net models. Currently\nsupported visualizations include:\n\n- Activation maximization\n- Saliency maps\n- Class activation maps\n\nAll visualizations by default support N-dimensional image inputs. i.e., it generalizes to N-dim image inputs \nto your model.\n\nThe toolkit generalizes all of the above as energy minimization problems with a clean, easy to use, \nand extendable interface. Compatible with both theano and tensorflow backends with \'channels_first\', \'channels_last\' \ndata format.\n\n## Quick links\n* Read the documentation at [https://raghakot.github.io/keras-vis](https://raghakot.github.io/keras-vis). \n    * The Japanese edition is [https://keisen.github.io/keras-vis-docs-ja](https://keisen.github.io/keras-vis-docs-ja).\n* Join the slack [channel](https://keras-vis.herokuapp.com/) for questions/discussions.\n* We are tracking new features/tasks in [waffle.io](https://waffle.io/raghakot/keras-vis). Would love it if you lend us \na hand and submit PRs.\n\n## Getting Started\nIn image backprop problems, the goal is to generate an input image that minimizes some loss function.\nSetting up an image backprop problem is easy.\n\n**Define weighted loss function**\n\nVarious useful loss functions are defined in [losses](https://raghakot.github.io/keras-vis/vis.losses).\nA custom loss function can be defined by implementing [Loss.build_loss](https://raghakot.github.io/keras-vis/vis.losses/#lossbuild_loss).\n\n```python\nfrom vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\n\nfilter_indices = [1, 2, 3]\n\n# Tuple consists of (loss_function, weight)\n# Add regularizers as needed.\nlosses = [\n    (ActivationMaximization(keras_layer, filter_indices), 1),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\n```\n\n**Configure optimizer to minimize weighted loss**\n\nIn order to generate natural looking images, image search space is constrained using regularization penalties. \nSome common regularizers are defined in [regularizers](https://raghakot.github.io/keras-vis/vis.regularizers).\nLike loss functions, custom regularizer can be defined by implementing \n[Loss.build_loss](https://raghakot.github.io/keras-vis/vis.losses/#lossbuild_loss).\n\n```python\nfrom vis.optimizer import Optimizer\n\noptimizer = Optimizer(model.input, losses)\nopt_img, grads, _ = optimizer.minimize()\n```\n\nConcrete examples of various supported visualizations can be found in \n[examples folder](https://github.com/raghakot/keras-vis/tree/master/examples).\n\n## Installation\n\n1) Install [keras](https://github.com/fchollet/keras/blob/master/README.md#installation) \nwith theano or tensorflow backend. Note that this library requires Keras > 2.0\n\n2) Install keras-vis\n> From sources\n```bash\nsudo python setup.py install\n```\n\n> PyPI package\n```bash\nsudo pip install keras-vis\n```\n\n## Visualizations\n\n**NOTE: The links are currently broken and the entire documentation is being reworked.\nPlease see examples/ for samples.**\n\nNeural nets are black boxes. In the recent years, several approaches for understanding and visualizing Convolutional \nNetworks have been developed in the literature. They give us a way to peer into the black boxes, \ndiagnose mis-classifications, and assess whether the network is over/under fitting. \n\nGuided backprop can also be used to create [trippy art](https://deepdreamgenerator.com/gallery), neural/texture \n[style transfer](https://github.com/jcjohnson/neural-style) among the list of other growing applications.\n\nVarious visualizations, documented in their own pages, are summarized here.\n\n<hr/>\n\n### [Conv filter visualization](https://raghakot.github.io/keras-vis/visualizations/conv_filters)\n<img src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/conv_vis/cover.jpg?raw=true"/>\n\n*Convolutional filters learn \'template matching\' filters that maximize the output when a similar template \npattern is found in the input image. Visualize those templates via Activation Maximization.*\n\n<hr/>\n\n### [Dense layer visualization](https://raghakot.github.io/keras-vis/visualizations/dense)\n\n<img src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/dense_vis/cover.png?raw=true"/>\n\n*How can we assess whether a network is over/under fitting or generalizing well?*\n\n<hr/>\n\n### [Attention Maps](https://raghakot.github.io/keras-vis/visualizations/attention)\n\n<img src="https://raw.githubusercontent.com/raghakot/keras-vis/master/images/attention_vis/cover.png?raw=true"/>\n\n*How can we assess whether a network is attending to correct parts of the image in order to generate a decision?*\n\n<hr/>\n\n### Generating animated gif of optimization progress\nIt is possible to generate an animated gif of optimization progress by leveraging \n[callbacks](https://raghakot.github.io/keras-vis/vis.callbacks). Following example shows how to visualize the \nactivation maximization for \'ouzel\' class (output_index: 20).\n\n```python\nfrom keras.applications import VGG16\n\nfrom vis.losses import ActivationMaximization\nfrom vis.regularizers import TotalVariation, LPNorm\nfrom vis.input_modifiers import Jitter\nfrom vis.optimizer import Optimizer\nfrom vis.callbacks import GifGenerator\n\n# Build the VGG16 network with ImageNet weights\nmodel = VGG16(weights=\'imagenet\', include_top=True)\nprint(\'Model loaded.\')\n\n# The name of the layer we want to visualize\n# (see model definition in vggnet.py)\nlayer_name = \'predictions\'\nlayer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\noutput_class = [20]\n\nlosses = [\n    (ActivationMaximization(layer_dict[layer_name], output_class), 2),\n    (LPNorm(model.input), 10),\n    (TotalVariation(model.input), 10)\n]\nopt = Optimizer(model.input, losses)\nopt.minimize(max_iter=500, verbose=True, input_modifiers=[Jitter()], callbacks=[GifGenerator(\'opt_progress\')])\n\n```\n\nNotice how the output jitters around? This is because we used [Jitter](https://raghakot.github.io/keras-vis/vis.modifiers/#jitter), \na kind of [ImageModifier](https://raghakot.github.io/keras-vis/vis.modifiers/#imagemodifier) that is known to produce \ncrisper activation maximization images. As an exercise, try:\n\n- Without Jitter\n- Varying various loss weights\n\n![opt_progress](https://raw.githubusercontent.com/raghakot/keras-vis/master/images/opt_progress.gif?raw=true "Optimization progress")\n\n<hr/>\n\n## Citation\n\nPlease cite keras-vis in your publications if it helped your research. Here is an example BibTeX entry:\n\n```\n@misc{raghakotkerasvis,\n  title={keras-vis},\n  author={Kotikalapudi, Raghavendra and contributors},\n  year={2017},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/raghakot/keras-vis}},\n}\n```\n'