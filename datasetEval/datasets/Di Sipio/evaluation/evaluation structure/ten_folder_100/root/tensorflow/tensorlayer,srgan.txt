b'## Super Resolution Examples\n\n\n\n\nWe run this script under [TensorFlow](https://www.tensorflow.org) 2.0 and the [TensorLayer](https://github.com/tensorlayer/tensorlayer) 2.0+. For TensorLayer 1.4 version, please check [release](https://github.com/tensorlayer/srgan/releases).\n\n<!---\n\xe2\x9a\xa0\xef\xb8\x8f This repo will be merged into example folder of [tensorlayer](https://github.com/zsdonghao/tensorlayer) soon.\n-->\n\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80 **THIS PROJECT WILL BE CLOSED AND MOVED TO [THIS FOLDER](https://github.com/tensorlayer/tensorlayer/tree/master/examples) IN A MONTH.**\n\n\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80 **THIS PROJECT WILL BE CLOSED AND MOVED TO [THIS FOLDER](https://github.com/tensorlayer/tensorlayer/tree/master/examples) IN A MONTH.**\n\n\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80\xf0\x9f\x9a\x80 **THIS PROJECT WILL BE CLOSED AND MOVED TO [THIS FOLDER](https://github.com/tensorlayer/tensorlayer/tree/master/examples) IN A MONTH.**\n\n<!--More cool Computer Vision applications such as pose estimation and style transfer can be found in this [organization](https://github.com/tensorlayer).**\n-->\n\n### SRGAN Architecture\n\nTensorFlow Implementation of ["Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network"](https://arxiv.org/abs/1609.04802)\n\n<a href="http://tensorlayer.readthedocs.io">\n<div align="center">\n\t<img src="img/model.jpeg" width="80%" height="10%"/>\n</div>\n</a>\n\n\n### Results\n\n<a href="http://tensorlayer.readthedocs.io">\n<div align="center">\n\t<img src="img/SRGAN_Result2.png" width="80%" height="50%"/>\n</div>\n</a>\n\n<a href="http://tensorlayer.readthedocs.io">\n<div align="center">\n\t<img src="img/SRGAN_Result3.png" width="80%" height="50%"/>\n</div>\n</a>\n\n### Prepare Data and Pre-trained VGG\n\n- 1. You need to download the pretrained VGG19 model in [here](https://mega.nz/#!xZ8glS6J!MAnE91ND_WyfZ_8mvkuSa2YcA7q-1ehfSm-Q1fxOvvs) as [tutorial_vgg19.py](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_vgg19.py) show.\n- 2. You need to have the high resolution images for training.\n  -  In this experiment, I used images from [DIV2K - bicubic downscaling x4 competition](http://www.vision.ee.ethz.ch/ntire17/), so the hyper-paremeters in `config.py` (like number of epochs) are seleted basic on that dataset, if you change a larger dataset you can reduce the number of epochs. \n  -  If you dont want to use DIV2K dataset, you can also use [Yahoo MirFlickr25k](http://press.liacs.nl/mirflickr/mirdownload.html), just simply download it using `train_hr_imgs = tl.files.load_flickr25k_dataset(tag=None)` in `main.py`. \n  -  If you want to use your own images, you can set the path to your image folder via `config.TRAIN.hr_img_path` in `config.py`.\n\n\n\n### Run\n- Set your image folder in `config.py`, if you download [DIV2K - bicubic downscaling x4 competition](http://www.vision.ee.ethz.ch/ntire17/) dataset, you don\'t need to change it. \n- Other links for DIV2K, in case you can\'t find it : [test\\_LR\\_bicubic_X4](https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_test_LR_bicubic_X4.zip), [train_HR](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip), [train\\_LR\\_bicubic_X4](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X4.zip), [valid_HR](https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_valid_HR.zip), [valid\\_LR\\_bicubic_X4](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X4.zip).\n\n```python\nconfig.TRAIN.img_path = "your_image_folder/"\n```\n\n- Start training.\n\n```bash\npython train.py\n```\n\n- Start evaluation. \n\n<!--([pretrained model](https://github.com/tensorlayer/srgan/releases/tag/1.2.0) for DIV2K)-->\n\n```bash\npython train.py --mode=evaluate \n```\n\n\n### Reference\n* [1] [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802)\n* [2] [Is the deconvolution layer the same as a convolutional layer ?](https://arxiv.org/abs/1609.07009)\n\n### Author\n- [zsdonghao](https://github.com/zsdonghao)\n\n### Citation\nIf you find this project useful, we would be grateful if you cite the TensorLayer paper\xef\xbc\x9a\n\n```\n@article{tensorlayer2017,\nauthor = {Dong, Hao and Supratak, Akara and Mai, Luo and Liu, Fangde and Oehmichen, Axel and Yu, Simiao and Guo, Yike},\njournal = {ACM Multimedia},\ntitle = {{TensorLayer: A Versatile Library for Efficient Deep Learning Development}},\nurl = {http://tensorlayer.org},\nyear = {2017}\n}\n```\n\n### Other Projects\n\n- [Style Transfer](https://github.com/tensorlayer/adaptive-style-transfer)\n- [Pose Estimation](https://github.com/tensorlayer/openpose)\n\n### Discussion\n\n- [TensorLayer Slack](https://join.slack.com/t/tensorlayer/shared_invite/enQtMjUyMjczMzU2Njg4LWI0MWU0MDFkOWY2YjQ4YjVhMzI5M2VlZmE4YTNhNGY1NjZhMzUwMmQ2MTc0YWRjMjQzMjdjMTg2MWQ2ZWJhYzc)\n- [TensorLayer WeChat](https://github.com/tensorlayer/tensorlayer-chinese/blob/master/docs/wechat_group.md)\n\n### License\n\n- For academic and non-commercial use only.\n- For commercial use, please contact tensorlayer@gmail.com.\n'