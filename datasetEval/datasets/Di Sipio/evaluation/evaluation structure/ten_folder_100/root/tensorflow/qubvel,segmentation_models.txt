b'.. raw:: html\n\n    <p align="center">\n      <img width="400" height="200" src="https://github.com/qubvel/segmentation_models/blob/master/images/logo.png">\n    </p>\n    \n    <h1 align="center"> Segmentation Models </h1>\n    \n    <p align="center">\n      <a href="https://badge.fury.io/py/segmentation-models" alt="PyPI">\n        <img src="https://badge.fury.io/py/segmentation-models.svg" /></a>\n      <a href="https://segmentation-models.readthedocs.io/en/latest/?badge=latest" alt="Documentation">\n        <img src="https://readthedocs.org/projects/segmentation-models/badge/?version=latest" /></a>\n      <a href="https://travis-ci.com/qubvel/segmentation_models" alt="Build Status">\n        <img src="https://travis-ci.com/qubvel/segmentation_models.svg?branch=master" /></a>\n    </p>\n\n**Segmentation models** is python library with Neural Networks for\n`Image\nSegmentation <https://en.wikipedia.org/wiki/Image_segmentation>`__ based\non `Keras <https://keras.io>`__\nand `Tensorflow Keras <https://www.tensorflow.org/>`__ frameworks.\n\n**The main features** of this library are:\n\n-  High level API (just two lines of code to create model for segmentation)\n-  **4** models architectures for binary and multi-class image segmentation\n   (including legendary **Unet**)\n-  **25** available backbones for each architecture\n-  All backbones have **pre-trained** weights for faster and better\n   convergence\n- Helpful segmentation losses (Jaccard, Dice, Focal) and metrics (IoU, F-score)\n\n**Important note**\n\n    Some models of version ``1.*`` are not compatible with previously trained models,\n    if you have such models and want to load them - roll back with:\n\n    $ pip install -U segmentation-models==0.2.1\n\nTable of Contents\n~~~~~~~~~~~~~~~~~\n - `Quick start`_\n - `Simple training pipeline`_\n - `Examples`_\n - `Models and Backbones`_\n - `Installation`_\n - `Documentation`_\n - `Change log`_\n - `Citing`_\n - `License`_\n \nQuick start\n~~~~~~~~~~~\nLibrary is build to work together with Keras and TensorFlow Keras frameworks\n\n.. code:: python\n\n    import segmentation_models as sm\n    # Segmentation Models: using `keras` framework.\n\nBy default it tries to import ``keras``, if it is not installed, it will try to start with ``tensorflow.keras`` framework.\nThere are several ways to choose framework:\n\n- Provide environment variable ``SM_FRAMEWORK=keras`` / ``SM_FRAMEWORK=tf.keras`` before import ``segmentation_models``\n- Change framework ``sm.set_framework(\'keras\')`` /  ``sm.set_framework(\'tf.keras\')``\n\nYou can also specify what kind of ``image_data_format`` to use, segmentation-models works with both: ``channels_last`` and ``channels_first``.\nThis can be useful for further model conversion to Nvidia TensorRT format or optimizing model for cpu/gpu computations.\n\n.. code:: python\n\n    import keras\n    # or from tensorflow import keras\n\n    keras.backend.set_image_data_format(\'channels_last\')\n    # or keras.backend.set_image_data_format(\'channels_first\')\n\nCreated segmentation model is just an instance of Keras Model, which can be build as easy as:\n\n.. code:: python\n    \n    model = sm.Unet()\n    \nDepending on the task, you can change the network architecture by choosing backbones with fewer or more parameters and use pretrainded weights to initialize it:\n\n.. code:: python\n\n    model = sm.Unet(\'resnet34\', encoder_weights=\'imagenet\')\n\nChange number of output classes in the model (choose your case):\n\n.. code:: python\n    \n    # binary segmentation (this parameters are default when you call Unet(\'resnet34\')\n    model = sm.Unet(\'resnet34\', classes=1, activation=\'sigmoid\')\n    \n.. code:: python\n    \n    # multiclass segmentation with non overlapping class masks (your classes + background)\n    model = sm.Unet(\'resnet34\', classes=3, activation=\'softmax\')\n    \n.. code:: python\n    \n    # multiclass segmentation with independent overlapping/non-overlapping class masks\n    model = sm.Unet(\'resnet34\', classes=3, activation=\'sigmoid\')\n    \n    \nChange input shape of the model:\n\n.. code:: python\n    \n    # if you set input channels not equal to 3, you have to set encoder_weights=None\n    # how to handle such case with encoder_weights=\'imagenet\' described in docs\n    model = Unet(\'resnet34\', input_shape=(None, None, 6), encoder_weights=None)\n   \nSimple training pipeline\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n    import segmentation_models as sm\n\n    BACKBONE = \'resnet34\'\n    preprocess_input = sm.get_preprocessing(BACKBONE)\n\n    # load your data\n    x_train, y_train, x_val, y_val = load_data(...)\n\n    # preprocess input\n    x_train = preprocess_input(x_train)\n    x_val = preprocess_input(x_val)\n\n    # define model\n    model = sm.Unet(BACKBONE, encoder_weights=\'imagenet\')\n    model.compile(\n        \'Adam\',\n        loss=sm.losses.bce_jaccard_loss,\n        metrics=[sm.metrics.iou_score],\n    )\n\n    # fit model\n    # if you use data generator use model.fit_generator(...) instead of model.fit(...)\n    # more about `fit_generator` here: https://keras.io/models/sequential/#fit_generator\n    model.fit(\n       x=x_train,\n       y=y_train,\n       batch_size=16,\n       epochs=100,\n       validation_data=(x_val, y_val),\n    )\n\nSame manipulations can be done with ``Linknet``, ``PSPNet`` and ``FPN``. For more detailed information about models API and  use cases `Read the Docs <https://segmentation-models.readthedocs.io/en/latest/>`__.\n\nExamples\n~~~~~~~~\nModels training examples:\n - [Jupyter Notebook] Binary segmentation (`cars`) on CamVid dataset `here <https://github.com/qubvel/segmentation_models/blob/master/examples/binary%20segmentation%20(camvid).ipynb>`__.\n - [Jupyter Notebook] Multi-class segmentation (`cars`, `pedestrians`) on CamVid dataset `here <https://github.com/qubvel/segmentation_models/blob/master/examples/multiclass%20segmentation%20(camvid).ipynb>`__.\n\nModels and Backbones\n~~~~~~~~~~~~~~~~~~~~\n**Models**\n\n-  `Unet <https://arxiv.org/abs/1505.04597>`__\n-  `FPN <http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf>`__\n-  `Linknet <https://arxiv.org/abs/1707.03718>`__\n-  `PSPNet <https://arxiv.org/abs/1612.01105>`__\n\n============= ==============\nUnet          Linknet\n============= ==============\n|unet_image|  |linknet_image|\n============= ==============\n\n============= ==============\nPSPNet        FPN\n============= ==============\n|psp_image|   |fpn_image|\n============= ==============\n\n.. _Unet: https://github.com/qubvel/segmentation_models/blob/readme/LICENSE\n.. _Linknet: https://arxiv.org/abs/1707.03718\n.. _PSPNet: https://arxiv.org/abs/1612.01105\n.. _FPN: http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n.. |unet_image| image:: https://github.com/qubvel/segmentation_models/blob/master/images/unet.png\n.. |linknet_image| image:: https://github.com/qubvel/segmentation_models/blob/master/images/linknet.png\n.. |psp_image| image:: https://github.com/qubvel/segmentation_models/blob/master/images/pspnet.png\n.. |fpn_image| image:: https://github.com/qubvel/segmentation_models/blob/master/images/fpn.png\n\n**Backbones**\n\n.. table:: \n\n    =============  ===== \n    Type           Names\n    =============  =====\n    VGG            ``\'vgg16\' \'vgg19\'``\n    ResNet         ``\'resnet18\' \'resnet34\' \'resnet50\' \'resnet101\' \'resnet152\'``\n    SE-ResNet      ``\'seresnet18\' \'seresnet34\' \'seresnet50\' \'seresnet101\' \'seresnet152\'``\n    ResNeXt        ``\'resnext50\' \'resnext101\'``\n    SE-ResNeXt     ``\'seresnext50\' \'seresnext101\'``\n    SENet154       ``\'senet154\'``\n    DenseNet       ``\'densenet121\' \'densenet169\' \'densenet201\'`` \n    Inception      ``\'inceptionv3\' \'inceptionresnetv2\'``\n    MobileNet      ``\'mobilenet\' \'mobilenetv2\'``\n    EfficientNet   ``\'efficientnetb0\' \'efficientnetb1\' \'efficientnetb2\' \'efficientnetb3\' \'efficientnetb4\' \'efficientnetb5\' efficientnetb6\' efficientnetb7\'``\n    =============  =====\n\n.. epigraph::\n    All backbones have weights trained on 2012 ILSVRC ImageNet dataset (``encoder_weights=\'imagenet\'``). \n\n\nInstallation\n~~~~~~~~~~~~\n\n**Requirements**\n\n1) python 3\n2) keras >= 2.2.0 or tensorflow >= 1.13\n3) keras-applications >= 1.0.7, <=1.0.8\n4) image-classifiers == 1.0.*\n5) efficientnet == 1.0.*\n\n**PyPI stable package**\n\n.. code:: bash\n\n    $ pip install -U segmentation-models\n\n**PyPI latest package**\n\n.. code:: bash\n\n    $ pip install -U --pre segmentation-models\n\n**Source latest version**\n\n.. code:: bash\n\n    $ pip install git+https://github.com/qubvel/segmentation_models\n    \nDocumentation\n~~~~~~~~~~~~~\nLatest **documentation** is avaliable on `Read the\nDocs <https://segmentation-models.readthedocs.io/en/latest/>`__\n\nChange Log\n~~~~~~~~~~\nTo see important changes between versions look at CHANGELOG.md_\n\nCiting\n~~~~~~~~\n\n.. code::\n\n    @misc{Yakubovskiy:2019,\n      Author = {Pavel Yakubovskiy},\n      Title = {Segmentation Models},\n      Year = {2019},\n      Publisher = {GitHub},\n      Journal = {GitHub repository},\n      Howpublished = {\\url{https://github.com/qubvel/segmentation_models}}\n    } \n\nLicense\n~~~~~~~\nProject is distributed under `MIT Licence`_.\n\n.. _CHANGELOG.md: https://github.com/qubvel/segmentation_models/blob/master/CHANGELOG.md\n.. _`MIT Licence`: https://github.com/qubvel/segmentation_models/blob/master/LICENSE\n'