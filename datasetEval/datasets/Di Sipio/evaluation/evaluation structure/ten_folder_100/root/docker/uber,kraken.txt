b'<p align="center"><img src="assets/kraken-logo-color.svg" width="175" title="Kraken Logo"></p>\n\n<p align="center">\n</a>\n<a href="https://travis-ci.com/uber/kraken"><img src="https://travis-ci.com/uber/kraken.svg?branch=master"></a>\n<a href="https://github.com/uber/kraken/releases"><img src="https://img.shields.io/github/release/uber/kraken.svg" /></a>\n<a href="https://godoc.org/github.com/uber/kraken"><img src="https://godoc.org/github.com/uber/kraken?status.svg"></a>\n<a href="https://goreportcard.com/badge/github.com/uber/kraken"><img src="https://goreportcard.com/badge/github.com/uber/kraken"></a>\n<a href="https://codecov.io/gh/uber/kraken"><img src="https://codecov.io/gh/uber/kraken/branch/master/graph/badge.svg"></a>\n</p>\n\nKraken is a P2P-powered Docker registry that focuses on scalability and availability. It is\ndesigned for Docker image management, replication and distribution in a hybrid cloud environment.\nWith pluggable backend support, Kraken can easily integrate into existing Docker registry setups\nas the distribution layer. \n\nKraken has been in production at Uber since early 2018. In our busiest cluster, Kraken distributes\nmore than 1 million blobs per day, including 100k 1G+ blobs. At its peak production load, Kraken\ndistributes 20K 100MB-1G blobs in under 30 sec.\n\nBelow is the visualization of a small Kraken cluster at work:\n\n<p align="center">\n  <img src="assets/visualization.gif" title="Visualization">\n</p>\n\n# Table of Contents\n\n- [Features](#features)\n- [Design](#design)\n- [Architecture](#architecture)\n- [Benchmark](#benchmark)\n- [Usage](#usage)\n- [Comparison With Other Projects](#comparison-with-other-projects)\n- [Limitations](#limitations)\n- [Contributing](#contributing)\n- [Contact](#contact)\n\n# Features\n\nFollowing are some highlights of Kraken:\n- **Highly scalable**. Kraken is capable of distributing Docker images at > 50% of max download\n  speed limit on every host. Cluster size and image size do not have significant impact on download\n  speed.\n  - Supports at least 15k hosts per cluster.\n  - Supports arbitrarily large blobs/layers. We normally limit max size to 20G for best performance.\n- **Highly available**. No component is a single point of failure.\n- **Secure**. Support uploader authentication and data integrity protection through TLS.\n- **Pluggable storage options**. Instead of managing data, Kraken plugs into reliable blob storage\n  options, like S3, GCS, ECR, HDFS or another registry. The storage interface is simple and new options\n  are easy to add.\n- **Lossless cross cluster replication**. Kraken supports rule-based async replication between\n  clusters.\n- **Minimal dependencies**. Other than pluggable storage, Kraken only has an optional dependency on\n  DNS.\n\n# Design\n\nThe high level idea of Kraken is to have a small number of dedicated hosts seeding content to a\nnetwork of agents running on each host in the cluster.\n\nA central component, the tracker, will orchestrate all participants in the network to form a\npseudo-random regular graph.\n\nSuch a graph has high connectivity and small diameter. As a result, even with only one seeder and\nhaving thousands of peers joining in the same second, all participants can reach a mininum of 80%\nmax upload/download speed in theory (60% with current implementation), and performance doesn\'t\ndegrade much as the blob size and cluster size increase. For more details, see the team\'s [tech\ntalk](https://www.youtube.com/watch?v=waVtYYSXkXU) at KubeCon + CloudNativeCon.\n\n# Architecture\n\n![](assets/architecture.svg)\n\n- Agent\n  - Deployed on every host\n  - Implements Docker registry interface\n  - Announces available content to tracker\n  - Connects to peers returned by tracker to download content\n- Origin\n  - Dedicated seeders\n  - Stores blobs as files on disk backed by pluggable storage (e.g. S3, GCS, ECR)\n  - Forms a self-healing hash ring to distribute load\n- Tracker\n  - Tracks which peers have what content (both in-progress and completed)\n  - Provides ordered lists of peers to connect to for any given blob\n- Proxy\n  - Implements Docker registry interface\n  - Uploads each image layer to the responsible origin (remember, origins form a hash ring)\n  - Uploads tags to build-index\n- Build-Index\n  - Mapping of human readable tag to blob digest\n  - No consistency guarantees: client should use unique tags\n  - Powers image replication between clusters (simple duplicated queues with retry)\n  - Stores tags as files on disk backed by pluggable storage (e.g. S3, GCS, ECR)\n\n# Benchmark\n\nThe following data is from a test where a 3G Docker image with 2 layers is downloaded by 2600 hosts\nconcurrently (5200 blob downloads), with 300MB/s speed limit on all agents (using 5 trackers and\n5 origins):\n\n![](assets/benchmark.svg)\n\n- p50 = 10s (at speed limit)\n- p99 = 18s\n- p99.9 = 22s\n\n# Usage\n\nAll Kraken components can be deployed as Docker containers. To build the Docker images:\n```\n$ make images\n```\nFor information about how to configure and use Kraken, please refer to the [documentation](docs/CONFIGURATION.md).\n\n## Kraken on Kubernetes\n\nYou can use our example Helm chart to deploy Kraken (with an example http fileserver backend) on\nyour k8s cluster:\n```\n$ helm install --name=kraken-demo ./helm\n```\nOnce deployed, each and every node will have a docker registry API exposed on `localhost:30081`.\nFor an example pod spec that pulls images from Kraken agent, see [example](examples/k8s/demo.json).\n\nFor more information on k8s setup, see [README](examples/k8s/README.md).\n\n## Devcluster\n\nTo start a herd container (which contains origin, tracker, build-index and proxy) and two agent\ncontainers with development configuration:\n```\n$ make devcluster\n```\n\nDocker-for-Mac is required for making dev-cluster work on your laptop.\nFor more information on devcluster, please check out devcluster [README](examples/devcluster/README.md).\n\n# Comparison With Other Projects\n\n## Dragonfly from Alibaba\n\nDragonfly cluster has one or a few "supernodes" that coordinates transfer of every 4MB chunk of data\nin the cluster.\n\nWhile the supernode would be able to make optimal decisions, the throughput of the whole cluster is\nlimited by the processing power of one or a few hosts, and the performance would degrade linearly as\neither blob size or cluster size increases.\n\nKraken\'s tracker only helps orchestrate the connection graph, and leaves negotiation of actual data\ntransfer to individual peers, so Kraken scales better with large blobs.\nOn top of that, Kraken is HA and supports cross cluster replication, both are required for a\nreliable hybrid cloud setup.\n\n## BitTorrent\n\nKraken was initially built with a BitTorrent driver, however we ended up implementing our own P2P\ndriver based on BitTorrent protocol to allow for tighter integration with storage solutions and more\ncontrol over performance optimizations.\n\nKraken\'s problem space is slightly different than what BitTorrent was designed for. Kraken\'s goal is\nto reduce global max download time and communication overhead in a stable environment, while\nBitTorrent was designed for an unpredictable and adversarial environment, so it needs to preserve more\ncopies of scarce data and defend against malicious or bad behaving peers.\n\nDespite the differences, we re-examine Kraken\'s protocol from time to time, and if it\'s feasible, we\nhope to make it compatible with BitTorrent again.\n\n# Limitations\n\n- If Docker registry throughput is not the bottleneck in your deployment workflow, switching to\nKraken will not magically speed up your `docker pull`. To actually speed up `docker pull`, consider\nswitching to [Makisu](https://github.com/uber/makisu) to improve layer reusability at build time, or\ntweak compression ratios, as `docker pull` spends most of the time on data decompression.\n- Mutating tags (e.g. updating a `latest` tag) is allowed, however a few things will not work: tag\nlookups immediately afterwards will still return the old value due to Nginx caching, and replication\nprobably won\'t trigger. We are working on supporting this functionality better. If you need tag\nmutation support right now, please reduce cache interval of build-index component. If you also need\nreplication in a multi-cluster setup, please consider setting up another Docker registry as Kraken\'s\nbackend.\n- Theoretically, Kraken should distribute blobs of any size without significant performance\ndegradation, but at Uber we enforce a 20G limit and cannot endorse of the production use of\nultra-large blobs (i.e. 100G+). Peers enforce connection limits on a per blob basis, and new peers\nmight be starved for connections if no peers become seeders relatively soon. If you have ultra-large\nblobs you\'d like to distribute, we recommend breaking them into <10G chunks first.\n\n# Contributing\n\nPlease check out our [guide](docs/CONTRIBUTING.md).\n\n# Contact\n\nTo contact us, please join our [Slack channel](https://join.slack.com/t/uber-container-tools/shared_invite/enQtNTIxODAwMDEzNjM1LWIwYzIxNmUwOGY3MmVmM2MxYTczOTQ4ZDU0YjAxMTA0NDgyNzdlZTA4ZWVkZGNlMDUzZDA1ZTJiZTQ4ZDY0YTM).\n'