b'# Machine Learning Refined: Notes, Exercises, and Jupyter notebooks [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Machine%20Learning%20Refined:%20notes,%20exercises,%20and%20Jupyter%20notebooks%20&url=https://github.com/jermwatt/machine_learning_refined)\n\n<img align="right" src="html/gifs/mlrefined_ed1_cover.jpeg" height="420">\n<strong>Publisher:</strong> Cambridge University Press <br><br>\n<strong>First edition:</strong> November 2016 <br>\n<strong>Second edition:</strong> January 2020 (expected) <br><br><br>\n\n\n\n\n# Table of contents\n\n- [A little sampler first](#a-little-sampler-first)\n- [What is in this book?](#what-is-in-this-book)\n- [Who is this book for?](#who-is-this-book-for)\n- [What is in the repo?](#what-is-in-the-repo)\n- [Notes](#notes)\n- [Installation](#installation)\n- [Creators](#creators)\n\n<br><br><br>\n\n## A little sampler first\n\n[(Back to top)](#table-of-contents)\n\nMany machine learning concepts - like convergence of an algorithm, evolution of a model from an underfitting one all the way to an overfitting model, etc. - can be illustrated and intuited best using animations (as opposed to static figures). You\'ll find a large number of both images and animated videos here - which you can modify yourself too via the raw Jupyter notebook version of these notes.  Here are just a few examples:<br><br>\n\n<img src="html/gifs/cross_validation_regression.gif" width="300px" height="auto"> | <img src="html/gifs/cross_validation_two_class.gif" width="300px" height="auto"> | <img src="html/gifs/cross_validation_multiclass.gif" width="300px" height="auto"> \n---|---|---\nCross-validation (regression) | Cross-validation (two-class classification) | Cross-validation (multi-class classification)\n\n<br><br>\n\n<img src="html/gifs/Kmeans.gif" width="300px" height="auto"> | <img src="html/gifs/feature_normalization.gif" width="300px" height="auto"> | <img src="html/gifs/normalized_gradient_descent.gif" width="300px" height="auto"> \n---|---|---\nK-means clustering | Feature normalization| Normalized gradient descent\n\n<br><br>\n\n<img src="html/gifs/Rotation.gif" width="300px" height="auto"> | <img src="html/gifs/convexification.gif" width="300px" height="auto"> | <img src="html/gifs/Nurgetson.gif" width="300px" height="auto"> \n---|---|---\nRotation | Convexification | Dogification!\n\n<br><br>\n\n<img src="html/gifs/nonlinear_transformation.gif" width="300px" height="auto"> | <img src="html/gifs/weighted_classification.gif" width="300px" height="auto"> | <img src="html/gifs/moving_average.gif" width="300px" height="auto"> \n---|---|---\nA nonlinear transformation | Weighted classification | The moving average\n\n<br><br>\n\n<img src="html/gifs/batch_normalization.gif" width="450px" height="auto"> | <img src="html/gifs/logistic_regression.gif" width="450px" height="auto"> \n---|---\nBatch normalization | Logistic regression\n\n<br><br>\n\n<img src="html/gifs/poly_vs_NN_vs_trees_regression.gif" width="450px" height="auto"> | <img src="html/gifs/poly_vs_NN_vs_trees_classification.gif" width="450px" height="auto"> \n---|---\nPolynomials vs. NNs vs. Trees (regression) | Polynomials vs. NNs vs. Trees (classification)\n\n<br><br>\n\n<img src="html/gifs/steplength_1D.gif" width="450px" height="auto"> | <img src="html/gifs/steplength_2D.gif" width="450px" height="auto"> \n---|---\nChanging gradient descent\'s steplength (1d) | Changing gradient descent\'s steplength (2d)\n\n<br><br>\n\n<img src="html/gifs/convex_combination.gif" width="450px" height="auto"> | <img src="html/gifs/taylor_series.gif" width="450px" height="auto"> \n---|---\nConvex combination of two functions | Taylor series approximation\n\n<br><br>\n\n<img src="html/gifs/feature_selection.gif" width="450px" height="auto"> | <img src="html/gifs/secant_2d.gif" width="450px" height="auto"> \n---|---\nFeature selection via regularization | Secant planes\n\n<br><br>\n\n<img src="html/gifs/function_approx_NN.gif" width="450px" height="auto"> | <img src="html/gifs/regression_tree.gif" width="450px" height="auto"> \n---|---\nFunction approximation with a neural network | A regression tree\n\n<br><br><br>\n## What is in this book?\n\n[(Back to top)](#table-of-contents)\n\nWe believe that understanding machine learning is impossible without having a firm grasp of its underlying mathematical machiney. But we also believe that the bulk of learning the subject takes place when learners "get their hands dirty" and code things up for themselves. **That\'s why in this book we discuss both how to derive machine learnig models mathematically and how to implement them from scratch**  (using `numpy`, `matplotlib`, and `autograd` libraries) - and yes, this includes multi-layer neural networks as well!\n<br><br><br>\n\n\n## Who is this book for?\n\n[(Back to top)](#table-of-contents)\n\nThis text aims to bridge the existing gap between **practicality** and **rigor** in machine learning education, in a market saturated with books that are either mathematically rigorous but not practical, or vice versa. Conventional textbooks usually place little to no emphasis on coding, leaving the reader struggling to put what they learned into practice. On the other hand the more hands-on books in the market typically lack rigor, leaving machine learning a \'black box\' to the reader.\n\nIf you\'re looking for a practical yet rigorous treatment of machine learning, then this book is for you. \n<br><br><br>\n\n\n## What is in the repo?\n\n[(Back to top)](#table-of-contents)\n\n### 1. Interatcive html notes\nThese notes - listed [here](#notes) - served as an early draft for the second edition of the text. You can also find them in the `notes` directory. Here\'s an example: <br><br>\n\n<p align="center"><img src="html/gifs/html.gif" width="70%" height="auto"></p>\n<br>\n\n### 2. Accompanying Jupyter notebooks (used to create the html notes) \nFeel free to take a peek under the hood, tweak the models, explore new datasets, etc. Here\'s an example: <br><br>\n\n<p align="center"><img src="html/gifs/ipynb.gif" width="65%" height="auto"></p>\n<br>\n\n### 3. Coding exercises (1st edition)\n\nIn the `exercises` directory you can find starting wrappers for coding exercises from the first edition of the text in `Python` and `MATLAB`. Exercises for the 2nd edition will be added soon.\n<br><br><br>\n\n\n## Notes\n[(Back to top)](#table-of-contents)\n\n### Chapter 2: Zero order / derivative free optimization\n\n[2.1  Introduction](https://jermwatt.github.io/machine_learning_refined/notes/2_Zero_order_methods/2_1_Introduction.html)  \n[2.2 Zero order optimiality conditions](https://jermwatt.github.io/machine_learning_refined/notes/2_Zero_order_methods/2_2_Zero.html)  \n[2.3 Global optimization](https://jermwatt.github.io/machine_learning_refined/notes/2_Zero_order_methods/2_3_Global.html)    \n[2.4 Local optimization techniques](https://jermwatt.github.io/machine_learning_refined/notes/2_Zero_order_methods/2_4_Local.html)   \n[2.5 Random search methods](https://jermwatt.github.io/machine_learning_refined/notes/2_Zero_order_methods/2_5_Random.html)  \n[2.6 Coordinate search and descent](https://jermwatt.github.io/machine_learning_refined/notes/2_Zero_order_methods/2_6_Coordinate.html)\n\n \n### Chapter 3: First order optimization methods\n\n[3.1 Introduction](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_1_Introduction.html)  \n[3.2 The first order optimzliaty condition](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_2_First.html)       \n[3.3 The anatomy of lines and hyperplanes](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_3_Hyperplane.html)    \n[3.4 The anatomy of first order Taylor series approximations](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_4_Tangent.html)     \n[3.5 Automatic differentiation and autograd](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_5_Automatic.html)     \n[3.6 Gradient descent](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_6_Descent.html)   \n[3.7 Two problems with the negative gradient direction](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_7_Problems.html)   \n[3.8 Momentum acceleration](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_8_Momentum.html)   \n[3.9 Normalized gradient descent procedures](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_9_Normalized.html)   \n[3.10 Advanced first order methods](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_10_Advanced.html)   \n[3.11 Mini-batch methods](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_11_Minibatch.html)   \n[3.12 Conservative steplength rules](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_12_Conservative.html)  \n\n### Chapter 4: Second order optimization methods\n\n4.1  Introduction <br>\n[4.2  The anatomy of quadratic functions](https://jermwatt.github.io/machine_learning_refined/notes/4_Second_order_methods/4_2_Quadratic.html)   \n[4.3 Curvature and the second order optimality condition](https://jermwatt.github.io/machine_learning_refined/notes/4_Second_order_methods/4_3_Second.html)   \n[4.4 Newton\'s method](https://jermwatt.github.io/machine_learning_refined/notes/4_Second_order_methods/4_4_Newtons.html)   \n[4.5 Two fundamental problems with Newton\'s method](https://jermwatt.github.io/machine_learning_refined/notes/4_Second_order_methods/4_5_Problems.html)   \n4.6 Quasi-newton\'s methods \n\n### Chapter 5: Linear regression\n\n5.1 Introduction <br>\n[5.2 Least squares regression](https://jermwatt.github.io/machine_learning_refined/notes/5_Linear_regression/5_2_Least.html)   \n[5.3 Least absolute deviations](https://jermwatt.github.io/machine_learning_refined/notes/5_Linear_regression/5_3_Absolute.html)   \n[5.4 Regression metrics](https://jermwatt.github.io/machine_learning_refined/notes/5_Linear_regression/5_4_Metrics.html)   \n[5.5 Weighted regression](https://jermwatt.github.io/machine_learning_refined/notes/5_Linear_regression/5_5_Weighted.html)   \n[5.6 Multi-output regression](https://jermwatt.github.io/machine_learning_refined/notes/5_Linear_regression/5_6_Multi.html)  \n\n### Chapter 6: Linear two-class classification\n\n6.1 Introduction <br>\n[6.2 Logistic regression and the cross-entropy cost](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_2_Cross_entropy.html)   \n[6.3 Logistic regression and the softmax cost](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_3_Softmax.html)   \n[6.4 The perceptron](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_4_Perceptron.html)   \n[6.5 Support vector machines](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_5_SVMs.html)   \n[6.6 Categorical labels](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_6_Categorical.html)   \n[6.7 Comparing two-class schemes](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_7_Comparison.html)   \n[6.8 Quality metrics](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_8_Metrics.html)   \n[6.9 Weighted two-class classification](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_9_Weighted.html)  \n\n### Chapter 7: Linear multi-class classification\n\n7.1 Introduction <br>\n[7.2 One-versus-All classification](https://jermwatt.github.io/machine_learning_refined/notes/7_Linear_multiclass_classification/7_2_OvA.html)   \n[7.3 The multi-class perceptron](https://jermwatt.github.io/machine_learning_refined/notes/7_Linear_multiclass_classification/7_3_Perceptron.html)   \n[7.4 Comparing multi-class schemes](https://jermwatt.github.io/machine_learning_refined/notes/7_Linear_multiclass_classification/7_4_Comparison.html)   \n[7.5 The categorical cross-entropy cost](https://jermwatt.github.io/machine_learning_refined/notes/7_Linear_multiclass_classification/7_5_Categorical.html)   \n[7.6 Multi-class quality metrics](https://jermwatt.github.io/machine_learning_refined/notes/7_Linear_multiclass_classification/7_6_Metrics.html)  \n\n\n### Chapter 8: Unsupervised learning\n\n8.1 Introduction <br>\n[8.2 Spanning sets and vector algebra](https://jermwatt.github.io/machine_learning_refined/notes/8_Linear_unsupervised_learning/8_2_Spanning.html)   \n[8.3 Learning proper spanning sets](https://jermwatt.github.io/machine_learning_refined/notes/8_Linear_unsupervised_learning/8_3_PCA.html)   \n[8.4 The linear Autoencoder](https://jermwatt.github.io/machine_learning_refined/notes/8_Linear_unsupervised_learning/8_4_Autoencoder.html)   \n[8.5 The class PCA solution](https://jermwatt.github.io/machine_learning_refined/notes/8_Linear_unsupervised_learning/8_5_Classic.html)   \n[8.6 Recommender systems](https://jermwatt.github.io/machine_learning_refined/notes/8_Linear_unsupervised_learning/8_6_Recommender.html)  \n[8.7 K-means clustering](https://jermwatt.github.io/machine_learning_refined/notes/8_Linear_unsupervised_learning/8_7_Kmeans.html)   \n[8.8 Matrix factorization techniques](https://jermwatt.github.io/machine_learning_refined/notes/8_Linear_unsupervised_learning/8_8_Factorization.html)  \n\n### Chapter 9: Principles of feature selection and engineering\n\n9.1 Introduction <br>\n[9.2 Histogram-based features](https://jermwatt.github.io/machine_learning_refined/notes/9_Feature_engineer_select/9_2_Histogram.html)   \n[9.3 Standard normalization and feature scaling](https://jermwatt.github.io/machine_learning_refined/notes/9_Feature_engineer_select/9_3_Scaling.html)   \n[9.4 Imputing missing values](https://jermwatt.github.io/machine_learning_refined/notes/9_Feature_engineer_select/9_4_Cleaning.html)   \n[9.5 PCA-sphereing](https://jermwatt.github.io/machine_learning_refined/notes/9_Feature_engineer_select/9_5_PCA_sphereing.html)   \n[9.6 Feature selection via boosting](https://jermwatt.github.io/machine_learning_refined/notes/9_Feature_engineer_select/9_6_Boosting.html)   \n[9.7 Feature selection via regularization](https://jermwatt.github.io/machine_learning_refined/notes/9_Feature_engineer_select/9_7_Regularization.html)  \n\n### Chapter 10: Introduction to nonlinear learning\n\n10.1 Introduction <br>\n[10.2 Nonlinear regression](https://jermwatt.github.io/machine_learning_refined/notes/10_Nonlinear_intro/10_2_Regression.html)  \n[10.3 Nonlinear multi-output regression](https://jermwatt.github.io/machine_learning_refined/notes/10_Nonlinear_intro/10_3_MultReg.html)  \n[10.4 Nonlinear two-class classification](https://jermwatt.github.io/machine_learning_refined/notes/10_Nonlinear_intro/10_4_Twoclass.html)  \n[10.5 Nonlinear multi-class classification](https://jermwatt.github.io/machine_learning_refined/notes/10_Nonlinear_intro/10_5_Multiclass.html)  \n[10.6 Nonlinear unsupervised learning](https://jermwatt.github.io/machine_learning_refined/notes/10_Nonlinear_intro/10_6_Unsupervised.html)  \n\n### Chapter 11: Principles of feature learning\n\n[11.1 Introduction](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_1_Introduction.html) <br>\n[11.2 Universal approximators](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_2_Universal.html) <br>\n[11.3 Universal approximation of real data](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_3_Real_approximation.html)  <br>\n[11.4 Naive cross-validation](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_4_Cross_validation.html)  <br>\n[11.5 Efficient cross-validation via boosting](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_5_Boosting.html)   <br>\n[11.6 Efficient cross-validation via regularization](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_6_Regularization.html)  <br>\n11.7 Testing data <br>\n11.8 Which universal approximator works best in practice? <br>\n[11.9 Bagging cross-validated models](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_9_Bagging.html) <br>\n[11.10 K-folds cross-validation](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_10_Kfolds.html)  <br>\n11.11 When feature learning fails <br>\n11.12 Conclusion <br>\n\n### Chapter 12: Kernels\n\n12.1 Introduction <br>\n12.2 The variety of kernel-based learners <br>\n12.3 The kernel trick  <br>\n12.4 Kernels as similarity measures<br> \n12.5 Scaling kernels <br>\n  \n### Chapter 13: Fully connected networks\n\n13.1 Introduction <br>\n[13.2 Fully connected networks](https://jermwatt.github.io/machine_learning_refined/notes/13_Multilayer_perceptrons/13_2_Multi_layer_perceptrons.html)  \n[13.3 Optimization issues](https://jermwatt.github.io/machine_learning_refined/notes/13_Multilayer_perceptrons/13_3_Optimization.html)  \n13.4 Activation functions  \n13.5 Backpropogation  \n[13.6 Batch normalization](https://jermwatt.github.io/machine_learning_refined/notes/13_Multilayer_perceptrons/13_6_Batch_normalization.html)  \n[13.7 Early-stopping](https://jermwatt.github.io/machine_learning_refined/notes/13_Multilayer_perceptrons/13_7_early_stopping.html)  \n\n### Chapter 14: Tree-based learners\n\n14.1 Introduction <br>\n14.2 Varieties of tree-based learners <br>\n14.3 Regression trees  <br>\n14.4 Classification trees  <br>\n14.5 Gradient boosting  <br>\n14.6 Random forests  <br>\n14.7 Cross-validating individual trees <br><br><br>\n\n\n### Chapter 15: Derivatives and Automatic Differentiation\n\n15.1 Introduction <br>\n15.2 The derivative <br>\n15.3 Derivative rules for elementary functions and operations  <br>\n15.4 The gradient <br>\n15.5 The computation graph  <br>\n15.6 The forward mode of automatic differentiation  <br>\n15.7 The reverse mode of automatic differentiation <br>\n15.8 Using the Autograd library <br>\n15.9 Higher order derivatives <br>\n15.10 Taylor series <br><br><br>\n\n\n### Chapter 16: Linear algebra\n\n16.1 Introduction <br>\n[16.2 Vectors and vector operations](https://jermwatt.github.io/machine_learning_refined/notes/16_Linear_algebra/16_2_Vectors.html)    \n[16.3 Matrices and matrix operations](https://jermwatt.github.io/machine_learning_refined/notes/16_Linear_algebra/16_3_Matrices.html)  \n[16.4 Eigenvalues and eigenvectors](https://jermwatt.github.io/machine_learning_refined/notes/16_Linear_algebra/16_4_Eigen.html)  \n[16.5 Vector and matrix norms](https://jermwatt.github.io/machine_learning_refined/notes/16_Linear_algebra/16_5_Norms.html)\n\n\n## Installation\n[(Back to top)](#table-of-contents)\n\nTo successfully run the Jupyter notebooks contained in this repo we highly recommend downloading the [Anaconda Python 3 distribution](https://www.anaconda.com/download/#macos). Many of these notebooks also employ the Automatic Differentiator [autograd](https://github.com/HIPS/autograd) which can be installed by typing the following command at your terminal\n      \n      pip install autograd\n      \nWith minor adjustment users can also run these notebooks using the GPU/TPU extended version of autograd [JAX](https://github.com/google/jax).<br><br><br>\n\n\n## Creators \n\n[(Back to top)](#table-of-contents)\n\nThis repository is in active development by [Jeremy Watt](mailto:jeremy@dgsix.com) and [Reza Borhani](mailto:reza@dgsix.com) - please do not hesitate to reach out with comments, questions, typos, etc.\n'