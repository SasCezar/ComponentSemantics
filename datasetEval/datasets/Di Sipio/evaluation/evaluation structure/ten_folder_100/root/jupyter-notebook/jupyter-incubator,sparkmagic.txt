b'[![Build Status](https://travis-ci.org/jupyter-incubator/sparkmagic.svg?branch=master)](https://travis-ci.org/jupyter-incubator/sparkmagic) [![Join the chat at https://gitter.im/sparkmagic/Lobby](https://badges.gitter.im/sparkmagic/Lobby.svg)](https://gitter.im/sparkmagic/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n# sparkmagic\n\nSparkmagic is a set of tools for interactively working with remote Spark clusters through [Livy](https://livy.incubator.apache.org/), a Spark REST server, in [Jupyter](http://jupyter.org) notebooks.\nThe Sparkmagic project includes a set of magics for interactively running Spark code in multiple languages, as well as some kernels that you can use to turn Jupyter into an integrated Spark environment.\n\n![Automatic SparkContext and SQLContext creation](screenshots/sparkcontext.png)\n\n![Automatic visualization](screenshots/autoviz.png)\n\n![Server-side visualization](screenshots/matplotlib.png)\n\n![Help](screenshots/help.png)\n\n## Features\n\n* Run Spark code in multiple languages against any remote Spark cluster through Livy\n* Automatic SparkContext (`sc`) and HiveContext (`sqlContext`) creation\n* Easily execute SparkSQL queries with the `%%sql` magic\n* Automatic visualization of SQL queries in the PySpark, Spark and SparkR kernels; use an easy visual interface to interactively construct visualizations, no code required\n* Easy access to Spark application information and logs (`%%info` magic)\n* Ability to capture the output of SQL queries as Pandas dataframes to interact with other Python libraries (e.g. matplotlib)\n* Send local files or dataframes to a remote cluster (e.g. sending pretrained local ML model straight to the Spark cluster)\n* Authenticate to Livy via Basic Access authentication or via Kerberos\n\n## Examples\n\nThere are two ways to use sparkmagic. Head over to the [examples](examples) section for a demonstration on how to use both models of execution.\n\n### 1. Via the IPython kernel\n\nThe sparkmagic library provides a %%spark magic that you can use to easily run code against a remote Spark cluster from a normal IPython notebook. See the [Spark Magics on IPython sample notebook](examples/Magics%20in%20IPython%20Kernel.ipynb)\n\n### 2. Via the PySpark and Spark kernels\n\nThe sparkmagic library also provides a set of Scala and Python kernels that allow you to automatically connect to a remote Spark cluster, run code and SQL queries, manage your Livy server and Spark job configuration, and generate automatic visualizations.\nSee [Pyspark](examples/Pyspark%20Kernel.ipynb) and [Spark](examples/Spark%20Kernel.ipynb) sample notebooks.\n\n### 3. Sending data to Spark%20Kernel\n\nSee the [sending data to Spark notebook](examples/Send local data to Spark.ipynb).\n\n## Installation\n\n1. Install the library\n\n        pip install sparkmagic\n\n2. Make sure that ipywidgets is properly installed by running\n\n        jupyter nbextension enable --py --sys-prefix widgetsnbextension \n \n3. If you\'re using JupyterLab, you\'ll need to run another command:\n\n        jupyter labextension install @jupyter-widgets/jupyterlab-manager\n\n4. (Optional) Install the wrapper kernels. Do `pip show sparkmagic` and it will show the path where `sparkmagic` is installed at. `cd` to that location and do:\n\n        jupyter-kernelspec install sparkmagic/kernels/sparkkernel\n        jupyter-kernelspec install sparkmagic/kernels/pysparkkernel\n        jupyter-kernelspec install sparkmagic/kernels/sparkrkernel\n        \n5. (Optional) Modify the configuration file at ~/.sparkmagic/config.json. Look at the [example_config.json](sparkmagic/example_config.json)\n\n6. (Optional) Enable the server extension so that clusters can be programatically changed:\n\n        jupyter serverextension enable --py sparkmagic\n\n## Authentication Methods\n\nSparkmagic supports:\n\n* No auth\n* Basic authentication\n* Kerberos\n\nKerberos support is implemented via the [requests-kerberos](https://github.com/requests/requests-kerberos) package. Sparkmagic expects a kerberos ticket to be available in the system. Requests-kerberos will pick up the kerberos ticket from a cache file. For the ticket to be available, the user needs to have run [kinit](https://web.mit.edu/kerberos/krb5-1.12/doc/user/user_commands/kinit.html) to create the kerberos ticket.\n\nCurrently, sparkmagic does not support passing a kerberos principal/token, but we welcome pull requests.\n\n## Papermill\n\nIf you want Papermill rendering to stop on a Spark error, edit the `~/.sparkmagic/config.json` with the following settings:\n\n```json\n{\n    "shutdown_session_on_spark_statement_errors": true,\n    "all_errors_are_fatal": true\n}\n```\n\n## Docker\n\nThe included `docker-compose.yml` file will let you spin up a full\nsparkmagic stack that includes a Jupyter notebook with the appropriate\nextensions installed, and a Livy server backed by a local-mode Spark instance.\n(This is just for testing and developing sparkmagic itself; in reality,\nsparkmagic is not very useful if your Spark instance is on the same machine!)\n\nIn order to use it, make sure you have [Docker](https://docker.com) and\n[Docker Compose](https://docs.docker.com/compose/) both installed, and\nthen simply run:\n\n    docker-compose build\n    docker-compose up\n\nYou will then be able to access the Jupyter notebook in your browser at\nhttp://localhost:8888. Inside this notebook, you can configure a\nsparkmagic endpoint at http://spark:8998. This endpoint is able to\nlaunch both Scala and Python sessions. You can also choose to start a\nwrapper kernel for Scala, Python, or R from the list of kernels.\n\nTo shut down the containers, you can interrupt `docker-compose` with\n`Ctrl-C`, and optionally remove the containers with `docker-compose\ndown`.\n\nIf you are developing sparkmagic and want to test out your changes in\nthe Docker container without needing to push a version to PyPI, you can\nset the `dev_mode` build arg in `docker-compose.yml` to `true`, and then\nre-build the container. This will cause the container to install your\nlocal version of autovizwidget, hdijupyterutils, and sparkmagic. Make\nsure to re-run `docker-compose build` before each test run.\n\n## Server extension API\n\n### `/reconnectsparkmagic`:\n* `POST`:\nAllows to specify Spark cluster connection information to a notebook passing in the notebook path and cluster information.\nKernel will be started/restarted and connected to cluster specified.\n\nRequest Body example:\n        ```\n        {\n                \'path\': \'path.ipynb\',\n                \'username\': \'username\',\n                \'password\': \'password\',\n                \'endpoint\': \'url\',\n                \'auth\': \'Kerberos\',\n                \'kernelname\': \'pysparkkernel\'\n        }\n        ```\n\n*Note that the auth can be either None, Basic_Access or Kerberos based on the authentication enabled in livy. The kernelname parameter is optional and defaults to the one specified on the config file or pysparkkernel if not on the config file.*\nReturns `200` if successful; `400` if body is not JSON string or key is not found; `500` if error is encountered changing clusters.\n\nReply Body example:\n        ```\n        {\n                \'success\': true,\n                \'error\': null\n        }\n        ```\n\n## Architecture\n\nSparkmagic uses Livy, a REST server for Spark, to remotely execute all user code. \nThe library then automatically collects the output of your code as plain text or a JSON document, displaying the results to you as formatted text or as a Pandas dataframe as appropriate.\n\n![Architecture](screenshots/diagram.png)\n\nThis architecture offers us some important advantages:\n\n1. Run Spark code completely remotely; no Spark components need to be installed on the Jupyter server\n\n2. Multi-language support; the Python, Python3, Scala and R kernels are equally feature-rich, and adding support for more languages will be easy\n\n3. Support for multiple endpoints; you can use a single notebook to start multiple Spark jobs in different languages and against different remote clusters\n\n4. Easy integration with any Python library for data science or visualization, like Pandas or [Plotly](https://plot.ly/python/offline)\n\nHowever, there are some important limitations to note:\n\n1. Some overhead added by sending all code and output through Livy\n\n2. Since all code is run on a remote driver through Livy, all structured data must be serialized to JSON and parsed by the Sparkmagic library so that it can be manipulated and visualized on the client side.\nIn practice this means that you must use Python for client-side data manipulation in `%%local` mode.\n\n## Contributing\n\nWe welcome contributions from everyone. \nIf you\'ve made an improvement to our code, please send us a [pull request](https://github.com/jupyter-incubator/sparkmagic/pulls).\n\nTo dev install, execute the following:\n\n        git clone https://github.com/jupyter-incubator/sparkmagic\n        pip install -e hdijupyterutils \n        pip install -e autovizwidget\n        pip install -e sparkmagic\n        \nand optionally follow steps 3 and 4 above.\n\nTo run unit tests, run:\n\n        nosetests hdijupyterutils autovizwidget sparkmagic\n\nIf you want to see an enhancement made but don\'t have time to work on it yourself, feel free to submit an [issue](https://github.com/jupyter-incubator/sparkmagic/issues) for us to deal with.\n'