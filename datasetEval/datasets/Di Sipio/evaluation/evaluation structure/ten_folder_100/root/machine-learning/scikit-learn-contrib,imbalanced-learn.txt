b'.. -*- mode: rst -*-\n\n.. _scikit-learn: http://scikit-learn.org/stable/\n\n.. _scikit-learn-contrib: https://github.com/scikit-learn-contrib\n\n|Azure|_ |Travis|_ |AppVeyor|_ |Codecov|_ |CircleCI|_ |PythonVersion|_ |Pypi|_ |Gitter|_\n\n.. |Azure| image:: https://dev.azure.com/imbalanced-learn/imbalanced-learn/_apis/build/status/scikit-learn-contrib.imbalanced-learn?branchName=master\n.. _Azure: https://dev.azure.com/imbalanced-learn/imbalanced-learn/_build\n\n.. |Travis| image:: https://travis-ci.org/scikit-learn-contrib/imbalanced-learn.svg?branch=master\n.. _Travis: https://travis-ci.org/scikit-learn-contrib/imbalanced-learn\n\n.. |AppVeyor| image:: https://ci.appveyor.com/api/projects/status/c8w4xb7re4euntvi/branch/master?svg=true\n.. _AppVeyor: https://ci.appveyor.com/project/glemaitre/imbalanced-learn/history\n\n.. |Codecov| image:: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn/branch/master/graph/badge.svg\n.. _Codecov: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn.svg?style=shield&circle-token=:circle-token\n.. _CircleCI: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn/tree/master\n\n.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg\n.. _PythonVersion: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg\n\n.. |Pypi| image:: https://badge.fury.io/py/imbalanced-learn.svg\n.. _Pypi: https://badge.fury.io/py/imbalanced-learn\n\n.. |Gitter| image:: https://badges.gitter.im/scikit-learn-contrib/imbalanced-learn.svg\n.. _Gitter: https://gitter.im/scikit-learn-contrib/imbalanced-learn?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n\nimbalanced-learn\n================\n\nimbalanced-learn is a python package offering a number of re-sampling techniques\ncommonly used in datasets showing strong between-class imbalance.\nIt is compatible with scikit-learn_ and is part of scikit-learn-contrib_\nprojects.\n\nDocumentation\n-------------\n\nInstallation documentation, API documentation, and examples can be found on the\ndocumentation_.\n\n.. _documentation: https://imbalanced-learn.org/stable/\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nimbalanced-learn is tested to work under Python 3.6+.\nThe dependency requirements are based on the last scikit-learn release:\n\n* scipy(>=0.17)\n* numpy(>=1.11)\n* scikit-learn(>=0.21)\n* joblib(>=0.11)\n* keras 2 (optional)\n* tensorflow (optional)\n\nAdditionally, to run the examples, you need matplotlib(>=2.0.0) and\npandas(>=0.22).\n\nInstallation\n~~~~~~~~~~~~\n\nimbalanced-learn is currently available on the PyPi\'s repository and you can\ninstall it via `pip`::\n\n  pip install -U imbalanced-learn\n\nThe package is release also in Anaconda Cloud platform::\n\n  conda install -c conda-forge imbalanced-learn\n\nIf you prefer, you can clone it and run the setup.py file. Use the following\ncommands to get a copy from GitHub and install all dependencies::\n\n  git clone https://github.com/scikit-learn-contrib/imbalanced-learn.git\n  cd imbalanced-learn\n  pip install .\n\nOr install using pip and GitHub::\n\n  pip install -U git+https://github.com/scikit-learn-contrib/imbalanced-learn.git\n\nTesting\n~~~~~~~\n\nAfter installation, you can use `pytest` to run the test suite::\n\n  make coverage\n\nDevelopment\n-----------\n\nThe development of this scikit-learn-contrib is in line with the one\nof the scikit-learn community. Therefore, you can refer to their\n`Development Guide\n<http://scikit-learn.org/stable/developers>`_.\n\nAbout\n-----\n\nIf you use imbalanced-learn in a scientific publication, we would appreciate\ncitations to the following paper::\n\n  @article{JMLR:v18:16-365,\n  author  = {Guillaume  Lema{{\\^i}}tre and Fernando Nogueira and Christos K. Aridas},\n  title   = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},\n  journal = {Journal of Machine Learning Research},\n  year    = {2017},\n  volume  = {18},\n  number  = {17},\n  pages   = {1-5},\n  url     = {http://jmlr.org/papers/v18/16-365}\n  }\n\nMost classification algorithms will only perform optimally when the number of\nsamples of each class is roughly the same. Highly skewed datasets, where the\nminority is heavily outnumbered by one or more classes, have proven to be a\nchallenge while at the same time becoming more and more common.\n\nOne way of addressing this issue is by re-sampling the dataset as to offset this\nimbalance with the hope of arriving at a more robust and fair decision boundary\nthan you would otherwise.\n\nRe-sampling techniques are divided in two categories:\n    1. Under-sampling the majority class(es).\n    2. Over-sampling the minority class.\n    3. Combining over- and under-sampling.\n    4. Create ensemble balanced sets.\n\nBelow is a list of the methods currently implemented in this module.\n\n* Under-sampling\n    1. Random majority under-sampling with replacement\n    2. Extraction of majority-minority Tomek links [1]_\n    3. Under-sampling with Cluster Centroids\n    4. NearMiss-(1 & 2 & 3) [2]_\n    5. Condensed Nearest Neighbour [3]_\n    6. One-Sided Selection [4]_\n    7. Neighboorhood Cleaning Rule [5]_\n    8. Edited Nearest Neighbours [6]_\n    9. Instance Hardness Threshold [7]_\n    10. Repeated Edited Nearest Neighbours [14]_\n    11. AllKNN [14]_\n\n* Over-sampling\n    1. Random minority over-sampling with replacement\n    2. SMOTE - Synthetic Minority Over-sampling Technique [8]_\n    3. SMOTENC - SMOTE for Nominal Continuous [8]_\n    4. bSMOTE(1 & 2) - Borderline SMOTE of types 1 and 2 [9]_\n    5. SVM SMOTE - Support Vectors SMOTE [10]_\n    6. ADASYN - Adaptive synthetic sampling approach for imbalanced learning [15]_\n    7. KMeans-SMOTE [17]_\n\n* Over-sampling followed by under-sampling\n    1. SMOTE + Tomek links [12]_\n    2. SMOTE + ENN [11]_\n\n* Ensemble classifier using samplers internally\n    1. Easy Ensemble classifier [13]_\n    2. Balanced Random Forest [16]_\n    3. Balanced Bagging\n    4. RUSBoost [18]_\n\n* Mini-batch resampling for Keras and Tensorflow\n\nThe different algorithms are presented in the sphinx-gallery_.\n\n.. _sphinx-gallery: https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/index.html\n\n\nReferences:\n-----------\n\n.. [1] : I. Tomek, \xe2\x80\x9cTwo modifications of CNN,\xe2\x80\x9d IEEE Transactions on Systems, Man, and Cybernetics, vol. 6, pp. 769-772, 1976.\n\n.. [2] : I. Mani, J. Zhang. \xe2\x80\x9ckNN approach to unbalanced data distributions: A case study involving information extraction,\xe2\x80\x9d In Proceedings of the Workshop on Learning from Imbalanced Data Sets, pp. 1-7, 2003.\n\n.. [3] : P. E. Hart, \xe2\x80\x9cThe condensed nearest neighbor rule,\xe2\x80\x9d IEEE Transactions on Information Theory, vol. 14(3), pp. 515-516, 1968.\n\n.. [4] : M. Kubat, S. Matwin, \xe2\x80\x9cAddressing the curse of imbalanced training sets: One-sided selection,\xe2\x80\x9d In Proceedings of the 14th International Conference on Machine Learning, vol. 97, pp. 179-186, 1997.\n\n.. [5] : J. Laurikkala, \xe2\x80\x9cImproving identification of difficult small classes by balancing class distribution,\xe2\x80\x9d Proceedings of the 8th Conference on Artificial Intelligence in Medicine in Europe, pp. 63-66, 2001.\n\n.. [6] : D. Wilson, \xe2\x80\x9cAsymptotic Properties of Nearest Neighbor Rules Using Edited Data,\xe2\x80\x9d IEEE Transactions on Systems, Man, and Cybernetrics, vol. 2(3), pp. 408-421, 1972.\n\n.. [7] : M. R. Smith, T. Martinez, C. Giraud-Carrier, \xe2\x80\x9cAn instance level analysis of data complexity,\xe2\x80\x9d Machine learning, vol. 95(2), pp. 225-256, 2014.\n\n.. [8] : N. V. Chawla, K. W. Bowyer, L. O. Hall, W. P. Kegelmeyer, \xe2\x80\x9cSMOTE: Synthetic minority over-sampling technique,\xe2\x80\x9d Journal of Artificial Intelligence Research, vol. 16, pp. 321-357, 2002.\n\n.. [9] : H. Han, W.-Y. Wang, B.-H. Mao, \xe2\x80\x9cBorderline-SMOTE: A new over-sampling method in imbalanced data sets learning,\xe2\x80\x9d In Proceedings of the 1st International Conference on Intelligent Computing, pp. 878-887, 2005.\n\n.. [10] : H. M. Nguyen, E. W. Cooper, K. Kamei, \xe2\x80\x9cBorderline over-sampling for imbalanced data classification,\xe2\x80\x9d In Proceedings of the 5th International Workshop on computational Intelligence and Applications, pp. 24-29, 2009.\n\n.. [11] : G. E. A. P. A. Batista, R. C. Prati, M. C. Monard, \xe2\x80\x9cA study of the behavior of several methods for balancing machine learning training data,\xe2\x80\x9d ACM Sigkdd Explorations Newsletter, vol. 6(1), pp. 20-29, 2004.\n\n.. [12] : G. E. A. P. A. Batista, A. L. C. Bazzan, M. C. Monard, \xe2\x80\x9cBalancing training data for automated annotation of keywords: A case study,\xe2\x80\x9d In Proceedings of the 2nd Brazilian Workshop on Bioinformatics, pp. 10-18, 2003.\n\n.. [13] : X.-Y. Liu, J. Wu and Z.-H. Zhou, \xe2\x80\x9cExploratory undersampling for class-imbalance learning,\xe2\x80\x9d IEEE Transactions on Systems, Man, and Cybernetics, vol. 39(2), pp. 539-550, 2009.\n\n.. [14] : I. Tomek, \xe2\x80\x9cAn experiment with the edited nearest-neighbor rule,\xe2\x80\x9d IEEE Transactions on Systems, Man, and Cybernetics, vol. 6(6), pp. 448-452, 1976.\n\n.. [15] : H. He, Y. Bai, E. A. Garcia, S. Li, \xe2\x80\x9cADASYN: Adaptive synthetic sampling approach for imbalanced learning,\xe2\x80\x9d In Proceedings of the 5th IEEE International Joint Conference on Neural Networks, pp. 1322-1328, 2008.\n\n.. [16] : C. Chao, A. Liaw, and L. Breiman. "Using random forest to learn imbalanced data." University of California, Berkeley 110 (2004): 1-12.\n\n.. [17] : Felix Last, Georgios Douzas, Fernando Bacao, "Oversampling for Imbalanced Learning Based on K-Means and SMOTE"\n\n.. [18] : Seiffert, C., Khoshgoftaar, T. M., Van Hulse, J., & Napolitano, A. "RUSBoost: A hybrid approach to alleviating class imbalance." IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans 40.1 (2010): 185-197.'