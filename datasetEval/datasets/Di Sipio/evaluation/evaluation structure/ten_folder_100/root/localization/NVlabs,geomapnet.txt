b'[![License CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC4.0-blue.svg)](https://raw.githubusercontent.com/NVIDIA/FastPhotoStyle/master/LICENSE.md)\n![Python 2.7](https://img.shields.io/badge/python-2.7-green.svg)\n# Geometry-Aware Learning of Maps for Camera Localization \n\n## License\n\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode). \n\n## Documentation \n\nThis is the PyTorch implementation of our CVPR 2018 paper\n\n"[Geometry-Aware Learning of Maps for Camera Localization](https://arxiv.org/abs/1712.03342)" - CVPR 2018 (Spotlight). [Samarth Brahmbhatt](https://samarth-robo.github.io/), [Jinwei Gu](http://www.gujinwei.org/), [Kihwan Kim](https://www.cc.gatech.edu/~kihwan23/), [James Hays](https://www.cc.gatech.edu/~hays/), and [Jan Kautz](http://jankautz.com/)\n\n### A four-minute video summary (click below for the video)\n\n[![mapnet](./figures/mapnet.png)](https://www.youtube.com/watch?v=X6mF_IbOb4A)\n\n## Setup\n\nMapNet uses a Conda environment that makes it easy to install all dependencies.\n\n1. Install [miniconda](https://docs.conda.io/en/latest/miniconda.html) with Python 2.7.\n\n2. Create the `mapnet` Conda environment: `conda env create -f environment.yml`.\n\n3. Activate the environment: `conda activate mapnet_release`.\n\n4. Note that our code has been tested with PyTorch v0.4.1 (the environment.yml file should take care of installing the appropriate version).\n\n## Data\nWe support the\n[7Scenes](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/)\nand [Oxford RobotCar](http://robotcar-dataset.robots.ox.ac.uk/) datasets right\nnow. You can also write your own PyTorch dataloader for other datasets and put it in the\n`dataset_loaders` directory. Refer to \n[this README file](./dataset_loaders/README.md) for more details.\n\nThe datasets live in the `data/deepslam_data` directory. We provide skeletons\nwith symlinks to get you started. Let us call your 7Scenes download directory\n7SCENES_DIR and your main RobotCar download directory (in which you untar all\nthe downloads from the website) ROBOTCAR_DIR. You will need to make the following\nsymlinks:\n\n`\ncd data/deepslam_data &&\nln -s 7SCENES_DIR 7Scenes &&\nln -s ROBOTCAR_DIR RobotCar_download \n`\n\n---\n\n#### Special instructions for RobotCar: (only needed for RobotCar data)\n\n1. Download\n[this fork](https://github.com/samarth-robo/robotcar-dataset-sdk/tree/master) of\nthe dataset SDK, and run `cd scripts && ./make_robotcar_symlinks.sh` after\nediting the `ROBOTCAR_SDK_ROOT` variable in it appropriately.\n\n2. For each sequence, you need to download the `stereo_centre`, `vo` and `gps`\ntar files from the dataset website (more details in [this comment](https://github.com/NVlabs/geomapnet/issues/26#issuecomment-537523192)).\n\n3. The directory for each \'scene\' (e.g. `full`) has .txt files defining the\ntrain/test split. While training MapNet++,\nyou must put the sequences for self-supervised learning (dataset T in the paper)\nin the `test_split.txt` file. The dataloader for the MapNet++ models will use\nboth images and ground-truth pose from sequences in `train_split.txt` and only\nimages from the sequences in `test_split.txt`.\n\n4. To make training faster, we pre-processed the images using\n`scripts/process_robotcar_images.py`. This script undistorts the images using\nthe camera models provided by the dataset, and scales them such that the shortest\nside is 256 pixels.\n\n---\n\n## Running the code\n\n\n### Demo/Inference\nThe trained models for all experiments presented in the paper can be downloaded\n[here](https://drive.google.com/open?id=1J2QG_nHrRTKcDf9CGXRK9MWH1h-GuMLy).\nThe inference script is `scripts/eval.py`. Here are some examples, assuming\nthe models are downloaded in `scripts/logs`. Please go to the `scripts` folder to run the commands.\n\n#### 7_Scenes\n- MapNet++ with pose-graph optimization (i.e., MapNet+PGO) on `heads`:\n```\n$ python eval.py --dataset 7Scenes --scene heads --model mapnet++ \\\n--weights logs/7Scenes_heads_mapnet++_mapnet++_7Scenes/epoch_005.pth.tar \\\n--config_file configs/pgo_inference_7Scenes.ini --val --pose_graph\nMedian error in translation = 0.12 m\nMedian error in rotation    = 8.46 degrees\n```\n![7Scenes_heads_mapnet+pgo](./figures/7Scenes_heads_mapnet+pgo.png)\n\n\n- For evaluating on the `train` split remove the `--val` flag\n\n- To save the results to disk without showing them on screen (useful for scripts),\nadd the `--output_dir ../results/` flag\n\n- See [this README file](./scripts/configs/README.md)\nfor more information on hyper-parameters and which config files to use.\n\n\n- MapNet++ on `heads`:\n```\n$ python eval.py --dataset 7Scenes --scene heads --model mapnet++ \\\n--weights logs/7Scenes_heads_mapnet++_mapnet++_7Scenes/epoch_005.pth.tar \\\n--config_file configs/mapnet.ini --val\nMedian error in translation = 0.13 m\nMedian error in rotation    = 11.13 degrees\n```\n\n- MapNet on `heads`:\n```\n$ python eval.py --dataset 7Scenes --scene heads --model mapnet \\\n--weights logs/7Scenes_heads_mapnet_mapnet_learn_beta_learn_gamma/epoch_250.pth.tar \\\n--config_file configs/mapnet.ini --val\nMedian error in translation = 0.18 m\nMedian error in rotation    = 13.33 degrees\n```\n\n- PoseNet (CVPR2017) on `heads`:\n```\n$ python eval.py --dataset 7Scenes --scene heads --model posenet \\\n--weights logs/7Scenes_heads_posenet_posenet_learn_beta_logq/epoch_300.pth.tar \\\n--config_file configs/posenet.ini --val\nMedian error in translation = 0.19 m\nMedian error in rotation    = 12.15 degrees\n```\n\n#### RobotCar\n- MapNet++ with pose-graph optimization on `loop`:\n```\n$ python eval.py --dataset RobotCar --scene loop --model mapnet++ \\\n--weights logs/RobotCar_loop_mapnet++_mapnet++_RobotCar_learn_beta_learn_gamma_2seq/epoch_005.pth.tar \\\n--config_file configs/pgo_inference_RobotCar.ini --val --pose_graph\nMean error in translation = 6.74 m\nMean error in rotation    = 2.23 degrees\n```\n![RobotCar_loop_mapnet+pgo](./figures/RobotCar_loop_mapnet+pgo.png)\n\n- MapNet++ on `loop`:\n```\n$ python eval.py --dataset RobotCar --scene loop --model mapnet++ \\\n--weights logs/RobotCar_loop_mapnet++_mapnet++_RobotCar_learn_beta_learn_gamma_2seq/epoch_005.pth.tar \\\n--config_file configs/mapnet.ini --val\nMean error in translation = 6.95 m\nMean error in rotation    = 2.38 degrees\n```\n\n- MapNet on `loop`:\n```\n$ python eval.py --dataset RobotCar --scene loop --model mapnet \\\n--weights logs/RobotCar_loop_mapnet_mapnet_learn_beta_learn_gamma/epoch_300.pth.tar \\\n--config_file configs/mapnet.ini --val\nMean error in translation = 9.84 m\nMean error in rotation    = 3.96 degrees\n```\n\n---\n\n### Train\nThe executable script is `scripts/train.py`. Please go to the `scripts` folder to run these commands. For example:\n\n- PoseNet on `chess` from `7Scenes`: `python train.py --dataset 7Scenes\n--scene chess --config_file configs/posenet.ini --model posenet --device 0\n--learn_beta --learn_gamma`\n\n![train.png](./figures/train.png)\n\n- MapNet on `chess` from `7Scenes`: `python train.py --dataset 7Scenes\n--scene chess --config_file configs/mapnet.ini --model mapnet\n--device 0 --learn_beta --learn_gamma`\n\n- MapNet++ is finetuned on top of a trained MapNet model:\n`python train.py --dataset 7Scenes --checkpoint <trained_mapnet_model.pth.tar>\n--scene chess --config_file configs/mapnet++_7Scenes.ini --model mapnet++\n--device 0 --learn_beta --learn_gamma`\n\nFor example, we can train MapNet++ model on `heads` from a pretrained MapNet model:\n\n```\n$ python train.py --dataset 7Scenes \\\n--checkpoint logs/7Scenes_heads_mapnet_mapnet_learn_beta_learn_gamma/epoch_250.pth.tar \\\n--scene heads --config_file configs/mapnet++_7Scenes.ini --model mapnet++ \\\n--device 0 --learn_beta --learn_gamma\n```\n\n\nFor MapNet++ training, you will need visual odometry (VO) data (or other\nsensory inputs such as noisy GPS measurements). For 7Scenes, we provided the\npreprocessed VO computed with the DSO method. For RobotCar, we use the provided\nstereo_vo. If you plan to use your own VO data (especially from a monocular\ncamera) for MapNet++ training, you will need to first align the VO with the\nworld coordinate (for rotation and scale). Please refer to the "Align VO"\nsection below for more detailed instructions.\n\n\nThe meanings of various command-line parameters are documented in\n`scripts/train.py`. The values of various hyperparameters are defined in a \nseparate .ini file. We provide some examples in the `scripts/configs` directory,\nalong with a [README](./scripts/configs/README.md) file explaining some\nhyper-parameters.\n\nIf you have `visdom = yes` in the config file, you will need to start a Visdom\nserver for logging the training progress:\n\n`python -m visdom.server -env_path=scripts/logs/`.\n\n--- \n\n### Network Attention Visualization\nCalculates the network attention visualizations and saves them in a video\n\n- For the MapNet model trained on `chess` in `7Scenes`:\n```\n$ python plot_activations.py --dataset 7Scenes --scene chess\n--weights <filename.pth.tar> --device 1 --val --config_file configs/mapnet.ini\n--output_dir ../results/\n```\nCheck [here](https://www.youtube.com/watch?v=hKlE45mJ2yY) for an example video of \ncomputed network attention of PoseNet vs. MapNet++.\n\n\n---\n\n### Other Tools \n\n#### Align VO to the ground truth poses\nThis has to be done before using VO in MapNet++ training. The executable script\nis `scripts/align_vo_poses.py`.\n\n- For the first sequence from `chess` in `7Scenes`:\n`python align_vo_poses.py --dataset 7Scenes --scene chess --seq 1 --vo_lib dso`.\nNote that alignment for `7Scenes` needs to be done separately for each sequence,\nand so the `--seq` flag is needed\n\n- For all `7Scenes` you can also use the script `align_vo_poses_7scenes.sh`\nThe script stores the information at the proper location in `data`\n\n#### Mean and stdev pixel statistics across a dataset\nThis must be calculated before any training. Use the `scripts/dataset_mean.py`,\nwhich also saves the information at the proper location. We provide pre-computed\nvalues for RobotCar and 7Scenes.\n\n#### Calculate pose translation statistics\nCalculates the mean and stdev and saves them automatically to appropriate files\n`python calc_pose_stats.py --dataset 7Scenes --scene redkitchen`\nThis information is needed to normalize the pose regression targets, so this\nscript must be run before any training. We provide pre-computed values for \nRobotCar and 7Scenes.\n\n#### Plot the ground truth and VO poses for debugging\n`python plot_vo_poses.py --dataset 7Scenes --scene heads --vo_lib dso --val`. To save the\noutput instead of displaying on screen, add the `--output_dir ../results/` flag\n\n#### Process RobotCar GPS\nThe `scripts/process_robotcar_gps.py` script must be run before using GPS for\nMapNet++ training. It converts the csv file into a format usable for training.\n\n#### Demosaic and undistort RobotCar images\nThis is advisable to do beforehand to speed up training. The\n`scripts/process_robotcar_images.py` script will do that and save the output\nimages to a `centre_processed` directory in the `stereo` directory. After the\nscript finishes, you must rename this directory to `centre` so that the dataloader\nuses these undistorted and demosaiced images.\n\n---\n\n### Citation\nIf you find this code useful for your research, please cite our paper\n\n```\n@inproceedings{mapnet2018,\n  title={Geometry-Aware Learning of Maps for Camera Localization},\n  author={Samarth Brahmbhatt and Jinwei Gu and Kihwan Kim and James Hays and Jan Kautz},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2018}\n}\n```\n'