b'# Hierarchical Localization\n\n:warning: :warning: **For a clean and research-friendly implementation of Hierarchical Localization, please refer to our CVPR 2019 paper at [ethz-asl/hfnet](https://github.com/ethz-asl/hfnet).** :warning: :warning:\n\nThis repository contains the training and deployment code used in our paper *[Leveraging Deep Visual Descriptors for Hierarchical Efficient Localization](https://arxiv.org/abs/1809.01019)* presented at [CoRL 2018](http://www.robot-learning.org/). This work introduces **MobileNetVLAD**, a mobile-friendly image retrieval deep neural network that significantly improves the performance of classical 6-DoF visual localization through a hierarchical search.\n\n<p align="center">\n    <a href="https://www.youtube.com/watch?v=8cg697oLUtg" target="_blank">\n        <img src="doc/video_thumbnail.png" width="60%" style="opacity:0.5; border:1px solid black"/>\n        <br /><em>The approach is described in details in our video (click to play).</em>\n    </a>\n</p>\n\n##\n\nWe introduce here two main features:\n- The deployment code of MobileNetVLAD: `global-loc`, a C++ ROS/Catkin package that can \n  - load any trained image retrieval model,\n  - efficiently perform the inference on GPU or CPU,\n  - index a given map and save it as a protobuf,\n  - and retrieve keyframes given a query image;\n- The training code: `retrievalnet`, a modular Python+Tensorflow package that allows to \n  - train the model on any target image domain,\n  - using the supervision of any existing teacher network.\n\nThe modularity of our system allows to train a model and index a map on a powerful workstation while performing the retrieval on a mobile platform. Our code has thus been extensively tested on an NVIDIA Jetson TX2, widely used for robotics research.\n\n<p align="center">\n    <a href="https://nbviewer.jupyter.org/github/ethz-asl/hierarchical_loc/blob/master/notebooks/tango_visualize_retrieval.ipynb">\n        <img src="doc/zurich_query_1.png" width="70%"/>\n        <img src="doc/zurich_query_2.png" width="70%"/>\n    </a>\n    <br /><em>Retrieval on our Zurich dataset: strong illumination and viewpoint changes.</em>\n</p>\n\n\n## Deployment\n\nThe package relies on map primitives provided by [maplab](https://github.com/ethz-asl/maplab), but can be easily adapted to other SLAM frameworks. We thus do not release the code performing the local matching. The trained MobileNetVLAD is provided in `global-loc/models/` and is loaded using [tensorflow_catkin](https://github.com/ethz-asl/tensorflow_catkin).\n\n### Installation\nBoth Ubuntu 14.04 and 16.04 are supported. First install the [system packages](https://github.com/ethz-asl/maplab/wiki/Installation-Ubuntu#install-required-system-packages) required by maplab.\n\nThen setup the Catkin workspace:\n```bash\nexport ROS_VERSION=kinetic #(Ubuntu 16.04: kinetic, Ubuntu 14.04: indigo)\nexport CATKIN_WS=~/maplab_ws\nmkdir -p $CATKIN_WS/src\ncd $CATKIN_WS\ncatkin init\ncatkin config --merge-devel # Necessary for catkin_tools >= 0.4.\ncatkin config --extend /opt/ros/$ROS_VERSION\ncatkin config --cmake-args \\\n\t-DCMAKE_BUILD_TYPE=Release \\\n\t-DENABLE_TIMING=1 \\\n\t-DENABLE_STATISTICS=1 \\\n\t-DCMAKE_CXX_FLAGS="-fext-numeric-literals -msse3 -msse4.1 -msse4.2" \\\n\t-DCMAKE_CXX_STANDARD=14\ncd src\n```\nIf you want to perform the inference on GPU (see the requirements of [tensorflow_catkin](https://github.com/ethz-asl/tensorflow_catkin)), add:\n```bash\ncatkin config --append-args --cmake-args -DUSE_GPU=ON\n```\nFinally clone the repository and build:\n```bash\ngit clone https://github.com/ethz-asl/hierarchical_loc.git --recursive\ntouch hierarchical_loc/catkin_dependencies/maplab_dependencies/3rd_party/eigen_catkin/CATKIN_IGNORE\ntouch hierarchical_loc/catkin_dependencies/maplab_dependencies/3rd_party/protobuf_catkin/CATKIN_IGNORE\ncd $CATKIN_WS && catkin build global_loc\n```\nRun the test examples:\n```bash\n./devel/lib/global_loc/test_inference\n./devel/lib/global_loc/test_query_index\n```\n\n### Indexing\nGiven a VI map in `global-loc/maps/`, an index of global descriptors can be created in `global-loc/data/`:\n```bash\n./devel/lib/global_loc/build_index \\\n\t--map_name <map_name> \\\n\t--model_name mobilenetvlad_depth-0.35 \\\n\t--proto_name <index_name.pb>\n```\nAs an example, we provide the [Zurich map](https://github.com/ethz-asl/hierarchical_loc/releases/download/1.0/lindenhof_afternoon-wet_aligned.tar.gz) used in our paper. Several indexing options are available in [place-retrieval.cc](global-loc/src/place-retrieval.cc), such as subsampling or mission selection.\n\n### Retrieval\nAn example of query is provided in [test_query_index.cc](global-loc/test/test_query_index.cc). Descriptor indexes for the Zurich dataset are included in `global-loc/data/` and can be used to time the queries:\n```bash\n./devel/lib/global_loc/time_query \\\n\t--map_name <map_name> \\\n\t--model_name mobilenetvlad_depth-0.35 \\\n\t--proto_name lindenhof_afternoon_aligned_mobilenet-d0.35.pb \\\n\t--query_mission f6837cac0168580aa8a66be7bbb20805 \\\n\t--use_pca --pca_dims 512 --max_num_queries 100\n```\n\nUse the same indexes to evaluate and visualize the retrieval: install [retrievalnet](#training), generate the [Python protobuf interface](notebooks/generate_proto_py.sh), and refer to [tango_evaluation.ipynb](https://nbviewer.jupyter.org/github/ethz-asl/hierarchical_loc/blob/master/notebooks/tango_evaluation.ipynb) and [tango_visualize_retrieval.ipynb](https://nbviewer.jupyter.org/github/ethz-asl/hierarchical_loc/blob/master/notebooks/tango_visualize_retrieval.ipynb).\n\n## Training\n\nWe use distillation to compress the original NetVLAD model into a smaller MobileNetVLAD with mobile real-time inference capability.\n<p align="center">\n\t<img src="doc/training_process.png" width="70%"/>\n</p>\n\n\n### Installation\n\nPython 3.5 is required. It is advised to run the following installation commands within a virtual environment. You will be prompted to provide the path to a data folder (subsequently referred as `$DATA_PATH`) containing the datasets and pre-trained models and to an experiment folder (`$EXPER_PATH`) containing the trained models, training logs, and exported descriptors for evaluation.\n```\ncd retrievalnet && make install\n```\n\n### Exporting the target descriptors\n\nIf you wish to train MobileNetVLAD on the Google Landmarks dataset as done in our paper, you first need to download [the index of images](https://github.com/ethz-asl/hierarchical_loc/releases/download/1.0/google_landmarks_index.csv) and then download the dataset itself with [download_google_landmarks.py](retrievalnet/downloading/download_google_landmarks.py). The [weights of the original NetVLAD model](http://rpg.ifi.uzh.ch/datasets/netvlad/vd16_pitts30k_conv5_3_vlad_preL2_intra_white.zip) are provided by [netvlad_tf_open](https://github.com/uzh-rpg/netvlad_tf_open) and should be extracted in `$DATA_PATH/weights/`.\n\nFinally export the descriptors of Google Landmarks:\n```\npython export_descriptors.py config/netvlad_export_distill.yaml google_landmarks/descriptors --as_dataset\n```\n\n### Training MobileNetVLAD\n\nExtract the MobileNet encoder [pre-trained on ImageNet](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_224.tgz) in `$DATA_PATH/weights/` and run:\n```bash\npython train.py config/mobilenetvlad_train_distill.yaml mobilenetvlad\n```\nThe training can be interrupted at any time using `Ctrl+C` and can be monitored with Tensorboard summaries saved in `$EXPER_PATH/mobilenetvlad/`. The weights are also saved there.\n\n### Exporting the model for deployment\n```bash\npython export_model.py config/mobilenetvlad_train_distill.yaml mobilenetvlad\n```\nwill export the model in `$EXPER_PATH/saved_models/mobilenetvlad/`.\n\n### Evaluating on the NCLT dataset\n\nDownload the [NCLT sequences](http://robots.engin.umich.edu/nclt/) in `$DATA_PATH/nclt/` along with the corresponding [pose files](https://github.com/ethz-asl/hierarchical_loc/releases/download/1.0/nclt_poses.zip) (generated with [nclt_generate_poses.ipynb](notebooks/nclt_generate_poses.ipynb)). Export the NCLT descriptors, e.g. for MobileNetVLAD:\n```bash\npython export_descriptors.py configs/mobilenetvlad_export_nclt.yaml mobilenetvlad\n```\nThese can be used to evaluate and visualize the retrieval (see [nclt_evaluation.ipynb](https://nbviewer.jupyter.org/github/ethz-asl/hierarchical_loc/blob/master/notebooks/nclt_evaluation.ipynb) and [nclt_visualize_retrieval.ipynb](https://nbviewer.jupyter.org/github/ethz-asl/hierarchical_loc/blob/master/notebooks/nclt_visualize_retrieval.ipynb)).\n\n## Citation\nPlease consider citing the corresponding publication if you use this work in an academic context:\n```\n@inproceedings{hloc2018,\n  title={Leveraging Deep Visual Descriptors for Hierarchical Efficient Localization},\n  author={Sarlin, P.-E. and Debraine, F. and Dymczyk, M. and Siegwart, R. and Cadena, C.},\n  booktitle={Conference on Robot Learning (CoRL)},\n  year={2018}\n}\n```\n'