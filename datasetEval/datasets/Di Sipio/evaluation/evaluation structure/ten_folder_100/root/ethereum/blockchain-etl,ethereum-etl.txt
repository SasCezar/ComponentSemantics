b'# Ethereum ETL\n\n[![Join the chat at https://gitter.im/ethereum-eth](https://badges.gitter.im/ethereum-etl.svg)](https://gitter.im/ethereum-etl/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Build Status](https://travis-ci.org/blockchain-etl/ethereum-etl.png)](https://travis-ci.org/blockchain-etl/ethereum-etl)\n[Join Telegram Group](https://t.me/joinchat/GsMpbA3mv1OJ6YMp3T5ORQ)\n\nInstall Ethereum ETL:\n\n```bash\npip3 install ethereum-etl\n```\n\nExport blocks and transactions ([Schema](#blockscsv), [Reference](#export_blocks_and_transactions)):\n\n```bash\n> ethereumetl export_blocks_and_transactions --start-block 0 --end-block 500000 \\\n--provider-uri https://mainnet.infura.io --blocks-output blocks.csv --transactions-output transactions.csv\n```\n\nExport ERC20 and ERC721 transfers ([Schema](#token_transferscsv), [Reference](#export_token_transfers)):\n\n```bash\n> ethereumetl export_token_transfers --start-block 0 --end-block 500000 \\\n--provider-uri file://$HOME/Library/Ethereum/geth.ipc --output token_transfers.csv\n```\n\nExport traces ([Schema](#tracescsv), [Reference](#export_traces)):\n\n```bash\n> ethereumetl export_traces --start-block 0 --end-block 500000 \\\n--provider-uri file://$HOME/Library/Ethereum/parity.ipc --output traces.csv\n```\n\n---\n\nStream blocks, transactions, logs, token_transfers continually to console ([Reference](#stream)):\n\n```bash\n> pip3 install ethereum-etl[streaming]\n> ethereumetl stream --start-block 500000 -e block,transaction,log,token_transfer --log-file log.txt\n```\n\nFind other commands [here](#command-reference).\n\nFor the latest version, check out the repo and call \n```bash\n> pip3 install -e . \n> python3 ethereumetl.py\n```\n\n[LIMITATIONS](#limitations)\n\n## Table of Contents\n\n- [Schema](#schema)\n  - [blocks.csv](#blockscsv)\n  - [transactions.csv](#transactionscsv)\n  - [token_transfers.csv](#token_transferscsv)\n  - [receipts.csv](#receiptscsv)\n  - [logs.csv](#logscsv)\n  - [contracts.csv](#contractscsv)\n  - [tokens.csv](#tokenscsv)\n  - [traces.csv](#tracescsv)\n- [Exporting the Blockchain](#exporting-the-blockchain)\n  - [Export in 2 Hours](#export-in-2-hours)\n  - [Command Reference](#command-reference)\n- [Ethereum Classic Support](#ethereum-classic-support)\n- [Querying in Amazon Athena](#querying-in-amazon-athena)\n- [Querying in Google BigQuery](#querying-in-google-bigquery)\n  - [Public Dataset](#public-dataset)\n  - [Useful Queries](#useful-queries)\n  - [How to Query Balances for all Ethereum Addresses](#how-to-query-balances-for-all-ethereum-addresses)\n  - [Building Token Recommender in Google Cloud Platform](#building-token-recommender-in-google-cloud-platform)\n- [Querying in Kaggle](#querying-in-kaggle)\n- [Blockchain ETL in Media](#blockchain-etl-in-media)\n\n\n## Schema\n\n### blocks.csv\n\nColumn            | Type               |\n------------------|--------------------|\nnumber            | bigint             |\nhash              | hex_string         |\nparent_hash       | hex_string         |\nnonce             | hex_string         |\nsha3_uncles       | hex_string         |\nlogs_bloom        | hex_string         |\ntransactions_root | hex_string         |\nstate_root        | hex_string         |\nreceipts_root     | hex_string         |\nminer             | address            |\ndifficulty        | numeric            |\ntotal_difficulty  | numeric            |\nsize              | bigint             |\nextra_data        | hex_string         |\ngas_limit         | bigint             |\ngas_used          | bigint             |\ntimestamp         | bigint             |\ntransaction_count | bigint             |\n\n### transactions.csv\n\nColumn           |    Type     |\n-----------------|-------------|\nhash             | hex_string  |\nnonce            | bigint      |\nblock_hash       | hex_string  |\nblock_number     | bigint      |\ntransaction_index| bigint      |\nfrom_address     | address     |\nto_address       | address     |\nvalue            | numeric     |\ngas              | bigint      |\ngas_price        | bigint      |\ninput            | hex_string  |\nblock_timestamp  | bigint      |\n\n### token_transfers.csv\n\nColumn              |    Type     |\n--------------------|-------------|\ntoken_address       | address     |\nfrom_address        | address     |\nto_address          | address     |\nvalue               | numeric     |\ntransaction_hash    | hex_string  |\nlog_index           | bigint      |\nblock_number        | bigint      |\n\n### receipts.csv\n\nColumn                       |    Type     |\n-----------------------------|-------------|\ntransaction_hash             | hex_string  |\ntransaction_index            | bigint      |\nblock_hash                   | hex_string  |\nblock_number                 | bigint      |\ncumulative_gas_used          | bigint      |\ngas_used                     | bigint      |\ncontract_address             | address     |\nroot                         | hex_string  |\nstatus                       | bigint      |\n\n### logs.csv\n\nColumn                   |    Type     |\n-------------------------|-------------|\nlog_index                | bigint      |\ntransaction_hash         | hex_string  |\ntransaction_index        | bigint      |\nblock_hash               | hex_string  |\nblock_number             | bigint      |\naddress                  | address     |\ndata                     | hex_string  |\ntopics                   | string      |\n\n### contracts.csv\n\nColumn                       |    Type     |\n-----------------------------|-------------|\naddress                      | address     |\nbytecode                     | hex_string  |\nfunction_sighashes           | string      |\nis_erc20                     | boolean     |\nis_erc721                    | boolean     |\nblock_number                 | bigint      |\n\n### tokens.csv\n\nColumn                       |    Type     |\n-----------------------------|-------------|\naddress                      | address     |\nsymbol                       | string      |\nname                         | string      |\ndecimals                     | bigint      |\ntotal_supply                 | numeric     |\n\n### traces.csv\n\nColumn                       |    Type     |\n-----------------------------|-------------|\nblock_number                 | bigint      |\ntransaction_hash             | hex_string  |\ntransaction_index            | bigint      |\nfrom_address                 | address     |\nto_address                   | address     |\nvalue                        | numeric     |\ninput                        | hex_string  |\noutput                       | hex_string  |\ntrace_type                   | string      |\ncall_type                    | string      |\nreward_type                  | string      |\ngas                          | bigint      |\ngas_used                     | bigint      |\nsubtraces                    | bigint      |\ntrace_address                | string      |\nerror                        | string      |\nstatus                       | bigint      |\n\nYou can find column descriptions in [https://github.com/medvedev1088/ethereum-etl-airflow](https://github.com/medvedev1088/ethereum-etl-airflow/tree/master/dags/resources/stages/raw/schemas)\n\nNote: for the `address` type all hex characters are lower-cased.\n`boolean` type can have 2 values: `True` or `False`.\n\n## LIMITATIONS\n\n- In case the contract is a proxy, which forwards all calls to a delegate, interface detection doesn\xe2\x80\x99t work,\nwhich means `is_erc20` and `is_erc721` will always be false for proxy contracts and they will be missing in the `tokens`\ntable.\n- The metadata methods (`symbol`, `name`, `decimals`, `total_supply`) for ERC20 are optional, so around 10% of the\ncontracts are missing this data. Also some contracts (EOS) implement these methods but with wrong return type,\nso the metadata columns are missing in this case as well.\n- `token_transfers.value`, `tokens.decimals` and `tokens.total_supply` have type `STRING` in BigQuery tables,\nbecause numeric types there can\'t handle 32-byte integers. You should use\n`cast(value as FLOAT64)` (possible loss of precision) or\n`safe_cast(value as NUMERIC)` (possible overflow) to convert to numbers.\n- The contracts that don\'t implement `decimals()` function but have the\n[fallback function](https://solidity.readthedocs.io/en/v0.4.21/contracts.html#fallback-function) that returns a `boolean`\nwill have `0` or `1` in the `decimals` column in the CSVs.\n\n## Exporting the Blockchain\n\nIf you\'d like to have the blockchain data platform \nset up and hosted for you in AWS or GCP, get in touch with us \n[here](https://d5ai.typeform.com/to/cmOoLe).\n\n1. Install python 3.5.3+ https://www.python.org/downloads/\n\n1. You can use Infura if you don\'t need ERC20 transfers (Infura doesn\'t support eth_getFilterLogs JSON RPC method).\nFor that use `-p https://mainnet.infura.io` option for the commands below. If you need ERC20 transfers or want to\nexport the data ~40 times faster, you will need to set up a local Ethereum node:\n\n1. Install geth https://github.com/ethereum/go-ethereum/wiki/Installing-Geth\n\n1. Start geth.\nMake sure it downloaded the blocks that you need by executing `eth.syncing` in the JS console.\nYou can export blocks below `currentBlock`,\nthere is no need to wait until the full sync as the state is not needed (unless you also need contracts bytecode\nand token details; for those you need to wait until the full sync).\n\n1. Install Ethereum ETL:\n\n    ```bash\n    > pip3 install ethereum-etl\n    ```\n\n1. Export all:\n\n    ```bash\n    > ethereumetl export_all --help\n    > ethereumetl export_all -s 0 -e 5999999 -b 100000 -p file://$HOME/Library/Ethereum/geth.ipc -o output\n    ```\n    \n    In case `ethereumetl` command is not available in PATH, use `python3 -m ethereumetl` instead.\n\n    The result will be in the `output` subdirectory, partitioned in Hive style:\n\n    ```bash\n    output/blocks/start_block=00000000/end_block=00099999/blocks_00000000_00099999.csv\n    output/blocks/start_block=00100000/end_block=00199999/blocks_00100000_00199999.csv\n    ...\n    output/transactions/start_block=00000000/end_block=00099999/transactions_00000000_00099999.csv\n    ...\n    output/token_transfers/start_block=00000000/end_block=00099999/token_transfers_00000000_00099999.csv\n    ...\n    ```\n\nShould work with geth and parity, on Linux, Mac, Windows.\nIf you use Parity you should disable warp mode with `--no-warp` option because warp mode\ndoes not place all of the block or receipt data into the database https://wiki.parity.io/Getting-Synced\n\nIf you see weird behavior, e.g. wrong number of rows in the CSV files or corrupted files,\ncheck out this issue: https://github.com/medvedev1088/ethereum-etl/issues/28\n\n### Export in 2 Hours\n\nYou can use AWS Auto Scaling and Data Pipeline to reduce the exporting time to a few hours.\nRead this article for details https://medium.com/@medvedev1088/how-to-export-the-entire-ethereum-blockchain-to-csv-in-2-hours-for-10-69fef511e9a2\n\n### Running in Docker\n\n1. Install Docker https://docs.docker.com/install/\n\n1. Build a docker image\n    ```bash\n    > docker build -t ethereum-etl:latest .\n    > docker image ls\n    ```\n\n1. Run a container out of the image\n    ```bash\n    > docker run -v $HOME/output:/ethereum-etl/output ethereum-etl:latest export_all -s 0 -e 5499999 -b 100000 -p https://mainnet.infura.io\n    > docker run -v $HOME/output:/ethereum-etl/output ethereum-etl:latest export_all -s 2018-01-01 -e 2018-01-01 -p https://mainnet.infura.io\n    ```\n\n1. Run streaming to console or Pub/Sub\n    ```bash\n    > docker build -t ethereum-etl:latest-streaming -f Dockerfile_with_streaming .\n    > echo "Stream to console"\n    > docker run ethereum-etl:latest-streaming stream --start-block 500000 --log-file log.txt\n    > echo "Stream to Pub/Sub"\n    > docker run -v /path_to_credentials_file/:/ethereum-etl/ --env GOOGLE_APPLICATION_CREDENTIALS=/ethereum-etl/credentials_file.json ethereum-etl:latest-streaming stream --start-block 500000 --output projects/<your-project>/topics/crypto_ethereum\n    ```\n\n### Command Reference\n\n- [export_blocks_and_transactions](#export_blocks_and_transactions)\n- [export_token_transfers](#export_token_transfers)\n- [extract_token_transfers](#extract_token_transfers)\n- [export_receipts_and_logs](#export_receipts_and_logs)\n- [export_contracts](#export_contracts)\n- [export_tokens](#export_tokens)\n- [export_traces](#export_traces)\n- [export_geth_traces](#export_geth_traces)\n- [extract_geth_traces](#extract_geth_traces)\n- [get_block_range_for_date](#get_block_range_for_date)\n- [get_keccak_hash](#get_keccak_hash)\n- [stream](#stream)\n\nAll the commands accept `-h` parameter for help, e.g.:\n\n```bash\n> ethereumetl export_blocks_and_transactions -h\n\nUsage: ethereumetl export_blocks_and_transactions [OPTIONS]\n\n  Export blocks and transactions.\n\nOptions:\n  -s, --start-block INTEGER   Start block\n  -e, --end-block INTEGER     End block  [required]\n  -b, --batch-size INTEGER    The number of blocks to export at a time.\n  -p, --provider-uri TEXT     The URI of the web3 provider e.g.\n                              file://$HOME/Library/Ethereum/geth.ipc or\n                              https://mainnet.infura.io\n  -w, --max-workers INTEGER   The maximum number of workers.\n  --blocks-output TEXT        The output file for blocks. If not provided\n                              blocks will not be exported. Use "-" for stdout\n  --transactions-output TEXT  The output file for transactions. If not\n                              provided transactions will not be exported. Use\n                              "-" for stdout\n  -h, --help                  Show this message and exit.\n```\n\nFor the `--output` parameters the supported types are csv and json. The format type is inferred from the output file name.\n\n#### export_blocks_and_transactions\n\n```bash\n> ethereumetl export_blocks_and_transactions --start-block 0 --end-block 500000 \\\n--provider-uri file://$HOME/Library/Ethereum/geth.ipc \\\n--blocks-output blocks.csv --transactions-output transactions.csv\n```\n\nOmit `--blocks-output` or `--transactions-output` options if you want to export only transactions/blocks.\n\nYou can tune `--batch-size`, `--max-workers` for performance.\n\n[Blocks and transactions schema](#blockscsv).\n\n#### export_token_transfers\n\nThe API used in this command is not supported by Infura, so you will need a local node.\nIf you want to use Infura for exporting ERC20 transfers refer to [extract_token_transfers](#extract_token_transfers)\n\n```bash\n> ethereumetl export_token_transfers --start-block 0 --end-block 500000 \\\n--provider-uri file://$HOME/Library/Ethereum/geth.ipc --batch-size 100 --output token_transfers.csv\n```\n\nInclude `--tokens <token1> --tokens <token2>` to filter only certain tokens, e.g.\n\n```bash\n> ethereumetl export_token_transfers --start-block 0 --end-block 500000 \\\n--provider-uri file://$HOME/Library/Ethereum/geth.ipc --output token_transfers.csv \\\n--tokens 0x86fa049857e0209aa7d9e616f7eb3b3b78ecfdb0 --tokens 0x06012c8cf97bead5deae237070f9587f8e7a266d\n```\n\nYou can tune `--batch-size`, `--max-workers` for performance.\n\n[Token transfers schema](#token_transferscsv).\n\n#### export_receipts_and_logs\n\nFirst extract transaction hashes from `transactions.csv`\n(Exported with [export_blocks_and_transactions](#export_blocks_and_transactions)):\n\n```bash\n> ethereumetl extract_csv_column --input transactions.csv --column hash --output transaction_hashes.txt\n```\n\nThen export receipts and logs:\n\n```bash\n> ethereumetl export_receipts_and_logs --transaction-hashes transaction_hashes.txt \\\n--provider-uri file://$HOME/Library/Ethereum/geth.ipc --receipts-output receipts.csv --logs-output logs.csv\n```\n\nOmit `--receipts-output` or `--logs-output` options if you want to export only logs/receipts.\n\nYou can tune `--batch-size`, `--max-workers` for performance.\n\nUpvote this feature request https://github.com/paritytech/parity/issues/9075,\nit will make receipts and logs export much faster.\n\n[Receipts and logs schema](#receiptscsv).\n\n#### extract_token_transfers\n\nFirst export receipt logs with [export_receipts_and_logs](#export_receipts_and_logs).\n\nThen extract transfers from the logs.csv file:\n\n```bash\n> ethereumetl extract_token_transfers --logs logs.csv --output token_transfers.csv\n```\n\nYou can tune `--batch-size`, `--max-workers` for performance.\n\n[Token transfers schema](#token_transferscsv).\n\n#### export_contracts\n\nFirst extract contract addresses from `receipts.csv`\n(Exported with [export_receipts_and_logs](#export_receipts_and_logs)):\n\n```bash\n> ethereumetl extract_csv_column --input receipts.csv --column contract_address --output contract_addresses.txt\n```\n\nThen export contracts:\n\n```bash\n> ethereumetl export_contracts --contract-addresses contract_addresses.txt \\\n--provider-uri file://$HOME/Library/Ethereum/geth.ipc --output contracts.csv\n```\n\nYou can tune `--batch-size`, `--max-workers` for performance.\n\n[Contracts schema](#contractscsv).\n\n#### export_tokens\n\nFirst extract token addresses from `contracts.json`\n(Exported with [export_contracts](#export_contracts)):\n\n```bash\n> ethereumetl filter_items -i contracts.json -p "item[\'is_erc20\'] or item[\'is_erc721\']" | \\\nethereumetl extract_field -f address -o token_addresses.txt\n```\n\nThen export ERC20 / ERC721 tokens:\n\n```bash\n> ethereumetl export_tokens --token-addresses token_addresses.txt \\\n--provider-uri file://$HOME/Library/Ethereum/geth.ipc --output tokens.csv\n```\n\nYou can tune `--max-workers` for performance.\n\n[Tokens schema](#tokenscsv).\n\n#### export_traces\n\nAlso called internal transactions.\nThe API used in this command is not supported by Infura, \nso you will need a local Parity archive node (`parity --tracing on`). \nMake sure your node has at least 8GB of memory, or else you will face timeout errors. \nSee [this issue](https://github.com/blockchain-etl/ethereum-etl/issues/137) \n\n```bash\n> ethereumetl export_traces --start-block 0 --end-block 500000 \\\n--provider-uri file://$HOME/Library/Ethereum/parity.ipc --batch-size 100 --output traces.csv\n```\n\nYou can tune `--batch-size`, `--max-workers` for performance.\n\n[Traces schema](#tracescsv).\n\n#### export_geth_traces\n\nRead [Differences between geth and parity traces.csv](#differences-between-geth-and-parity-tracescsv)\n\nThe API used in this command is not supported by Infura, \nso you will need a local Geth archive node (`geth --gcmode archive --syncmode full --ipcapi debug`).\nWhen using rpc, add `--rpc --rpcapi debug` options.\n\n```bash\n> ethereumetl export_geth_traces --start-block 0 --end-block 500000 \\\n--provider-uri file://$HOME/Library/Ethereum/geth.ipc --batch-size 100 --output geth_traces.json\n```\n\nYou can tune `--batch-size`, `--max-workers` for performance.\n\n#### extract_geth_traces\n\n```bash\n> ethereumetl extract_geth_traces --input geth_traces.json --output traces.csv\n```\n\nYou can tune `--batch-size`, `--max-workers` for performance.\n\n#### get_block_range_for_date\n\n```bash\n> ethereumetl get_block_range_for_date --provider-uri=https://mainnet.infura.io --date 2018-01-01\n4832686,4838611\n```\n\n#### get_keccak_hash\n\n```bash\n> ethereumetl get_keccak_hash -i "transfer(address,uint256)"\n0xa9059cbb2ab09eb219583f4a59a5d0623ade346d962bcd4e46b11da047c9049b\n```\n\n#### stream\n\n```bash\n> pip3 install ethereum-etl[streaming]\n> ethereumetl stream --provider-uri https://mainnet.infura.io --start-block 500000\n```\n\n- This command outputs blocks, transactions, logs, token_transfers to the console by default.\n- Entity types can be specified with the `-e` option, \ne.g. `-e block,transaction,log,token_transfer,trace,contract,token`.\n- Use `--output` option to specify the Google Pub/Sub topic where to publish blockchain data, \ne.g. `projects/<your-project>/topics/bitcoin_blockchain`. Data will be pushed to \n`projects/<your-project>/topics/bitcoin_blockchain.blocks`, `projects/<your-project>/topics/bitcoin_blockchain.transactions` \netc. topics.\n- The command saves its state to `last_synced_block.txt` file where the last synced block number is saved periodically.\n- Specify either `--start-block` or `--last-synced-block-file` option. `--last-synced-block-file` should point to the \nfile where the block number, from which to start streaming the blockchain data, is saved.\n- Use the `--lag` option to specify how many blocks to lag behind the head of the blockchain. It\'s the simplest way to \nhandle chain reorganizations - they are less likely the further a block from the head.\n- You can tune `--period-seconds`, `--batch-size`, `--block-batch-size`, `--max-workers` for performance.\n- Refer to [blockchain-etl-streaming](https://github.com/blockchain-etl/blockchain-etl-streaming) for\ninstructions on deploying it to Kubernetes. \n\nStream blockchain data continually to Google Pub/Sub:\n\n```bash\n> export GOOGLE_APPLICATION_CREDENTIALS=/path_to_credentials_file.json\n> ethereumetl stream --start-block 500000 --output projects/<your-project>/topics/crypto_ethereum\n```\n\n### Running Tests\n\n```bash\n> pip3 install -e .[dev,streaming]\n> export ETHEREUM_ETL_RUN_SLOW_TESTS=True\n> pytest -vv\n```\n\n### Running Tox Tests\n\n```bash\n> pip3 install tox\n> tox\n```\n\n### Ethereum Classic Support\n\nFor getting ETC csv files, make sure you pass in the `--chain classic` param where it\'s required for the scripts you want to export. \nETC won\'t run if your `--provider-uri` is Infura. It will provide a warning and change the provider-uri to `https://ethereumclassic.network` instead. For faster performance, run a client instead locally for classic such as `parity chain=classic` and Geth-classic.\n\n### Differences between geth and parity traces.csv\n\n- `to_address` field differs for `callcode` trace (geth seems to return correct value, as parity value of `to_address` is same as `to_address` of parent call);\n- geth output doesn\'t have `reward` traces;\n- geth output doesn\'t have `to_address`, `from_address`, `value` for `suicide` traces;\n- `error` field contains human readable error message, which might differ in geth/parity output;\n- geth output doesn\'t have `transaction_hash`;\n- `gas_used` is 0 on traces with error in geth, empty in parity;\n- zero output of subcalls is `0x000...` in geth, `0x` in parity;\n\n## Querying in Amazon Athena\n\n- Upload the files to S3:\n\n```bash\n> cd output\n> aws s3 sync . s3://<your_bucket>/ethereumetl/export --region ap-southeast-1\n```\n\n- Sign in to Athena https://console.aws.amazon.com/athena/home\n\n- Create a database:\n\n```sql\nCREATE DATABASE ethereumetl;\n```\n\n- Create the tables:\n  - blocks: [schemas/aws/blocks.sql](schemas/aws/blocks.sql)\n  - transactions: [schemas/aws/transactions.sql](schemas/aws/transactions.sql)\n  - token_transfers: [schemas/aws/token_transfers.sql](schemas/aws/token_transfers.sql)\n  - contracts: [schemas/aws/contracts.sql](schemas/aws/contracts.sql)\n  - receipts: [schemas/aws/receipts.sql](schemas/aws/receipts.sql)\n  - logs: [schemas/aws/logs.sql](schemas/aws/logs.sql)\n  - tokens: [schemas/aws/tokens.sql](schemas/aws/tokens.sql)\n\n### Airflow DAGs\n\nRefer to https://github.com/medvedev1088/ethereum-etl-airflow for the instructions.\n\n### Tables for Parquet Files\n\nRead this article on how to convert CSVs to Parquet https://medium.com/@medvedev1088/converting-ethereum-etl-files-to-parquet-399e048ddd30\n\n- Create the tables:\n  - parquet_blocks: [schemas/aws/parquet/parquet_blocks.sql](schemas/aws/parquet/parquet_blocks.sql)\n  - parquet_transactions: [schemas/aws/parquet/parquet_transactions.sql](schemas/aws/parquet/parquet_transactions.sql)\n  - parquet_token_transfers: [schemas/aws/parquet/parquet_token_transfers.sql](schemas/aws/parquet/parquet_token_transfers.sql)\n\nNote that DECIMAL type is limited to 38 digits in Hive https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-decimal\nso values greater than 38 decimals will be null.\n\n## Querying in Google BigQuery\n\n### Public Dataset\n\nYou can query the data that\'s updated daily in the public BigQuery dataset\nhttps://medium.com/@medvedev1088/ethereum-blockchain-on-google-bigquery-283fb300f579\n\n### Useful Queries\n\nhttps://github.com/blockchain-etl/awesome-bigquery-views\n\n### How to Query Balances for all Ethereum Addresses\n\nRead this article \nhttps://medium.com/google-cloud/how-to-query-balances-for-all-ethereum-addresses-in-bigquery-fb594e4034a7\n\n### Building Token Recommender in Google Cloud Platform\n\nRead this article \nhttps://medium.com/google-cloud/building-token-recommender-in-google-cloud-platform-1be5a54698eb\n\n### Querying in Kaggle\n\nYou can access the Ethereum dataset in Kaggle https://www.kaggle.com/bigquery/ethereum-blockchain.\n\n## Blockchain ETL in Media\n\n- A Technical Breakdown Of Google\'s New Blockchain Search Tools: https://www.forbes.com/sites/michaeldelcastillo/2019/02/05/google-launches-search-for-bitcoin-ethereum-bitcoin-cash-dash-dogecoin-ethereum-classic-litecoin-and-zcash/#394fc868c789\n- Navigating Bitcoin, Ethereum, XRP: How Google Is Quietly Making Blockchains Searchable: https://www.forbes.com/sites/michaeldelcastillo/2019/02/04/navigating-bitcoin-ethereum-xrp-how-google-is-quietly-making-blockchains-searchable/?ss=crypto-blockchain#49e111da4248\n- Querying the Ethereum Blockchain in Snowflake: https://community.snowflake.com/s/article/Querying-the-Ethereum-Blockchain-in-Snowflake\n\n'