b'**IMPORTANT: This is the documentation for the latest `SNAPSHOT` version. Please refer to the website at [http://getquill.io](http://getquill.io) for the latest release\'s documentation.**\n\n![quill](https://raw.githubusercontent.com/getquill/quill/master/quill.png)\n\n**Compile-time Language Integrated Query for Scala**\n\n[![Build Status](https://travis-ci.org/getquill/quill.svg?branch=master)](https://travis-ci.org/getquill/quill)\n[![Codacy Badge](https://api.codacy.com/project/badge/grade/36ab84c7ff43480489df9b7312a4bdc1)](https://www.codacy.com/app/fwbrasil/quill)\n[![codecov.io](https://codecov.io/github/getquill/quill/coverage.svg?branch=master)](https://codecov.io/github/getquill/quill?branch=master)\n[![Join the chat at https://gitter.im/getquill/quill](https://img.shields.io/badge/gitter-join%20chat-green.svg)](https://gitter.im/getquill/quill?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/io.getquill/quill_2.11/badge.svg)](https://maven-badges.herokuapp.com/maven-central/io.getquill/quill_2.11)\n[![Javadocs](https://www.javadoc.io/badge/io.getquill/quill_2.11.svg)](https://www.javadoc.io/doc/io.getquill/quill-core_2.11)\n\nQuill provides a Quoted Domain Specific Language ([QDSL](http://homepages.inf.ed.ac.uk/wadler/papers/qdsl/qdsl.pdf)) to express queries in Scala and execute them in a target language. The library\'s core is designed to support multiple target languages, currently featuring specializations for Structured Query Language ([SQL](https://en.wikipedia.org/wiki/SQL)) and Cassandra Query Language ([CQL](https://cassandra.apache.org/doc/latest/cql/)).\n\n![example](https://raw.githubusercontent.com/getquill/quill/master/example.gif)\n\n1. **Boilerplate-free mapping**: The database schema is mapped using simple case classes.\n2. **Quoted DSL**: Queries are defined inside a `quote` block. Quill parses each quoted block of code (quotation) at compile time and translates them to an internal Abstract Syntax Tree (AST)\n3. **Compile-time query generation**: The `ctx.run` call reads the quotation\'s AST and translates it to the target language at compile time, emitting the query string as a compilation message. As the query string is known at compile time, the runtime overhead is very low and similar to using the database driver directly.\n4. **Compile-time query validation**: If configured, the query is verified against the database at compile time and the compilation fails if it is not valid. The query validation **does not** alter the database state.\n\nNote: The GIF example uses Eclipse, which shows compilation messages to the user.\n\n# Quotation\n\n## Introduction\n\nThe QDSL allows the user to write plain Scala code, leveraging Scala\'s syntax and type system. Quotations are created using the `quote` method and can contain any excerpt of code that uses supported operations. To create quotations, first create a context instance. Please see the [context](#contexts) section for more details on the different context available.\n\nFor this documentation, a special type of context that acts as a [mirror](#mirror-context) is used:\n\n```scala\nimport io.getquill._\n\nval ctx = new SqlMirrorContext(MirrorSqlDialect, Literal)\n```\n\n> ### **Note:** [Scastie](https://scastie.scala-lang.org/) is a great tool to try out Quill without having to prepare a local environment. It works with [mirror contexts](#mirror-context), see [this](https://scastie.scala-lang.org/QwOewNEiR3mFlKIM7v900A) snippet as an example.\n\nThe context instance provides all the types, methods, and encoders/decoders needed for quotations:\n\n```scala\nimport ctx._\n```\n\nA quotation can be a simple value:\n\n```scala\nval pi = quote(3.14159)\n```\n\nAnd be used within another quotation:\n\n```scala\ncase class Circle(radius: Float)\n\nval areas = quote {\n  query[Circle].map(c => pi * c.radius * c.radius)\n}\n```\n\nQuotations can also contain high-order functions and inline values:\n\n```scala\nval area = quote {\n  (c: Circle) => {\n    val r2 = c.radius * c.radius\n    pi * r2\n  }\n}\n```\n\n```scala\nval areas = quote {\n  query[Circle].map(c => area(c))\n}\n```\n\nQuill\'s normalization engine applies reduction steps before translating the quotation to the target language. The correspondent normalized quotation for both versions of the `areas` query is:\n\n```scala\nval areas = quote {\n  query[Circle].map(c => 3.14159 * c.radius * c.radius)\n}\n```\n\nScala doesn\'t have support for high-order functions with type parameters. It\'s possible to use a method type parameter for this purpose:\n\n```scala\ndef existsAny[T] = quote {\n  (xs: Query[T]) => (p: T => Boolean) =>\n    \txs.filter(p(_)).nonEmpty\n}\n\nval q = quote {\n  query[Circle].filter { c1 =>\n    existsAny(query[Circle])(c2 => c2.radius > c1.radius)\n  }\n}\n```\n\n## Compile-time quotations\n\nQuotations are both compile-time and runtime values. Quill uses a type refinement to store the quotation\'s AST as an annotation available at compile-time and the `q.ast` method exposes the AST as runtime value.\n\nIt is important to avoid giving explicit types to quotations when possible. For instance, this quotation can\'t be read at compile-time as the type refinement is lost:\n\n```scala\n// Avoid type widening (Quoted[Query[Circle]]), or else the quotation will be dynamic.\nval q: Quoted[Query[Circle]] = quote {\n  query[Circle].filter(c => c.radius > 10)\n}\n\nctx.run(q) // Dynamic query\n```\n\nQuill falls back to runtime normalization and query generation if the quotation\'s AST can\'t be read at compile-time. Please refer to [dynamic queries](#dynamic-queries) for more information.\n\n#### Inline queries\n\nQuoting is implicit when writing a query in a `run` statement.\n\n```scala\nctx.run(query[Circle].map(_.radius))\n// SELECT r.radius FROM Circle r\n```\n\n## Bindings\n\nQuotations are designed to be self-contained, without references to runtime values outside their scope. There are two mechanisms to explicitly bind runtime values to a quotation execution.\n\n### Lifted values\n\nA runtime value can be lifted to a quotation through the method `lift`:\n\n```scala\ndef biggerThan(i: Float) = quote {\n  query[Circle].filter(r => r.radius > lift(i))\n}\nctx.run(biggerThan(10)) // SELECT r.radius FROM Circle r WHERE r.radius > ?\n```\n\n### Lifted queries\n\nA `Iterable` instance can be lifted as a `Query`. There are two main usages for lifted queries:\n\n#### contains\n\n```scala\ndef find(radiusList: List[Float]) = quote {\n  query[Circle].filter(r => liftQuery(radiusList).contains(r.radius))\n}\nctx.run(find(List(1.1F, 1.2F))) \n// SELECT r.radius FROM Circle r WHERE r.radius IN (?)\n```\n\n#### batch action\n```scala\ndef insert(circles: List[Circle]) = quote {\n  liftQuery(circles).foreach(c => query[Circle].insert(c))\n}\nctx.run(insert(List(Circle(1.1F), Circle(1.2F)))) \n// INSERT INTO Circle (radius) VALUES (?)\n```\n\n## Schema\n\nThe database schema is represented by case classes. By default, quill uses the class and field names as the database identifiers:\n\n```scala\ncase class Circle(radius: Float)\n\nval q = quote {\n  query[Circle].filter(c => c.radius > 1)\n}\n\nctx.run(q) // SELECT c.radius FROM Circle c WHERE c.radius > 1\n```\n\n### Schema customization\n\nAlternatively, the identifiers can be customized:\n\n```scala\nval circles = quote {\n  querySchema[Circle]("circle_table", _.radius -> "radius_column")\n}\n\nval q = quote {\n  circles.filter(c => c.radius > 1)\n}\n\nctx.run(q)\n// SELECT c.radius_column FROM circle_table c WHERE c.radius_column > 1\n```\n\nIf multiple tables require custom identifiers, it is good practice to define a `schema` object with all table queries to be reused across multiple queries:\n\n```scala\ncase class Circle(radius: Int)\ncase class Rectangle(length: Int, width: Int)\nobject schema {\n  val circles = quote {\n    querySchema[Circle](\n        "circle_table",\n        _.radius -> "radius_column")\n  }\n  val rectangles = quote {\n    querySchema[Rectangle](\n        "rectangle_table",\n        _.length -> "length_column",\n        _.width -> "width_column")\n  }\n}\n```\n\n### Database-generated values\n\n#### returningGenerated\n\nDatabase generated values can be returned from an insert query by using `.returningGenerated`. These properties\nwill also be excluded from the insertion since they are database generated.\n\n```scala\ncase class Product(id: Int, description: String, sku: Long)\n\nval q = quote {\n  query[Product].insert(lift(Product(0, "My Product", 1011L))).returningGenerated(_.id)\n}\n\nval returnedIds = ctx.run(q) //: List[Int]\n// INSERT INTO Product (description,sku) VALUES (?, ?) -- NOTE that \'id\' is not being inserted.\n```\n\nMultiple properties can be returned in a Tuple or Case Class and all of them will be excluded from insertion.\n\n> NOTE: Using multiple properties is currently only supported by Postgres and Oracle\n\n```scala\n// Assuming sku is generated by the database.\nval q = quote {\n  query[Product].insert(lift(Product(0, "My Product", 1011L))).returningGenerated(r => (id, sku))\n}\n\nval returnedIds = ctx.run(q) //: List[(Int, Long)]\n// INSERT INTO Product (description) VALUES (?) RETURNING id, sku -- NOTE that \'id\' and \'sku\' are not being inserted.\n```\n\n#### returning\n\nIn certain situations, we might want to return fields that are not auto generated as well. In this case we do not want \nthe fields to be automatically excluded from the insertion. The `returning` method is used for that.\n \n```scala\nval q = quote {\n  query[Product].insert(lift(Product(0, "My Product", 1011L))).returning(r => (id, description))\n}\n\nval returnedIds = ctx.run(q) //: List[(Int, String)]\n// INSERT INTO Product (id, description, sku) VALUES (?, ?, ?) RETURNING id, description\n```\n\nWait a second! Why did we just insert `id` into the database? That is because `returning` does not exclude values\nfrom the insertion! We can fix this situation by manually specifying the columns to insert:\n\n```scala\nval q = quote {\n  query[Product].insert(_.description -> "My Product", _.sku -> 1011L))).returning(r => (id, description))\n}\n\nval returnedIds = ctx.run(q) //: List[(Int, String)]\n// INSERT INTO Product (description, sku) VALUES (?, ?) RETURNING id, description\n```\n \n We can also fix this situation by using an insert-meta.\n\n```scala\nimplicit val productInsertMeta = insertMeta[Product](_.id)\nval q = quote {\n  query[Product].insert(lift(Product(0L, "My Product", 1011L))).returning(r => (id, description))\n}\n\nval returnedIds = ctx.run(q) //: List[(Int, String)]\n// INSERT INTO Product (description, sku) VALUES (?, ?) RETURNING id, description\n```\n\n#### Customization\n\nThe `returning` and `returningGenerated` methods also support arithmetic operations, SQL UDFs and \neven entire queries. These are inserted directly into the SQL `RETURNING` clause.\n> Currently this is only supported in Postgres but will be extended to other databases in the future.\n\nAssuming this basic query:\n```scala\nval q = quote {\n  query[Product].insert(_.description -> "My Product", _.sku -> 1011L)))\n}\n```\n\nAdd 100 to the value of `id`:\n```scala\nctx.run(q.returning(r => id + 100)) //: List[Int]\n// INSERT INTO Product (description, sku) VALUES (?, ?) RETURNING id + 100\n```\n\nPass the value of `id` into a UDF:\n```scala\nval udf = quote { (i: Long) => infix"myUdf($i)".as[Int] }\nctx.run(q.returning(r => udf(id))) //: List[Int]\n// INSERT INTO Product (description, sku) VALUES (?, ?) RETURNING myUdf(id)\n```\n\nUse the return value of `sku` to issue a query:\n```scala\ncase class Supplier(id: Int, clientSku: Long)\nctx.run { \n  q.returning(r => query[Supplier].filter(s => s.sku == r.sku).map(_.id).max) \n} //: List[Option[Long]]\n// INSERT INTO Product (description,sku) VALUES (\'My Product\', 1011) RETURNING (SELECT MAX(s.id) FROM Supplier s WHERE s.sku = clientSku)\n```\n\nAs is typically the case with Quill, you can use all of these features together.\n```scala\nctx.run {\n  q.returning(r => \n    (r.id + 100, udf(r.id), query[Supplier].filter(s => s.sku == r.sku).map(_.id).max)\n  ) \n} // List[(Int, Int, Option[Long])]\n// INSERT INTO Product (description,sku) VALUES (\'My Product\', 1011) \n// RETURNING id + 100, myUdf(id), (SELECT MAX(s.id) FROM Supplier s WHERE s.sku = sku)\n```\n\n> NOTE: Queries used inside of return clauses can only return a single row per insert.\nOtherwise, Postgres will throw:\n`ERROR: more than one row returned by a subquery used as an expression`. This is why is it strongly\nrecommended that you use aggregators such as `max` or `min`inside of quill returning-clause queries.\nIn the case that this is impossible (e.g. when using Postgres booleans), you can use the `.value` method: \n`q.returning(r => query[Supplier].filter(s => s.sku == r.sku).map(_.id).value)`.\n\n\n### Embedded case classes\n\nQuill supports nested `Embedded` case classes:\n\n```scala\ncase class Contact(phone: String, address: String) extends Embedded\ncase class Person(id: Int, name: String, contact: Contact)\n\nctx.run(query[Person])\n// SELECT x.id, x.name, x.phone, x.address FROM Person x\n```\n\nNote that default naming behavior uses the name of the nested case class properties. It\'s possible to override this default behavior using a custom `schema`:\n\n```scala\ncase class Contact(phone: String, address: String) extends Embedded\ncase class Person(id: Int, name: String, homeContact: Contact, workContact: Option[Contact])\n\nval q = quote {\n  querySchema[Person](\n    "Person",\n    _.homeContact.phone          -> "homePhone",\n    _.homeContact.address        -> "homeAddress",\n    _.workContact.map(_.phone)   -> "workPhone",\n    _.workContact.map(_.address) -> "workAddress"\n  )\n}\n\nctx.run(q)\n// SELECT x.id, x.name, x.homePhone, x.homeAddress, x.workPhone, x.workAddress FROM Person x\n```\n\n## Queries\n\nThe overall abstraction of quill queries uses database tables as if they were in-memory collections. Scala for-comprehensions provide syntactic sugar to deal with these kinds of monadic operations:\n\n```scala\ncase class Person(id: Int, name: String, age: Int)\ncase class Contact(personId: Int, phone: String)\n\nval q = quote {\n  for {\n    p <- query[Person] if(p.id == 999)\n    c <- query[Contact] if(c.personId == p.id)\n  } yield {\n    (p.name, c.phone)\n  }\n}\n\nctx.run(q)\n// SELECT p.name, c.phone FROM Person p, Contact c WHERE (p.id = 999) AND (c.personId = p.id)\n```\n\nQuill normalizes the quotation and translates the monadic joins to applicative joins, generating a database-friendly query that avoids nested queries.\n\nAny of the following features can be used together with the others and/or within a for-comprehension:\n\n### filter\n```scala\nval q = quote {\n  query[Person].filter(p => p.age > 18)\n}\n\nctx.run(q)\n// SELECT p.id, p.name, p.age FROM Person p WHERE p.age > 18\n```\n\n### map\n```scala\nval q = quote {\n  query[Person].map(p => p.name)\n}\n\nctx.run(q)\n// SELECT p.name FROM Person p\n```\n\n### flatMap\n```scala\nval q = quote {\n  query[Person].filter(p => p.age > 18).flatMap(p => query[Contact].filter(c => c.personId == p.id))\n}\n\nctx.run(q)\n// SELECT c.personId, c.phone FROM Person p, Contact c WHERE (p.age > 18) AND (c.personId = p.id)\n```\n\n### concatMap\n```scala\n// similar to `flatMap` but for transformations that return a traversable instead of `Query`\n\nval q = quote {\n  query[Person].concatMap(p => p.name.split(" "))\n}\n\nctx.run(q)\n// SELECT UNNEST(SPLIT(p.name, " ")) FROM Person p\n```\n\n### sortBy\n```scala\nval q1 = quote {\n  query[Person].sortBy(p => p.age)\n}\n\nctx.run(q1)\n// SELECT p.id, p.name, p.age FROM Person p ORDER BY p.age ASC NULLS FIRST\n\nval q2 = quote {\n  query[Person].sortBy(p => p.age)(Ord.descNullsLast)\n}\n\nctx.run(q2)\n// SELECT p.id, p.name, p.age FROM Person p ORDER BY p.age DESC NULLS LAST\n\nval q3 = quote {\n  query[Person].sortBy(p => (p.name, p.age))(Ord(Ord.asc, Ord.desc))\n}\n\nctx.run(q3)\n// SELECT p.id, p.name, p.age FROM Person p ORDER BY p.name ASC, p.age DESC\n```\n\n### drop/take\n\n```scala\nval q = quote {\n  query[Person].drop(2).take(1)\n}\n\nctx.run(q)\n// SELECT x.id, x.name, x.age FROM Person x LIMIT 1 OFFSET 2\n```\n\n### groupBy\n```scala\nval q = quote {\n  query[Person].groupBy(p => p.age).map {\n    case (age, people) =>\n      (age, people.size)\n  }\n}\n\nctx.run(q)\n// SELECT p.age, COUNT(*) FROM Person p GROUP BY p.age\n```\n\n### union\n```scala\nval q = quote {\n  query[Person].filter(p => p.age > 18).union(query[Person].filter(p => p.age > 60))\n}\n\nctx.run(q)\n// SELECT x.id, x.name, x.age FROM (SELECT id, name, age FROM Person p WHERE p.age > 18\n// UNION SELECT id, name, age FROM Person p1 WHERE p1.age > 60) x\n```\n\n### unionAll/++\n```scala\nval q = quote {\n  query[Person].filter(p => p.age > 18).unionAll(query[Person].filter(p => p.age > 60))\n}\n\nctx.run(q)\n// SELECT x.id, x.name, x.age FROM (SELECT id, name, age FROM Person p WHERE p.age > 18\n// UNION ALL SELECT id, name, age FROM Person p1 WHERE p1.age > 60) x\n\nval q2 = quote {\n  query[Person].filter(p => p.age > 18) ++ query[Person].filter(p => p.age > 60)\n}\n\nctx.run(q2)\n// SELECT x.id, x.name, x.age FROM (SELECT id, name, age FROM Person p WHERE p.age > 18\n// UNION ALL SELECT id, name, age FROM Person p1 WHERE p1.age > 60) x\n```\n\n### aggregation\n```scala\nval r = quote {\n  query[Person].map(p => p.age)\n}\n\nctx.run(r.min) // SELECT MIN(p.age) FROM Person p\nctx.run(r.max) // SELECT MAX(p.age) FROM Person p\nctx.run(r.avg) // SELECT AVG(p.age) FROM Person p\nctx.run(r.sum) // SELECT SUM(p.age) FROM Person p\nctx.run(r.size) // SELECT COUNT(p.age) FROM Person p\n```\n\n### isEmpty/nonEmpty\n```scala\nval q = quote {\n  query[Person].filter{ p1 =>\n    query[Person].filter(p2 => p2.id != p1.id && p2.age == p1.age).isEmpty\n  }\n}\n\nctx.run(q)\n// SELECT p1.id, p1.name, p1.age FROM Person p1 WHERE\n// NOT EXISTS (SELECT * FROM Person p2 WHERE (p2.id <> p1.id) AND (p2.age = p1.age))\n\nval q2 = quote {\n  query[Person].filter{ p1 =>\n    query[Person].filter(p2 => p2.id != p1.id && p2.age == p1.age).nonEmpty\n  }\n}\n\nctx.run(q2)\n// SELECT p1.id, p1.name, p1.age FROM Person p1 WHERE\n// EXISTS (SELECT * FROM Person p2 WHERE (p2.id <> p1.id) AND (p2.age = p1.age))\n```\n\n### contains\n```scala\nval q = quote {\n  query[Person].filter(p => liftQuery(Set(1, 2)).contains(p.id))\n}\n\nctx.run(q)\n// SELECT p.id, p.name, p.age FROM Person p WHERE p.id IN (?, ?)\n\nval q1 = quote { (ids: Query[Int]) =>\n  query[Person].filter(p => ids.contains(p.id))\n}\n\nctx.run(q1(liftQuery(List(1, 2))))\n// SELECT p.id, p.name, p.age FROM Person p WHERE p.id IN (?, ?)\n\nval peopleWithContacts = quote {\n  query[Person].filter(p => query[Contact].filter(c => c.personId == p.id).nonEmpty)\n}\nval q2 = quote {\n  query[Person].filter(p => peopleWithContacts.contains(p.id))\n}\n\nctx.run(q2)\n// SELECT p.id, p.name, p.age FROM Person p WHERE p.id IN (SELECT p1.* FROM Person p1 WHERE EXISTS (SELECT c.* FROM Contact c WHERE c.personId = p1.id))\n```\n\n### distinct\n```scala\nval q = quote {\n  query[Person].map(p => p.age).distinct\n}\n\nctx.run(q)\n// SELECT DISTINCT p.age FROM Person p\n```\n\n### nested\n```scala\nval q = quote {\n  query[Person].filter(p => p.name == "John").nested.map(p => p.age)\n}\n\nctx.run(q)\n// SELECT p.age FROM (SELECT p.age FROM Person p WHERE p.name = \'John\') p\n```\n\n### joins\nJoins are arguably the largest source of complexity in most SQL queries.\nQuill offers a few different syntaxes so you can choose the right one for your use-case!\n\n```scala\ncase class A(id: Int)\ncase class B(fk: Int)\n\n// Applicative Joins:\nquote {\n  query[A].join(query[B]).on(_.id == _.fk)\n}\n \n// Implicit Joins:\nquote {\n  for {\n    a <- query[A]\n    b <- query[B] if (a.id == b.fk) \n  } yield (a, b)\n}\n \n// Flat Joins:\nquote {\n  for {\n    a <- query[A]\n    b <- query[B].join(_.fk == a.id)\n  } yield (a, b)\n}\n```\n\nLet\'s see them one by one assuming the following schema:\n```scala\ncase class Person(id: Int, name: String)\ncase class Address(street: String, zip: Int, fk: Int)\n```\n(Note: If your use case involves lots and lots of joins, both inner and outer. Skip right to the flat-joins section!)\n\n#### applicative joins\n\nApplicative joins are useful for joining two tables together,\nthey are straightforward to understand, and typically look good on one line.\nQuill supports inner, left-outer, right-outer, and full-outer (i.e. cross) applicative joins.\n\n```scala\n// Inner Join\nval q = quote {\n  query[Person].join(query[Address]).on(_.id == _.fk)\n}\n \nctx.run(q) //: List[(Person, Address)]\n// SELECT x1.id, x1.name, x2.street, x2.zip, x2.fk \n// FROM Person x1 INNER JOIN Address x2 ON x1.id = x2.fk\n \n// Left (Outer) Join\nval q = quote {\n  query[Person].leftJoin(query[Address]).on((p, a) => p.id == a.fk)\n}\n \nctx.run(q) //: List[(Person, Option[Address])]\n// Note that when you use named-variables in your comprehension, Quill does its best to honor them in the query.\n// SELECT p.id, p.name, a.street, a.zip, a.fk \n// FROM Person p LEFT JOIN Address a ON p.id = a.fk\n \n// Right (Outer) Join\nval q = quote {\n  query[Person].rightJoin(query[Address]).on((p, a) => p.id == a.fk)\n}\n \nctx.run(q) //: List[(Option[Person], Address)]\n// SELECT p.id, p.name, a.street, a.zip, a.fk \n// FROM Person p RIGHT JOIN Address a ON p.id = a.fk\n \n// Full (Outer) Join\nval q = quote {\n  query[Person].fullJoin(query[Address]).on((p, a) => p.id == a.fk)\n}\n \nctx.run(q) //: List[(Option[Person], Option[Address])]\n// SELECT p.id, p.name, a.street, a.zip, a.fk \n// FROM Person p FULL JOIN Address a ON p.id = a.fk\n```\n \nWhat about joining more than two tables with the applicative syntax?\nHere\'s how to do that:\n```scala\ncase class Company(zip: Int)\n\n// All is well for two tables but for three or more, the nesting mess begins:\nval q = quote {\n  query[Person]\n    .join(query[Address]).on({case (p, a) => p.id == a.fk}) // Let\'s use `case` here to stay consistent\n    .join(query[Company]).on({case ((p, a), c) => a.zip == c.zip})\n}\n \nctx.run(q) //: List[((Person, Address), Company)]\n// (Unfortunately when you use `case` statements, Quill can\'t help you with the variables names either!)\n// SELECT x01.id, x01.name, x11.street, x11.zip, x11.fk, x12.name, x12.zip \n// FROM Person x01 INNER JOIN Address x11 ON x01.id = x11.fk INNER JOIN Company x12 ON x11.zip = x12.zip\n```\nNo worries though, implicit joins and flat joins have your other use-cases covered!\n\n#### implicit joins\n\nQuill\'s implicit joins use a monadic syntax making them pleasant to use for joining many tables together.\nThey look a lot like Scala collections when used in for-comprehensions\nmaking them familiar to a typical Scala developer. \nWhat\'s the catch? They can only do inner-joins.\n\n```scala\nval q = quote {\n  for {\n    p <- query[Person]\n    a <- query[Address] if (p.id == a.fk)\n  } yield (p, a)\n}\n \nrun(q) //: List[(Person, Address)]\n// SELECT p.id, p.name, a.street, a.zip, a.fk \n// FROM Person p, Address a WHERE p.id = a.fk\n```\n \nNow, this is great because you can keep adding more and more joins\nwithout having to do any pesky nesting.\n```scala\nval q = quote {\n  for {\n    p <- query[Person]\n    a <- query[Address] if (p.id == a.fk)\n    c <- query[Address] if (c.zip == a.zip)\n  } yield (p, a, c)\n}\n \nrun(q) //: List[(Person, Address, Company)]\n// SELECT p.id, p.name, a.street, a.zip, a.fk, c.name, c.zip \n// FROM Person p, Address a, Company c WHERE p.id = a.fk AND c.zip = a.zip\n```\nWell that looks nice but wait! What If I need to inner, **and** outer join lots of tables nicely?\nNo worries, flat-joins are here to help!\n\n### flat joins\n\nFlat Joins give you the best of both worlds! In the monadic syntax, you can use both inner joins,\nand left-outer joins together without any of that pesky nesting.\n\n```scala\n// Inner Join\nval q = quote {\n  for { \n    p <- query[Person]\n    a <- query[Address].join(a => a.fk == p.id)\n  } yield (p,a)\n}\n \nctx.run(q) //: List[(Person, Address)]\n// SELECT p.id, p.name, a.street, a.zip, a.fk\n// FROM Person p INNER JOIN Address a ON a.fk = p.id\n \n// Left (Outer) Join\nval q = quote {\n  for { \n    p <- query[Person]\n    a <- query[Address].leftJoin(a => a.fk == p.id)\n  } yield (p,a)\n}\n \nctx.run(q) //: List[(Person, Option[Address])]\n// SELECT p.id, p.name, a.street, a.zip, a.fk \n// FROM Person p LEFT JOIN Address a ON a.fk = p.id\n```\n \nNow you can keep adding both right and left joins without nesting!\n```scala\nval q = quote {\n  for { \n    p <- query[Person]\n    a <- query[Address].join(a => a.fk == p.id)\n    c <- query[Company].leftJoin(c => c.zip == a.zip)\n  } yield (p,a,c)\n}\n \nctx.run(q) //: List[(Person, Address, Option[Company])]\n// SELECT p.id, p.name, a.street, a.zip, a.fk, c.name, c.zip \n// FROM Person p \n// INNER JOIN Address a ON a.fk = p.id \n// LEFT JOIN Company c ON c.zip = a.zip\n```\n\nCan\'t figure out what kind of join you want to use? Who says you have to choose?\n\nWith Quill the following multi-join queries are equivalent, use them according to preference:\n\n```scala\n\ncase class Employer(id: Int, personId: Int, name: String)\n\nval qFlat = quote {\n  for{\n    (p,e) <- query[Person].join(query[Employer]).on(_.id == _.personId)\n       c  <- query[Contact].leftJoin(_.personId == p.id)\n  } yield(p, e, c)\n}\n\nval qNested = quote {\n  for{\n    ((p,e),c) <-\n      query[Person].join(query[Employer]).on(_.id == _.personId)\n      .leftJoin(query[Contact]).on(\n        _._1.id == _.personId\n      )\n  } yield(p, e, c)\n}\n\nctx.run(qFlat)\nctx.run(qNested)\n// SELECT p.id, p.name, p.age, e.id, e.personId, e.name, c.id, c.phone\n// FROM Person p INNER JOIN Employer e ON p.id = e.personId LEFT JOIN Contact c ON c.personId = p.id\n```\n\nNote that in some cases implicit and flat joins cannot be used together, for example, the following\nquery will fail.\n```scala\nval q = quote {\n  for {\n    p <- query[Person]\n    p1 <- query[Person] if (p1.name == p.name)\n    c <- query[Contact].leftJoin(_.personId == p.id)\n  } yield (p, c)\n}\n \n// ctx.run(q)\n// java.lang.IllegalArgumentException: requirement failed: Found an `ON` table reference of a table that is \n// not available: Set(p). The `ON` condition can only use tables defined through explicit joins.\n```\nThis happens because an explicit join typically cannot be done after an implicit join in the same query.\n \nA good guideline is in any query or subquery, choose one of the following:\n * Use flat-joins + applicative joins or\n * Use implicit joins\n \nAlso, note that not all Option operations are available on outer-joined tables (i.e. tables wrapped in an `Option` object),\nonly a specific subset. This is mostly due to the inherent limitations of SQL itself. For more information, see the\n\'Optional Tables\' section.\n\n### Optionals / Nullable Fields\n\n> Note that the behavior of Optionals has recently changed to include stricter null-checks. See the [orNull / getOrNull](#ornull--getornull) section for more details.\n\nOption objects are used to encode nullable fields.\nSay you have the following schema:\n```sql\nCREATE TABLE Person(\n  id INT NOT NULL PRIMARY KEY,\n  name VARCHAR(255) -- This is nullable!\n);\nCREATE TABLE Address(\n  fk INT, -- This is nullable!\n  street VARCHAR(255) NOT NULL,\n  zip INT NOT NULL,\n  CONSTRAINT a_to_p FOREIGN KEY (fk) REFERENCES Person(id)\n);\nCREATE TABLE Company(\n  name VARCHAR(255) NOT NULL,\n  zip INT NOT NULL\n)\n```\nThis would encode to the following:\n```scala\ncase class Person(id:Int, name:Option[String])\ncase class Address(fk:Option[Int], street:String, zip:Int)\ncase class Company(name:String, zip:Int)\n```\n\nSome important notes regarding Optionals and nullable fields.\n\n> In many cases, Quill tries to rely on the null-fallthrough behavior that is ANSI standard:\n>  * `null == null := false`\n>  * `null == [true | false] := false`\n> \n> This allows the generated SQL for most optional operations to be simple. For example, the expression\n> `Option[String].map(v => v + "foo")` can be expressed as the SQL `v || \'foo\'` as opposed to \n> `CASE IF (v is not null) v || \'foo\' ELSE null END` so long as the concatenation operator `||`\n> "falls-through" and returns `null` when the input is null. This is not true of all databases (e.g. [Oracle](https://community.oracle.com/ideas/19866)),\n> forcing Quill to return the longer expression with explicit null-checking. Also, if there are conditionals inside\n> of an Option operation (e.g. `o.map(v => if (v == "x") "y" else "z")`) this creates SQL with case statements,\n> which will never fall-through when the input value is null. This forces Quill to explicitly null-check such statements in every\n> SQL dialect.\n\nLet\'s go through the typical operations of optionals.\n\n#### isDefined / isEmpty\n\nThe `isDefined` method is generally a good way to null-check a nullable field:\n```scala\nval q = quote {\n  query[Address].filter(a => a.fk.isDefined)\n}\nctx.run(q)\n// SELECT a.fk, a.street, a.zip FROM Address a WHERE a.fk IS NOT NULL\n```\nThe `isEmpty` method works the same way:\n```scala\nval q = quote {\n  query[Address].filter(a => a.fk.isEmpty)\n}\nctx.run(q)\n// SELECT a.fk, a.street, a.zip FROM Address a WHERE a.fk IS NULL\n```\n\n \n#### exists\n\nThis method is typically used for inspecting nullable fields inside of boolean conditions, most notably joining!\n```scala\nval q = quote {\n  query[Person].join(query[Address]).on((p, a)=> a.fk.exists(_ == p.id))\n}\nctx.run(q)\n// SELECT p.id, p.name, a.fk, a.street, a.zip FROM Person p INNER JOIN Address a ON a.fk = p.id\n```\n\nNote that in the example above, the `exists` method does not cause the generated\nSQL to do an explicit null-check in order to express the `False` case. This is because Quill relies on the\ntypical database behavior of immediately falsifying a statement that has `null` on one side of the equation.\n\n#### forall\n\nUse this method in boolean conditions that should succeed in the null case.\n```scala\nval q = quote {\n  query[Person].join(query[Address]).on((p, a) => a.fk.forall(_ == p.id))\n}\nctx.run(q)\n// SELECT p.id, p.name, a.fk, a.street, a.zip FROM Person p INNER JOIN Address a ON a.fk IS NULL OR a.fk = p.id\n```\nTypically this is useful when doing negative conditions, e.g. when a field is **not** some specified value (e.g. `"Joe"`).\nBeing `null` in this case is typically a matching result.\n```scala\nval q = quote {\n  query[Person].filter(p => p.name.forall(_ != "Joe"))\n}\n \nctx.run(q)\n// SELECT p.id, p.name FROM Person p WHERE p.name IS NULL OR p.name <> \'Joe\'\n```\n\n#### map\nAs in regular Scala code, performing any operation on an optional value typically requires using the `map` function.\n```scala\nval q = quote {\n for {\n    p <- query[Person]\n  } yield (p.id, p.name.map("Dear " + _))\n}\n \nctx.run(q)\n// SELECT p.id, \'Dear \' || p.name FROM Person p\n// * In Dialects where `||` does not fall-through for nulls (e.g. Oracle):\n// * SELECT p.id, CASE WHEN p.name IS NOT NULL THEN \'Dear \' || p.name ELSE null END FROM Person p\n```\n\nAdditionally, this method is useful when you want to get a non-optional field out of an outer-joined table\n(i.e. a table wrapped in an `Option` object).\n\n```scala\nval q = quote {\n  query[Company].leftJoin(query[Address])\n    .on((c, a) => c.zip == a.zip)\n    .map {case(c,a) =>                          // Row type is (Company, Option[Address])\n      (c.name, a.map(_.street), a.map(_.zip))   // Use `Option.map` to get `street` and `zip` fields\n    }\n}\n \nrun(q)\n// SELECT c.name, a.street, a.zip FROM Company c LEFT JOIN Address a ON c.zip = a.zip\n```\n\nFor more details about this operation (and some caveats), see the \'Optional Tables\' section.\n\n#### flatMap and flatten\n\nUse these when the `Option.map` functionality is not sufficient. This typically happens when you need to manipulate\nmultiple nullable fields in a way which would otherwise result in `Option[Option[T]]`.\n```scala\nval q = quote {\n  for {\n    a <- query[Person]\n    b <- query[Person] if (a.id > b.id)\n  } yield (\n    // If this was `a.name.map`, resulting record type would be Option[Option[String]]\n    a.name.flatMap(an =>\n      b.name.map(bn => \n        an+" comes after "+bn)))\n}\n \nctx.run(q) //: List[Option[String]]\n// SELECT (a.name || \' comes after \') || b.name FROM Person a, Person b WHERE a.id > b.id\n// * In Dialects where `||` does not fall-through for nulls (e.g. Oracle):\n// * SELECT CASE WHEN a.name IS NOT NULL AND b.name IS NOT NULL THEN (a.name || \' comes after \') || b.name ELSE null END FROM Person a, Person b WHERE a.id > b.id\n \n// Alternatively, you can use `flatten`\nval q = quote {\n  for {\n    a <- query[Person]\n    b <- query[Person] if (a.id > b.id)\n  } yield (\n    a.name.map(an => \n      b.name.map(bn => \n        an + " comes after " + bn)).flatten)\n}\n \nctx.run(q) //: List[Option[String]]\n// SELECT (a.name || \' comes after \') || b.name FROM Person a, Person b WHERE a.id > b.id\n``` \nThis is also very useful when selecting from outer-joined tables i.e. where the entire table\nis inside of an `Option` object. Note how below we get the `fk` field from `Option[Address]`.\n\n```scala\nval q = quote {\n  query[Person].leftJoin(query[Address])\n    .on((p, a) => a.fk.exists(_ == p.id))\n    .map {case (p /*Person*/, a /*Option[Address]*/) => (p.name, a.flatMap(_.fk))}\n}\n \nctx.run(q) //: List[(Option[String], Option[Int])]\n// SELECT p.name, a.fk FROM Person p LEFT JOIN Address a ON a.fk = p.id\n```\n\n#### orNull / getOrNull\n\nThe `orNull` method can be used to convert an Option-enclosed row back into a regular row.\nSince `Option[T].orNull` does not work for primitive types (e.g. `Int`, `Double`, etc...),\nyou can use the `getOrNull` method inside of quoted blocks to do the same thing.\n\n> Note that since the presence of null columns can cause queries to break in some data sources (e.g. Spark), so use this operation very carefully.\n\n```scala\nval q = quote {\n  query[Person].join(query[Address])\n    .on((p, a) => a.fk.exists(_ == p.id))\n    .filter {case (p /*Person*/, a /*Option[Address]*/) => \n      a.fk.getOrNull != 123 } // Exclude a particular value from the query.\n                              // Since we already did an inner-join on this value, we know it is not null.\n}\n \nctx.run(q) //: List[(Address, Person)]\n// SELECT p.id, p.name, a.fk, a.street, a.zip FROM Person p INNER JOIN Address a ON a.fk IS NOT NULL AND a.fk = p.id WHERE a.fk <> 123\n```\n\nIn certain situations, you may wish to pretend that a nullable-field is not actually nullable and perform regular operations\n(e.g. arithmetic, concatenation, etc...) on the field. You can use a combination of `Option.apply` and `orNull` (or `getOrNull` where needed)\nin order to do this.\n\n```scala\nval q = quote {\n  query[Person].map(p => Option(p.name.orNull + " suffix"))\n}\n \nctx.run(q)\n// SELECT p.name || \' suffix\' FROM Person p \n// i.e. same as the previous behavior\n```\n\nIn all other situations, since Quill strictly checks nullable values, and `case.. if` conditionals will work correctly in all Optional constructs.\nHowever, since they may introduce behavior changes in your codebase, the following warning has been introduced:\n\n> Conditionals inside of Option.[map | flatMap | exists | forall] will create a `CASE` statement in order to properly null-check the sub-query (...)\n\n```\nval q = quote {\n  query[Person].map(p => p.name.map(n => if (n == "Joe") "foo" else "bar").getOrElse("baz"))\n}\n// Information:(16, 15) Conditionals inside of Option.map will create a `CASE` statement in order to properly null-check the sub-query: `p.name.map((n) => if(n == "Joe") "foo" else "bar")`. \n// Expressions like Option(if (v == "foo") else "bar").getOrElse("baz") will now work correctly, but expressions that relied on the broken behavior (where "bar" would be returned instead) need to be modified  (see the "orNull / getOrNull" section of the documentation of more detail).\n \nctx.run(a)\n// Used to be this:\n// SELECT CASE WHEN CASE WHEN p.name = \'Joe\' THEN \'foo\' ELSE \'bar\' END IS NOT NULL THEN CASE WHEN p.name = \'Joe\' THEN \'foo\' ELSE \'bar\' END ELSE \'baz\' END FROM Person p\n// Now is this:\n// SELECT CASE WHEN p.name IS NOT NULL AND CASE WHEN p.name = \'Joe\' THEN \'foo\' ELSE \'bar\' END IS NOT NULL THEN CASE WHEN p.name = \'Joe\' THEN \'foo\' ELSE \'bar\' END ELSE \'baz\' END FROM Person p\n```\n\n### equals\n\nThe `==`, `!=`, and `.equals` methods can be used to compare regular types as well Option types in a scala-idiomatic way.\nThat is to say, either `T == T` or `Option[T] == Option[T]` is supported and the following "truth-table" is observed:\n\nLeft         | Right        | Equality   | Result\n-------------|--------------|------------|----------\n`a`          | `b`          | `==`       | `a == b`\n`Some[T](a)` | `Some[T](b)` | `==`       | `a == b`\n`Some[T](a)` | `None`       | `==`       | `false`\n`None      ` | `Some[T](b)` | `==`       | `false`\n`None      ` | `None`       | `==`       | `true`\n`Some[T]   ` | `Some[R]   ` | `==`       | Exception thrown.\n`a`          | `b`          | `!=`       | `a != b`\n`Some[T](a)` | `Some[T](b)` | `!=`       | `a != b`\n`Some[T](a)` | `None`       | `!=`       | `true`\n`None      ` | `Some[T](b)` | `!=`       | `true`\n`Some[T]   ` | `Some[R]   ` | `!=`       | Exception thrown.\n`None      ` | `None`       | `!=`       | `false`\n\n```scala\ncase class Node(id:Int, status:Option[String], otherStatus:Option[String])\n\nval q = quote { query[Node].filter(n => n.id == 123) }\nctx.run(q)\n// SELECT n.id, n.status, n.otherStatus FROM Node n WHERE p.id = 123\n\nval q = quote { query[Node].filter(r => r.status == r.otherStatus) }\nctx.run(q)\n// SELECT r.id, r.status, r.otherStatus FROM Node r WHERE r.status IS NULL AND r.otherStatus IS NULL OR r.status = r.otherStatus\n\nval q = quote { query[Node].filter(n => n.status == Option("RUNNING")) }\nctx.run(q)\n// SELECT n.id, n.status, n.otherStatus FROM node n WHERE n.status IS NOT NULL AND n.status = \'RUNNING\'\n\nval q = quote { query[Node].filter(n => n.status != Option("RUNNING")) }\nctx.run(q)\n// SELECT n.id, n.status, n.otherStatus FROM node n WHERE n.status IS NULL OR n.status <> \'RUNNING\'\n```\n\nIf you would like to use an equality operator that follows that ansi-idiomatic approach, failing\nthe comparison if either side is null as well as the principle that `null = null := false`, you can import `===` (and `=!=`) \nfrom `Context.extras`. These operators work across `T` and `Option[T]` allowing comparisons like `T === Option[T]`,\n`Option[T] == T` etc... to be made. You can use also `===`\ndirectly in Scala code and it will have the same behavior, returning `false` when other the left-hand\nor right-hand side is `None`. This is particularity useful in paradigms like Spark where\nyou will typically transition inside and outside of Quill code.\n\n> When using `a === b` or `a =!= b` sometimes you will see the extra `a IS NOT NULL AND b IS NOT NULL` comparisons\n> and sometimes you will not. This depends on `equalityBehavior` in `SqlIdiom` which determines whether the given SQL\n> dialect already does ansi-idiomatic comparison to `a`, and `b` when an `=` operator is used,\n> this allows us to omit the extra `a IS NOT NULL AND b IS NOT NULL`.\n\n\n```scala\nimport ctx.extras._\n\n// === works the same way inside of a quotation\nval q = run( query[Node].filter(n => n.status === "RUNNING") )\n// SELECT n.id, n.status FROM node n WHERE n.status IS NOT NULL AND n.status = \'RUNNING\'\n\n// as well as outside\n(nodes:List[Node]).filter(n => n.status === "RUNNING")\n```\n\n#### Optional Tables\n\nAs we have seen in the examples above, only the `map` and `flatMap` methods are available on outer-joined tables\n(i.e. tables wrapped in an `Option` object).\n \nSince you cannot use `Option[Table].isDefined`, if you want to null-check a whole table\n(e.g. if a left-join was not matched), you have to `map` to a specific field on which you can do the null-check.\n\n```scala\nval q = quote {\n  query[Company].leftJoin(query[Address])\n    .on((c, a) => c.zip == a.zip)         // Row type is (Company, Option[Address])\n    .filter({case(c,a) => a.isDefined})   // You cannot null-check a whole table\n}\n```\n \nInstead, map the row-variable to a specific field and then check that field.\n```scala\nval q = quote {\n  query[Company].leftJoin(query[Address])\n    .on((c, a) => c.zip == a.zip)                     // Row type is (Company, Option[Address])\n    .filter({case(c,a) => a.map(_.street).isDefined}) // Null-check a non-nullable field instead\n}\nctx.run(q)\n// SELECT c.name, c.zip, a.fk, a.street, a.zip \n// FROM Company c \n// LEFT JOIN Address a ON c.zip = a.zip \n// WHERE a.street IS NOT NULL\n```\n \nFinally, it is worth noting that a whole table can be wrapped into an `Option` object. This is particularly\nuseful when doing a union on table-sets that are both right-joined and left-joined together.\n```scala\nval aCompanies = quote {\n  for {\n    c <- query[Company] if (c.name like "A%")\n    a <- query[Address].join(_.zip == c.zip)\n  } yield (c, Option(a))  // change (Company, Address) to (Company, Option[Address]) \n}\nval bCompanies = quote {\n  for {\n    c <- query[Company] if (c.name like "A%")\n    a <- query[Address].leftJoin(_.zip == c.zip)\n  } yield (c, a) // (Company, Option[Address])\n}\nval union = quote {\n  aCompanies union bCompanies\n}\nctx.run(union)\n// SELECT x.name, x.zip, x.fk, x.street, x.zip FROM (\n// (SELECT c.name name, c.zip zip, x1.zip zip, x1.fk fk, x1.street street \n// FROM Company c INNER JOIN Address x1 ON x1.zip = c.zip WHERE c.name like \'A%\') \n// UNION \n// (SELECT c1.name name, c1.zip zip, x2.zip zip, x2.fk fk, x2.street street \n// FROM Company c1 LEFT JOIN Address x2 ON x2.zip = c1.zip WHERE c1.name like \'A%\')\n// ) x\n```\n\n### Ad-Hoc Case Classes\n\nCase Classes can also be used inside quotations as output values:\n\n```scala\ncase class Person(id: Int, name: String, age: Int)\ncase class Contact(personId: Int, phone: String)\ncase class ReachablePerson(name:String, phone: String)\n\nval q = quote {\n  for {\n    p <- query[Person] if(p.id == 999)\n    c <- query[Contact] if(c.personId == p.id)\n  } yield {\n    ReachablePerson(p.name, c.phone)\n  }\n}\n\nctx.run(q)\n// SELECT p.name, c.phone FROM Person p, Contact c WHERE (p.id = 999) AND (c.personId = p.id)\n```\n\nAs well as in general:\n\n```scala\ncase class IdFilter(id:Int)\n\nval q = quote {\n  val idFilter = new IdFilter(999)\n  for {\n    p <- query[Person] if(p.id == idFilter.id)\n    c <- query[Contact] if(c.personId == p.id)\n  } yield {\n    ReachablePerson(p.name, c.phone)\n  }\n}\n\nctx.run(q)\n// SELECT p.name, c.phone FROM Person p, Contact c WHERE (p.id = 999) AND (c.personId = p.id)\n```\n***Note*** however that this functionality has the following restrictions:\n1. The Ad-Hoc Case Class can only have one constructor with one set of parameters.\n2. The Ad-Hoc Case Class must be constructed inside the quotation using one of the following methods:\n    1. Using the `new` keyword: `new Person("Joe", "Bloggs")`\n    2. Using a companion object\'s apply method:  `Person("Joe", "Bloggs")`\n    3. Using a companion object\'s apply method explicitly: `Person.apply("Joe", "Bloggs")`\n4. Any custom logic in a constructor/apply-method of an Ad-Hoc case class will not be invoked when it is \'constructed\' inside a quotation. To construct an Ad-Hoc case class with custom logic inside a quotation, you can use a quoted method.\n\n## Query probing\n\nQuery probing validates queries against the database at compile time, failing the compilation if it is not valid. The query validation does not alter the database state.\n\nThis feature is disabled by default. To enable it, mix the `QueryProbing` trait to the database configuration:\n\n```\nobject myContext extends YourContextType with QueryProbing\n```\n\nThe context must be created in a separate compilation unit in order to be loaded at compile time. Please use [this guide](http://www.scala-sbt.org/0.13/docs/Macro-Projects.html) that explains how to create a separate compilation unit for macros, that also serves to the purpose of defining a query-probing-capable context. `context` could be used instead of `macros` as the name of the separate compilation unit.\n\nThe configurations correspondent to the config key must be available at compile time. You can achieve it by adding this line to your project settings:\n\n```\nunmanagedClasspath in Compile += baseDirectory.value / "src" / "main" / "resources"\n```\n\nIf your project doesn\'t have a standard layout, e.g. a play project, you should configure the path to point to the folder that contains your config file.\n\n## Actions\n\nDatabase actions are defined using quotations as well. These actions don\'t have a collection-like API but rather a custom DSL to express inserts, deletes, and updates.\n\n### insert\n\n```scala\nval a = quote(query[Contact].insert(lift(Contact(999, "+1510488988"))))\n\nctx.run(a)\n// INSERT INTO Contact (personId,phone) VALUES (?, ?)\n```\n\n#### It is also possible to insert specific columns:\n\n```scala\nval a = quote {\n  query[Contact].insert(_.personId -> lift(999), _.phone -> lift("+1510488988"))\n}\n\nctx.run(a)\n// INSERT INTO Contact (personId,phone) VALUES (?, ?)\n```\n\n### batch insert\n```scala\nval a = quote {\n  liftQuery(List(Person(0, "John", 31))).foreach(e => query[Person].insert(e))\n}\n\nctx.run(a)\n// INSERT INTO Person (id,name,age) VALUES (?, ?, ?)\n```\n\n### update\n```scala\nval a = quote {\n  query[Person].filter(_.id == 999).update(lift(Person(999, "John", 22)))\n}\n\nctx.run(a)\n// UPDATE Person SET id = ?, name = ?, age = ? WHERE id = 999\n```\n\n#### Using specific columns:\n\n```scala\nval a = quote {\n  query[Person].filter(p => p.id == lift(999)).update(_.age -> lift(18))\n}\n\nctx.run(a)\n// UPDATE Person SET age = ? WHERE id = ?\n```\n\n#### Using columns as part of the update:\n\n```scala\nval a = quote {\n  query[Person].filter(p => p.id == lift(999)).update(p => p.age -> (p.age + 1))\n}\n\nctx.run(a)\n// UPDATE Person SET age = (age + 1) WHERE id = ?\n```\n\n### batch update\n\n```scala\nval a = quote {\n  liftQuery(List(Person(1, "name", 31))).foreach { person =>\n     query[Person].filter(_.id == person.id).update(_.name -> person.name, _.age -> person.age)\n  }\n}\n\nctx.run(a)\n// UPDATE Person SET name = ?, age = ? WHERE id = ?\n```\n\n### delete\n```scala\nval a = quote {\n  query[Person].filter(p => p.name == "").delete\n}\n\nctx.run(a)\n// DELETE FROM Person WHERE name = \'\'\n```\n\n### insert or update (upsert, conflict)\n\nUpsert is supported by Postgres, SQLite, and MySQL\n\n#### Postgres and SQLite\n\n##### Ignore conflict\n```scala\nval a = quote {\n  query[Product].insert(_.id -> 1, _.sku -> 10).onConflictIgnore\n}\n\n// INSERT INTO Product AS t (id,sku) VALUES (1, 10) ON CONFLICT DO NOTHING\n```\n\nIgnore conflict by explicitly setting conflict target\n```scala\nval a = quote {\n  query[Product].insert(_.id -> 1, _.sku -> 10).onConflictIgnore(_.id)\n}\n\n// INSERT INTO Product AS t (id,sku) VALUES (1, 10) ON CONFLICT (id) DO NOTHING\n```\n\nMultiple properties can be used as well.\n```scala\nval a = quote {\n  query[Product].insert(_.id -> 1, _.sku -> 10).onConflictIgnore(_.id, _.description)\n}\n\n// INSERT INTO Product (id,sku) VALUES (1, 10) ON CONFLICT (id,description) DO NOTHING\n```\n\n##### Update on Conflict\n\nResolve conflict by updating existing row if needed. In `onConflictUpdate(target)((t, e) => assignment)`: `target` refers to\nconflict target, `t` - to existing row and `e` - to excluded, e.g. row proposed for insert.\n```scala\nval a = quote {\n  query[Product]\n    .insert(_.id -> 1, _.sku -> 10)\n    .onConflictUpdate(_.id)((t, e) => t.sku -> (t.sku + e.sku))\n}\n\n// INSERT INTO Product AS t (id,sku) VALUES (1, 10) ON CONFLICT (id) DO UPDATE SET sku = (t.sku + EXCLUDED.sku)\n```\nMultiple properties can be used with `onConflictUpdate` as well.\n```scala\nval a = quote {\n  query[Product]\n    .insert(_.id -> 1, _.sku -> 10)\n    .onConflictUpdate(_.id, _.description)((t, e) => t.sku -> (t.sku + e.sku))\n}\n\nINSERT INTO Product AS t (id,sku) VALUES (1, 10) ON CONFLICT (id,description) DO UPDATE SET sku = (t.sku + EXCLUDED.sku)\n```\n\n#### MySQL\n\nIgnore any conflict, e.g. `insert ignore`\n```scala\nval a = quote {\n  query[Product].insert(_.id -> 1, _.sku -> 10).onConflictIgnore\n}\n\n// INSERT IGNORE INTO Product (id,sku) VALUES (1, 10)\n```\n\nIgnore duplicate key conflict by explicitly setting it\n```scala\nval a = quote {\n  query[Product].insert(_.id -> 1, _.sku -> 10).onConflictIgnore(_.id)\n}\n\n// INSERT INTO Product (id,sku) VALUES (1, 10) ON DUPLICATE KEY UPDATE id=id\n```\n\nResolve duplicate key by updating existing row if needed. In `onConflictUpdate((t, e) => assignment)`: `t` refers to\nexisting row and `e` - to values, e.g. values proposed for insert.\n```scala\nval a = quote {\n  query[Product]\n    .insert(_.id -> 1, _.sku -> 10)\n    .onConflictUpdate((t, e) => t.sku -> (t.sku + e.sku))\n}\n\n// INSERT INTO Product (id,sku) VALUES (1, 10) ON DUPLICATE KEY UPDATE sku = (sku + VALUES(sku))\n```\n\n## Printing Queries\n\nThe `translate` method is used to convert a Quill query into a string which can then be printed.\n\n```scala\nval str = ctx.translate(query[Person])\nprintln(str)\n// SELECT x.id, x.name, x.age FROM Person x\n```\n\nInsert queries can also be printed:\n\n```scala\nval str = ctx.translate(query[Person].insert(lift(Person(0, "Joe", 45))))\nprintln(str)\n// INSERT INTO Person (id,name,age) VALUES (0, \'Joe\', 45)\n```\n\nAs well as batch insertions:\n\n```scala\nval q = quote {\n  liftQuery(List(Person(0, "Joe",44), Person(1, "Jack",45)))\n    .foreach(e => query[Person].insert(e))\n}\nval strs: List[String] = ctx.translate(q)\nstrs.map(println)\n// INSERT INTO Person (id, name,age) VALUES (0, \'Joe\', 44)\n// INSERT INTO Person (id, name,age) VALUES (1, \'Jack\', 45)\n```\n\nThe `translate` method is available in every Quill context as well as the Cassandra and OrientDB contexts,\nthe latter two, however, do not support Insert and Batch Insert query printing.\n\n## IO Monad\n\nQuill provides an IO monad that allows the user to express multiple computations and execute them separately. This mechanism is also known as a free monad, which provides a way of expressing computations as referentially-transparent values and isolates the unsafe IO operations into a single operation. For instance:\n\n```\n// this code using Future\n\ncase class Person(id: Int, name: String, age: Int)\n\nval p = Person(0, "John", 22)\nctx.run(query[Person].insert(lift(p))).flatMap { _ =>\n  ctx.run(query[Person])\n}\n\n// isn\'t referentially transparent because if you refactor the second database \n// interaction into a value, the result will be different:\n\nval allPeople = ctx.run(query[Person])\nctx.run(query[Person].insert(lift(p))).flatMap { _ =>\n  allPeople\n}\n\n// this happens because `ctx.run` executes the side-effect (database IO) immediately\n```\n\n```scala\n// The IO monad doesn\'t perform IO immediately, so both computations:\n\nval p = Person(0, "John", 22)\n\nval a =\n  ctx.runIO(query[Person].insert(lift(p))).flatMap { _ =>\n    ctx.runIO(query[Person])\n  }\n\n\nval allPeople = ctx.runIO(query[Person])\n\nval b =\n  ctx.runIO(query[Person].insert(lift(p))).flatMap { _ =>\n    allPeople\n  }\n\n// produce the same result when executed\n\nperformIO(a) == performIO(b)\n```\n\nThe IO monad has an interface similar to `Future`; please refer to [the class](https://github.com/getquill/quill/blob/master/quill-core/src/main/scala/io/getquill/monad/IOMonad.scala#L39) for more information regarding the available operations.\n\nThe return type of `performIO` varies according to the context. For instance, async contexts return `Future`s while JDBC returns values synchronously.\n\n***NOTE***: Avoid using the variable name `io` since it conflicts with Quill\'s package `io.getquill`, otherwise you will get the following error.\n```\nrecursive value io needs type\n```\n\n### IO Monad and transactions\n\n`IO` also provides the `transactional` method that delimits a transaction:\n\n```scala\nval a =\n  ctx.runIO(query[Person].insert(lift(p))).flatMap { _ =>\n    ctx.runIO(query[Person])\n  }\n\nperformIO(a.transactional) // note: transactional can be used outside of `performIO`\n```\n\n### Getting a ResultSet\n\nQuill JDBC Contexts allow you to use `prepare` in order to get a low-level `ResultSet` that is useful\nfor interacting with legacy APIs. This function  returns a `f: (Connection) => (PreparedStatement)` \nclosure as opposed to a `PreparedStatement` in order to guarantee that JDBC Exceptions are not\nthrown until you can wrap them into the appropriate Exception-handling mechanism (e.g.\n`try`/`catch`, `Try` etc...).\n\n```scala\nval q = quote {\n  query[Product].filter(_.id == 1)\n}\nval preparer: (Connection) => (PreparedStatement)  = ctx.prepare(q)\n// SELECT x1.id, x1.description, x1.sku FROM Product x1 WHERE x1.id = 1\n\n// Use ugly stateful code, bracketed effects, or try-with-resources here:\nvar preparedStatement: PreparedStatement = _\nvar resultSet: ResultSet = _\n\ntry {\n  preparedStatement = preparer(myCustomDataSource.getConnection)\n  resultSet = preparedStatement.executeQuery()\n} catch {\n  case e: Exception =>\n    // Close the preparedStatement and catch possible exceptions\n    // Close the resultSet and catch possible exceptions\n}\n```\n\nThe `prepare` function can also be used with `insert`, and `update` queries.\n\n```scala\nval q = quote {\n  query[Product].insert(lift(Product(1, "Desc", 123))\n}\nval preparer: (Connection) => (PreparedStatement)  = ctx.prepare(q)\n// INSERT INTO Product (id,description,sku) VALUES (?, ?, ?)\n```\n\nAs well as with batch queries.\n> Make sure to first quote your batch query and then pass the result into the `prepare` function\n(as is done in the example below) or the Scala compiler may not type the output correctly\n[#1518](https://github.com/getquill/quill/issues/1518).\n\n```scala\nval q = quote {\n  liftQuery(products).foreach(e => query[Product].insert(e))\n}\nval preparers: Connection => List[PreparedStatement] = ctx.prepare(q)\nval preparedStatement: List[PreparedStatement] = preparers(jdbcConf.dataSource.getConnection)\n```\n\n\n### Effect tracking\n\nThe IO monad tracks the effects that a computation performs in its second type parameter:\n\n```scala\nval a: IO[ctx.RunQueryResult[Person], Effect.Write with Effect.Read] =\n  ctx.runIO(query[Person].insert(lift(p))).flatMap { _ =>\n    ctx.runIO(query[Person])\n  }\n```\n\nThis mechanism is useful to limit the kind of operations that can be performed. See this [blog post](http://danielwestheide.com/blog/2015/06/28/put-your-writes-where-your-master-is-compile-time-restriction-of-slick-effect-types.html) as an example.\n\n## Implicit query\n\nQuill provides implicit conversions from case class companion objects to `query[T]` through an additional trait:\n\n```scala\nval ctx = new SqlMirrorContext(MirrorSqlDialect, Literal) with ImplicitQuery\n\nimport ctx._\n\nval q = quote {\n  for {\n    p <- Person if(p.id == 999)\n    c <- Contact if(c.personId == p.id)\n  } yield {\n    (p.name, c.phone)\n  }\n}\n\nctx.run(q)\n// SELECT p.name, c.phone FROM Person p, Contact c WHERE (p.id = 999) AND (c.personId = p.id)\n```\n\nNote the usage of `Person` and `Contact` instead of `query[Person]` and `query[Contact]`.\n\n## SQL-specific operations\n\nSome operations are SQL-specific and not provided with the generic quotation mechanism. The SQL contexts provide implicit classes for this kind of operation:\n\n```scala\nval ctx = new SqlMirrorContext(MirrorSqlDialect, Literal)\nimport ctx._\n```\n\n### like\n\n```scala\nval q = quote {\n  query[Person].filter(p => p.name like "%John%")\n}\nctx.run(q)\n// SELECT p.id, p.name, p.age FROM Person p WHERE p.name like \'%John%\'\n```\n\n## SQL-specific encoding\n\n### Arrays\n\nQuill provides SQL Arrays support. In Scala we represent them as any collection that implements `Seq`:\n```scala\nimport java.util.Date\n\ncase class Book(id: Int, notes: List[String], pages: Vector[Int], history: Seq[Date])\n\nctx.run(query[Book])\n// SELECT x.id, x.notes, x.pages, x.history FROM Book x\n```\nNote that not all drivers/databases provides such feature hence only `PostgresJdbcContext` and\n`PostgresAsyncContext` support SQL Arrays.\n\n\n## Cassandra-specific encoding\n\n```scala\nval ctx = new CassandraMirrorContext(Literal)\nimport ctx._\n```\n\n### Collections\n\nThe Cassandra context provides List, Set, and Map encoding:\n\n```scala\n\ncase class Book(id: Int, notes: Set[String], pages: List[Int], history: Map[Int, Boolean])\n\nctx.run(query[Book])\n// SELECT id, notes, pages, history FROM Book\n```\n\n### User-Defined Types\n\nThe cassandra context provides encoding of UDT (user-defined types).\n```scala\nimport io.getquill.context.cassandra.Udt\n\ncase class Name(firstName: String, lastName: String) extends Udt\n```\n\nTo encode the UDT and bind it into the query (insert/update queries), the context needs to retrieve UDT metadata from\nthe cluster object. By default, the context looks for UDT metadata within the currently logged keyspace, but it\'s also possible to specify a\nconcrete keyspace with `udtMeta`:\n\n```scala\nimplicit val nameMeta = udtMeta[Name]("keyspace2.my_name")\n```\nWhen a keyspace is not set in `udtMeta` then the currently logged one is used.\n\nSince it\'s possible to create a context without\nspecifying a keyspace, (e.g. the keyspace parameter is null and the session is not bound to any keyspace), the UDT metadata will be\nresolved throughout the entire cluster.\n\nIt is also possible to rename UDT columns with `udtMeta`:\n\n```scala\nimplicit val nameMeta = udtMeta[Name]("name", _.firstName -> "first", _.lastName -> "last")\n```\n\n## Cassandra-specific operations\n\nThe cassandra context also provides a few additional operations:\n\n### allowFiltering\n```scala\nval q = quote {\n  query[Person].filter(p => p.age > 10).allowFiltering\n}\nctx.run(q)\n// SELECT id, name, age FROM Person WHERE age > 10 ALLOW FILTERING\n```\n\n### ifNotExists\n```scala\nval q = quote {\n  query[Person].insert(_.age -> 10, _.name -> "John").ifNotExists\n}\nctx.run(q)\n// INSERT INTO Person (age,name) VALUES (10, \'John\') IF NOT EXISTS\n```\n\n### ifExists\n```scala\nval q = quote {\n  query[Person].filter(p => p.name == "John").delete.ifExists\n}\nctx.run(q)\n// DELETE FROM Person WHERE name = \'John\' IF EXISTS\n```\n\n### usingTimestamp\n```scala\nval q1 = quote {\n  query[Person].insert(_.age -> 10, _.name -> "John").usingTimestamp(99)\n}\nctx.run(q1)\n// INSERT INTO Person (age,name) VALUES (10, \'John\') USING TIMESTAMP 99\n\nval q2 = quote {\n  query[Person].usingTimestamp(99).update(_.age -> 10)\n}\nctx.run(q2)\n// UPDATE Person USING TIMESTAMP 99 SET age = 10\n```\n\n### usingTtl\n```scala\nval q1 = quote {\n  query[Person].insert(_.age -> 10, _.name -> "John").usingTtl(11)\n}\nctx.run(q1)\n// INSERT INTO Person (age,name) VALUES (10, \'John\') USING TTL 11\n\nval q2 = quote {\n  query[Person].usingTtl(11).update(_.age -> 10)\n}\nctx.run(q2)\n// UPDATE Person USING TTL 11 SET age = 10\n\nval q3 = quote {\n  query[Person].usingTtl(11).filter(_.name == "John").delete\n}\nctx.run(q3)  \n// DELETE FROM Person USING TTL 11 WHERE name = \'John\'\n```\n\n### using\n```scala\nval q1 = quote {\n  query[Person].insert(_.age -> 10, _.name -> "John").using(ts = 99, ttl = 11)\n}\nctx.run(q1)\n// INSERT INTO Person (age,name) VALUES (10, \'John\') USING TIMESTAMP 99 AND TTL 11\n\nval q2 = quote {\n  query[Person].using(ts = 99, ttl = 11).update(_.age -> 10)\n}\nctx.run(q2)\n// UPDATE Person USING TIMESTAMP 99 AND TTL 11 SET age = 10\n\nval q3 = quote {\n  query[Person].using(ts = 99, ttl = 11).filter(_.name == "John").delete\n}\nctx.run(q3)\n// DELETE FROM Person USING TIMESTAMP 99 AND TTL 11 WHERE name = \'John\'\n```\n\n### ifCond\n```scala\nval q1 = quote {\n  query[Person].update(_.age -> 10).ifCond(_.name == "John")\n}\nctx.run(q1)\n// UPDATE Person SET age = 10 IF name = \'John\'\n\nval q2 = quote {\n  query[Person].filter(_.name == "John").delete.ifCond(_.age == 10)\n}\nctx.run(q2)\n// DELETE FROM Person WHERE name = \'John\' IF age = 10\n```\n\n### delete column\n```scala\nval q = quote {\n  query[Person].map(p => p.age).delete\n}\nctx.run(q)\n// DELETE p.age FROM Person\n```\n\n### list.contains / set.contains\nrequires `allowFiltering`\n```scala\nval q = quote {\n  query[Book].filter(p => p.pages.contains(25)).allowFiltering\n}\nctx.run(q)\n// SELECT id, notes, pages, history FROM Book WHERE pages CONTAINS 25 ALLOW FILTERING\n```\n\n### map.contains\nrequires `allowFiltering`\n```scala\nval q = quote {\n  query[Book].filter(p => p.history.contains(12)).allowFiltering\n}\nctx.run(q)\n// SELECT id, notes, pages, history FROM book WHERE history CONTAINS 12 ALLOW FILTERING\n```\n\n### map.containsValue\nrequires `allowFiltering`\n```scala\nval q = quote {\n  query[Book].filter(p => p.history.containsValue(true)).allowFiltering\n}\nctx.run(q)\n// SELECT id, notes, pages, history FROM book WHERE history CONTAINS true ALLOW FILTERING\n```\n\n## Dynamic queries\n\nQuill\'s default operation mode is compile-time, but there are queries that have their structure defined only at runtime. Quill automatically falls back to runtime normalization and query generation if the query\'s structure is not static. Example:\n\n```scala\nval ctx = new SqlMirrorContext(MirrorSqlDialect, Literal)\n\nimport ctx._\n\nsealed trait QueryType\ncase object Minor extends QueryType\ncase object Senior extends QueryType\n\ndef people(t: QueryType): Quoted[Query[Person]] =\n  t match {\n    case Minor => quote {\n      query[Person].filter(p => p.age < 18)\n    }\n    case Senior => quote {\n      query[Person].filter(p => p.age > 65)\n    }\n  }\n\nctx.run(people(Minor))\n// SELECT p.id, p.name, p.age FROM Person p WHERE p.age < 18\n\nctx.run(people(Senior))\n// SELECT p.id, p.name, p.age FROM Person p WHERE p.age > 65\n```\n\n### Dynamic query API\n\nAdditionally, Quill provides a separate query API to facilitate the creation of dynamic queries. This API allows users to easily manipulate quoted values instead of working only with quoted transformations. \n\n**Important**: A few of the dynamic query methods accept runtime string values. It\'s important to keep in mind that these methods could be a vector for SQL injection.\n\nLet\'s use the `filter` transformation as an example. In the regular API, this method has no implementation since it\'s an abstract member of a trait:\n\n```\ndef filter(f: T => Boolean): EntityQuery[T]\n```\n\nIn the dynamic API, `filter` is has a different signature and a body that is executed at runtime:\n\n```\ndef filter(f: Quoted[T] => Quoted[Boolean]): DynamicQuery[T] =\n  transform(f, Filter)\n```\n\nIt takes a `Quoted[T]` as input and produces a `Quoted[Boolean]`. The user is free to use regular scala code within the transformation:\n\n```scala\ndef people(onlyMinors: Boolean) =\n  dynamicQuery[Person].filter(p => if(onlyMinors) quote(p.age < 18) else quote(true))\n```\n\nIn order to create a dynamic query, use one of the following methods:\n\n```scala\ndynamicQuery[Person]\ndynamicQuerySchema[Person]("people", alias(_.name, "pname"))\n```\n\nIt\'s also possible to transform a `Quoted` into a dynamic query:\n\n```scala\nval q = quote {\n  query[Person]\n}\nq.dynamic.filter(p => quote(p.name == "John"))\n```\n\nThe dynamic query API is very similar to the regular API but has a few differences:\n\n**Queries**\n```scala\n// schema queries use `alias` instead of tuples\ndynamicQuerySchema[Person]("people", alias(_.name, "pname"))\n\n// this allows users to use a dynamic list of aliases\nval aliases = List(alias[Person](_.name, "pname"), alias[Person](_.age, "page"))\ndynamicQuerySchema[Person]("people", aliases:_*)\n\n// a few methods have an overload with the `Opt` suffix,\n// which apply the transformation only if the option is defined:\n\ndef people(minAge: Option[Int]) =\n  dynamicQuery[Person].filterOpt(minAge)((person, minAge) => quote(person.age >= minAge))\n\ndef people(maxRecords: Option[Int]) =\n  dynamicQuery[Person].takeOpt(maxRecords)\n\ndef people(dropFirst: Option[Int]) =\n  dynamicQuery[Person].dropOpt(dropFirst)\n  \n// method with `If` suffix, for better chaining  \ndef people(userIds: Seq[Int]) =\n  dynamicQuery[Person].filterIf(userIds.nonEmpty)(person => quote(liftQuery(userIds).contains(person.id)))\n```\n\n**Actions**\n```scala\n// actions use `set` \ndynamicQuery[Person].update(set(_.name, quote("John")))\n\n// or `setValue` if the value is not quoted\ndynamicQuery[Person].insert(setValue(_.name, "John"))\n\n// or `setOpt` that will be applied only the option is defined\ndynamicQuery[Person].insert(setOpt(_.name, Some("John")))\n\n// it\'s also possible to use a runtime string value as the column name\ndynamicQuery[Person].update(set("name", quote("John")))\n\n// to insert or update a case class instance, use `insertValue`/`updateValue`\nval p = Person(0, "John", 21)\ndynamicQuery[Person].insertValue(p)\ndynamicQuery[Person].updateValue(p)\n```\n\n# Extending quill\n\n## Infix\n\nInfix is a very flexible mechanism to use non-supported features without having to use plain queries in the target language. It allows the insertion of arbitrary strings within quotations.\n\nFor instance, quill doesn\'t support the `FOR UPDATE` SQL feature. It can still be used through infix and implicit classes:\n\n```scala\nimplicit class ForUpdate[T](q: Query[T]) {\n  def forUpdate = quote(infix"$q FOR UPDATE".as[Query[T]])\n}\n\nval a = quote {\n  query[Person].filter(p => p.age < 18).forUpdate\n}\n\nctx.run(a)\n// SELECT p.name, p.age FROM person p WHERE p.age < 18 FOR UPDATE\n```\n\nThe `forUpdate` quotation can be reused for multiple queries.\n\nQueries that contain `infix` will generally not be flattened since it is not assumed that the contents\nof the infix are a pure function.\n> Since SQL is typically less performant when there are many nested queries,\nbe careful with the use of `infix` in queries that have multiple `map`+`filter` clauses.\n\n```scala\ncase class Data(id: Int)\ncase class DataAndRandom(id: Int, value: Int)\n\n// This should be alright:\nval q = quote {\n  query[Data].map(e => DataAndRandom(e.id, infix"RAND()".as[Int])).filter(r => r.value <= 10)\n}\nrun(q)\n// SELECT e.id, e.value FROM (SELECT RAND() AS value, e.id AS id FROM Data e) AS e WHERE e.value <= 10\n\n// This might not be:\nval q = quote {\n  query[Data]\n    .map(e => DataAndRandom(e.id, infix"SOME_UDF(${e.id})".as[Int]))\n    .filter(r => r.value <= 10)\n    .map(e => DataAndRandom(e.id, infix"SOME_OTHER_UDF(${e.value})".as[Int]))\n    .filter(r => r.value <= 100)\n}\n// Produces too many layers of nesting!\nrun(q)\n// SELECT e.id, e.value FROM (\n//   SELECT SOME_OTHER_UDF(e.value) AS value, e.id AS id FROM (\n//     SELECT SOME_UDF(e.id) AS value, e.id AS id FROM Data e\n//   ) AS e WHERE e.value <= 10\n// ) AS e WHERE e.value <= 100\n```\n\nIf you are sure that the the content of your infix is a pure function, you canse use the `pure` method\nin order to indicate to Quill that the infix clause can be copied in the query. This gives Quill much\nmore leeway to flatten your query, possibly improving performance.\n\n```scala\nval q = quote {\n  query[Data]\n    .map(e => DataAndRandom(e.id, infix"SOME_UDF(${e.id})".pure.as[Int]))\n    .filter(r => r.value <= 10)\n    .map(e => DataAndRandom(e.id, infix"SOME_OTHER_UDF(${e.value})".pure.as[Int]))\n    .filter(r => r.value <= 100)\n}\n// Copying SOME_UDF and SOME_OTHER_UDF allows the query to be completely flattened.\nrun(q)\n// SELECT e.id, SOME_OTHER_UDF(SOME_UDF(e.id)) FROM Data e \n// WHERE SOME_UDF(e.id) <= 10 AND SOME_OTHER_UDF(SOME_UDF(e.id)) <= 100\n```\n\n### Dynamic infix\n\nInfix supports runtime string values through the `#$` prefix. Example:\n\n```scala\ndef test(functionName: String) =\n  ctx.run(query[Person].map(p => infix"#$functionName(${p.name})".as[Int]))\n```\n\n### Raw SQL queries\n\nYou can also use infix to port raw SQL queries to Quill and map it to regular Scala tuples.\n\n```scala\nval rawQuery = quote {\n  (id: Int) => infix"""SELECT id, name FROM my_entity WHERE id = $id""".as[Query[(Int, String)]]\n}\nctx.run(rawQuery(1))\n//SELECT x._1, x._2 FROM (SELECT id AS "_1", name AS "_2" FROM my_entity WHERE id = 1) x\n```\n\nNote that in this case the result query is nested.\nIt\'s required since Quill is not aware of a query tree and cannot safely unnest it.\nThis is different from the example above because infix starts with the query `infix"$q...` where its tree is already compiled\n\n### Database functions\n\nA custom database function can also be used through infix:\n\n```scala\nval myFunction = quote {\n  (i: Int) => infix"MY_FUNCTION($i)".as[Int]\n}\n\nval q = quote {\n  query[Person].map(p => myFunction(p.age))\n}\n\nctx.run(q)\n// SELECT MY_FUNCTION(p.age) FROM Person p\n```\n\n### Comparison operators\n\nYou can implement comparison operators by defining implicit conversion and using infix.\n\n```scala\nimport java.util.Date\n\nimplicit class DateQuotes(left: Date) {\n  def >(right: Date) = quote(infix"$left > $right".as[Boolean])\n\n  def <(right: Date) = quote(infix"$left < $right".as[Boolean])\n}\n```\n\n### batch with infix\n\n```scala\nimplicit class OnDuplicateKeyIgnore[T](q: Insert[T]) {\n  def ignoreDuplicate = quote(infix"$q ON DUPLICATE KEY UPDATE id=id".as[Insert[T]])\n}\n\nctx.run(\n  liftQuery(List(\n    Person(1, "Test1", 30),\n    Person(2, "Test2", 31)\n  )).foreach(row => query[Person].insert(row).ignoreDuplicate)\n)\n```\n\n## Custom encoding\n\nQuill uses `Encoder`s to encode query inputs and `Decoder`s to read values returned by queries. The library provides a few built-in encodings and two mechanisms to define custom encodings: mapped encoding and raw encoding.\n\n### Mapped Encoding\n\nIf the correspondent database type is already supported, use `MappedEncoding`. In this example, `String` is already supported by Quill and the `UUID` encoding from/to `String` is defined through mapped encoding:\n\n```scala\nimport ctx._\nimport java.util.UUID\n\nimplicit val encodeUUID = MappedEncoding[UUID, String](_.toString)\nimplicit val decodeUUID = MappedEncoding[String, UUID](UUID.fromString(_))\n```\n\nA mapped encoding also can be defined without a context instance by importing `io.getquill.MappedEncoding`:\n\n```scala\nimport io.getquill.MappedEncoding\nimport java.util.UUID\n\nimplicit val encodeUUID = MappedEncoding[UUID, String](_.toString)\nimplicit val decodeUUID = MappedEncoding[String, UUID](UUID.fromString(_))\n```\nNote that can it be also used to provide mapping for element types of collection (SQL Arrays or Cassandra Collections).\n\n### Raw Encoding\n\nIf the database type is not supported by Quill, it is possible to provide "raw" encoders and decoders:\n\n```scala\ntrait UUIDEncodingExample {\n  val jdbcContext: PostgresJdbcContext[Literal] // your context should go here\n\n  import jdbcContext._\n\n  implicit val uuidDecoder: Decoder[UUID] =\n    decoder((index, row) =>\n      UUID.fromString(row.getObject(index).toString)) // database-specific implementation\n    \n  implicit val uuidEncoder: Encoder[UUID] =\n    encoder(java.sql.Types.OTHER, (index, value, row) =>\n        row.setObject(index, value, java.sql.Types.OTHER)) // database-specific implementation\n\n  // Only for postgres\n  implicit def arrayUUIDEncoder[Col <: Seq[UUID]]: Encoder[Col] = arrayRawEncoder[UUID, Col]("uuid")\n  implicit def arrayUUIDDecoder[Col <: Seq[UUID]](implicit bf: CBF[UUID, Col]): Decoder[Col] =\n    arrayRawDecoder[UUID, Col]\n}\n```\n\n## `AnyVal`\n\nQuill automatically encodes `AnyVal`s (value classes):\n\n```scala\ncase class UserId(value: Int) extends AnyVal\ncase class User(id: UserId, name: String)\n\nval q = quote {\n  for {\n    u <- query[User] if u.id == lift(UserId(1))\n  } yield u\n}\n\nctx.run(q)\n// SELECT u.id, u.name FROM User u WHERE (u.id = 1)\n```\n\n## Meta DSL\n\nThe meta DSL allows the user to customize how Quill handles the expansion and execution of quotations through implicit meta instances.\n\n### Schema meta\n\nBy default, quill expands `query[Person]` to `querySchema[Person]("Person")`. It\'s possible to customize this behavior using an implicit instance of `SchemaMeta`:\n\n```scala\ndef example = {\n  implicit val personSchemaMeta = schemaMeta[Person]("people", _.id -> "person_id")\n\n  ctx.run(query[Person])\n  // SELECT x.person_id, x.name, x.age FROM people x\n}\n```\n\n### Insert meta\n\n`InsertMeta` customizes the expansion of case classes for insert actions (`query[Person].insert(p)`). By default, all columns are expanded and through an implicit `InsertMeta`, it\'s possible to exclude columns from the expansion:\n\n```scala\nimplicit val personInsertMeta = insertMeta[Person](_.id)\n\nctx.run(query[Person].insert(lift(Person(-1, "John", 22))))\n// INSERT INTO Person (name,age) VALUES (?, ?)\n```\n\nNote that the parameter of `insertMeta` is called `exclude`, but it isn\'t possible to use named parameters for macro invocations.\n\n### Update meta\n\n`UpdateMeta` customizes the expansion of case classes for update actions (`query[Person].update(p)`). By default, all columns are expanded, and through an implicit `UpdateMeta`, it\'s possible to exclude columns from the expansion:\n\n```scala\nimplicit val personUpdateMeta = updateMeta[Person](_.id)\n\nctx.run(query[Person].filter(_.id == 1).update(lift(Person(1, "John", 22))))\n// UPDATE Person SET name = ?, age = ? WHERE id = 1\n```\n\nNote that the parameter of `updateMeta` is called `exclude`, but it isn\'t possible to use named parameters for macro invocations.\n\n### Query meta\n\nThis kind of meta instance customizes the expansion of query types and extraction of the final value. For instance, it\'s possible to use this feature to normalize values before reading them from the database:\n\n```scala\nimplicit val personQueryMeta = \n  queryMeta(\n    (q: Query[Person]) =>\n      q.map(p => (p.id, infix"CONVERT(${p.name} USING utf8)".as[String], p.age))\n  ) {\n    case (id, name, age) =>\n      Person(id, name, age)\n  }\n```\n\nThe query meta definition is open and allows the user to even join values from other tables before reading the final value. This kind of usage is not encouraged.\n\n# Contexts\n\nContexts represent the database and provide an execution interface for queries.\n\n## Mirror context\n\nQuill provides a mirror context for testing purposes. Instead of running the query, the mirror context returns a structure with the information that would be used to run the query. There are three mirror context instances:\n\n- `io.getquill.MirrorContext`: Mirrors the quotation AST\n- `io.getquill.SqlMirrorContext`: Mirrors the SQL query\n- `io.getquill.CassandraMirrorContext`: Mirrors the CQL query\n\n## Dependent contexts\n\nThe context instance provides all methods and types to interact with quotations and the database. Depending on how the context import happens, Scala won\'t be able to infer that the types are compatible.\n\nFor instance, this example **will not** compile:\n\n```\nclass MyContext extends SqlMirrorContext(MirrorSqlDialect, Literal)\n\ncase class MySchema(c: MyContext) {\n\n  import c._\n  val people = quote {\n    querySchema[Person]("people")\n  }\n}\n\ncase class MyDao(c: MyContext, schema: MySchema) {\n\n  def allPeople = \n    c.run(schema.people)\n// ERROR: [T](quoted: MyDao.this.c.Quoted[MyDao.this.c.Query[T]])MyDao.this.c.QueryResult[T]\n cannot be applied to (MyDao.this.schema.c.Quoted[MyDao.this.schema.c.EntityQuery[Person]]{def quoted: io.getquill.ast.ConfiguredEntity; def ast: io.getquill.ast.ConfiguredEntity; def id1854281249(): Unit; val bindings: Object})\n}\n```\n\n### Context Traits\n\nOne way to compose applications with this kind of context is to use traits with an abstract context variable:\n\n```scala\nclass MyContext extends SqlMirrorContext(MirrorSqlDialect, Literal)\n\ntrait MySchema {\n\n  val c: MyContext\n  import c._\n\n  val people = quote {\n    querySchema[Person]("people")\n  }\n}\n\ncase class MyDao(c: MyContext) extends MySchema {\n  import c._\n\n  def allPeople = \n    c.run(people)\n}\n```\n\n### Modular Contexts\n\nAnother simple way to modularize Quill code is by extending `Context` as a self-type and applying mixins. Using this strategy,\nit is possible to create functionality that is fully portable across databases and even different types of databases\n(e.g. creating common queries for both Postgres and Spark).\n\nFor example, create the following abstract context:\n\n```scala\ntrait ModularContext[I <: Idiom, N <: NamingStrategy] { this: Context[I, N] =>\n  def peopleOlderThan = quote {\n    (age:Int, q:Query[Person]) => q.filter(p => p.age > age)\n  }\n}\n```\n \nLet\'s see how this can be used across different kinds of databases and Quill contexts.\n \n#### Use `ModularContext` in a mirror context:\n\n```scala\n// Note: In some cases need to explicitly specify [MirrorSqlDialect, Literal].\nval ctx = \n  new SqlMirrorContext[MirrorSqlDialect, Literal](MirrorSqlDialect, Literal) \n    with ModularContext[MirrorSqlDialect, Literal]\n  \nimport ctx._ \nprintln( run(peopleOlderThan(22, query[Person])).string )\n```\n\n#### Use `ModularContext` to query a Postgres Database\n\n```scala\nval ctx = \n  new PostgresJdbcContext[Literal](Literal, ds) \n    with ModularContext[PostgresDialect, Literal]\n  \nimport ctx._ \nval results = run(peopleOlderThan(22, query[Person]))\n```\n\n#### Use `ModularContext` to query a Spark Dataset\n\n```scala\nobject CustomQuillSparkContext extends QuillSparkContext \n  with ModularContext[SparkDialect, Literal]\n \nval results = run(peopleOlderThan(22, liftQuery(dataset)))\n```\n\n\n## Spark Context\n\nQuill provides a fully type-safe way to use Spark\'s highly-optimized SQL engine. It\'s an alternative to `Dataset`\'s weakly-typed API.\n\n### sbt dependency\n```\nlibraryDependencies ++= Seq(\n  "io.getquill" %% "quill-spark" % "3.4.11-SNAPSHOT"\n)\n```\n\n### Usage\n\nUnlike the other modules, the Spark context is a companion object. Also, it does not depend on a spark session. To use it, add the following import:\n\n```scala\nimport io.getquill.QuillSparkContext._\n```\n\nThe spark session must be provided by the user through an **implicit** value:\n\n```scala\nimport org.apache.spark.sql.SparkSession\n\n// Replace by your spark SQL context creation\nimplicit lazy val sqlContext =\n  SparkSession.builder().master("local").appName("spark test").getOrCreate().sqlContext\n```\n\nQuill decoders and meta instances are not used by the quill-spark module, Spark\'s `Encoder`s are used instead. Add this import to have them in scope:\n\n```scala\nimport sqlContext.implicits._\n```\n\nThe `liftQuery` method converts `Dataset`s to Quill queries:\n\n```scala\nimport org.apache.spark.sql.Dataset\n\ndef filter(myDataset: Dataset[Person], name: String): Dataset[Int] =\n  run {\n    liftQuery(myDataset).filter(_.name == lift(name)).map(_.age)\n  }\n// SELECT x1.age _1 FROM (?) x1 WHERE x1.name = ?\n```\n\nNote that the `run` method returns a `Dataset` transformed by the Quill query using the SQL engine.\n\nAdditionally, note that the queries printed from `run(myQuery)` during compile time escape question marks via a backslash them in order to\nbe able to substitute liftings properly. They are then returned back to their original form before running.\n```scala\nimport org.apache.spark.sql.Dataset\n\ndef filter(myDataset: Dataset[Person]): Dataset[Int] =\n  run {\n    liftQuery(myDataset).filter(_.name == "?").map(_.age)\n  }\n// This is generated during compile time:\n// SELECT x1.age _1 FROM (?) x1 WHERE x1.name = \'\\?\'\n// It is reverted upon run-time:\n// SELECT x1.age _1 FROM (ds1) x1 WHERE x1.name = \'?\'\n```\n\n\n**Important**: Spark doesn\'t support transformations of inner classes. Use top-level classes.\n\n## SQL Contexts\n\nExample:\n\n```scala\nlazy val ctx = new MysqlJdbcContext(SnakeCase, "ctx")\n```\n\n### Dialect\n\nThe SQL dialect parameter defines the specific database dialect to be used. Some context types are specific to a database and thus not require it.\n\nQuill has five built-in dialects:\n\n- `io.getquill.H2Dialect`\n- `io.getquill.MySQLDialect`\n- `io.getquill.PostgresDialect`\n- `io.getquill.SqliteDialect`\n- `io.getquill.SQLServerDialect`\n- `io.getquill.OracleDialect`\n\n### Naming strategy\n\nThe naming strategy parameter defines the behavior when translating identifiers (table and column names) to SQL.\n\n|           strategy                  |          example              |\n|-------------------------------------|-------------------------------|\n| `io.getquill.naming.Literal`        | some_ident  -> some_ident     |\n| `io.getquill.naming.Escape`         | some_ident  -> "some_ident"   |\n| `io.getquill.naming.UpperCase`      | some_ident  -> SOME_IDENT     |\n| `io.getquill.naming.LowerCase`      | SOME_IDENT  -> some_ident     |\n| `io.getquill.naming.SnakeCase`      | someIdent   -> some_ident     |\n| `io.getquill.naming.CamelCase`      | some_ident  -> someIdent      |\n| `io.getquill.naming.MysqlEscape`    | some_ident  -> \\`some_ident\\` |\n| `io.getquill.naming.PostgresEscape` | $some_ident -> $some_ident    |\n\nMultiple transformations can be defined using `NamingStrategy()`. For instance, the naming strategy\n\n```NamingStrategy(SnakeCase, UpperCase)```\n\nproduces the following transformation:\n\n```someIdent -> SOME_IDENT```\n\nThe transformations are applied from left to right.\n\n### Configuration\n\nThe string passed to the context is used as the key in order to obtain configurations using the [typesafe config](http://github.com/typesafehub/config) library.\n\nAdditionally, the contexts provide multiple constructors. For instance, with `JdbcContext` it\'s possible to specify a `DataSource` directly, without using the configuration:\n\n```scala\ndef createDataSource: javax.sql.DataSource with java.io.Closeable = ???\n\nlazy val ctx = new MysqlJdbcContext(SnakeCase, createDataSource)\n```\n\n## quill-jdbc\n\nThe `quill-jdbc` module provides a simple blocking JDBC context for standard use-cases. For transactions, the JDBC connection is kept in a thread-local variable.\n\nQuill uses [HikariCP](https://github.com/brettwooldridge/HikariCP) for connection pooling. Please refer to HikariCP\'s [documentation](https://github.com/brettwooldridge/HikariCP#configuration-knobs-baby) for a detailed explanation of the available configurations.\n\nNote that there are `dataSource` configurations, that go under `dataSource`, like `user` and `password`, but some pool settings may go under the root config, like `connectionTimeout`.\n\n#### transactions\n\nThe `JdbcContext` provides thread-local transaction support:\n\n```\nctx.transaction {\n  ctx.run(query[Person].delete)\n  // other transactional code\n}\n```\n\nThe body of `transaction` can contain calls to other methods and multiple `run` calls since the transaction is propagated through a thread-local.\n\n### MySQL (quill-jdbc)\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "mysql" % "mysql-connector-java" % "8.0.17",\n  "io.getquill" %% "quill-jdbc" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new MysqlJdbcContext(SnakeCase, "ctx")\n```\n\n#### application.properties\n```\nctx.dataSourceClassName=com.mysql.cj.jdbc.MysqlDataSource\nctx.dataSource.url=jdbc:mysql://host/database\nctx.dataSource.user=root\nctx.dataSource.password=root\nctx.dataSource.cachePrepStmts=true\nctx.dataSource.prepStmtCacheSize=250\nctx.dataSource.prepStmtCacheSqlLimit=2048\nctx.connectionTimeout=30000\n```\n\n### Postgres (quill-jdbc)\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "org.postgresql" % "postgresql" % "42.2.8",\n  "io.getquill" %% "quill-jdbc" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new PostgresJdbcContext(SnakeCase, "ctx")\n```\n\n#### application.properties\n```\nctx.dataSourceClassName=org.postgresql.ds.PGSimpleDataSource\nctx.dataSource.user=root\nctx.dataSource.password=root\nctx.dataSource.databaseName=database\nctx.dataSource.portNumber=5432\nctx.dataSource.serverName=host\nctx.connectionTimeout=30000\n```\n\n### Sqlite (quill-jdbc)\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "org.xerial" % "sqlite-jdbc" % "3.28.0",\n  "io.getquill" %% "quill-jdbc" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new SqliteJdbcContext(SnakeCase, "ctx")\n```\n\n#### application.properties\n```\nctx.driverClassName=org.sqlite.JDBC\nctx.jdbcUrl=jdbc:sqlite:/path/to/db/file.db\n```\n\n### H2 (quill-jdbc)\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "com.h2database" % "h2" % "1.4.199",\n  "io.getquill" %% "quill-jdbc" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new H2JdbcContext(SnakeCase, "ctx")\n```\n\n#### application.properties\n```\nctx.dataSourceClassName=org.h2.jdbcx.JdbcDataSource\nctx.dataSource.url=jdbc:h2:mem:yourdbname\nctx.dataSource.user=sa\n```\n\n### SQL Server (quill-jdbc)\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "com.microsoft.sqlserver" % "mssql-jdbc" % "7.4.1.jre8",\n  "io.getquill" %% "quill-jdbc" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new SqlServerJdbcContext(SnakeCase, "ctx")\n```\n\n### Oracle (quill-jdbc)\n\nQuill supports Oracle version 12c and up although due to licensing restrictions, version 18c XE is used for testing.\n\nNote that the latest Oracle JDBC drivers are not publicly available. In order to get them,\nyou will need to connect to Oracle\'s private maven repository as instructed [here](https://docs.oracle.com/middleware/1213/core/MAVEN/config_maven_repo.htm#MAVEN9012).\nUnfortunately, this procedure currently does not work for SBT. There are various workarounds\navailable for this situation [here](https://stackoverflow.com/questions/1074869/find-oracle-jdbc-driver-in-maven-repository?rq=1).\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "com.oracle.jdbc" % "ojdbc8" % "18.3.0.0.0",\n  "io.getquill" %% "quill-jdbc" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new OracleJdbcContext(SnakeCase, "ctx")\n```\n\n\n#### application.properties\n```\nctx.dataSourceClassName=com.microsoft.sqlserver.jdbc.SQLServerDataSource\nctx.dataSource.user=user\nctx.dataSource.password=YourStrongPassword\nctx.dataSource.databaseName=database\nctx.dataSource.portNumber=1433\nctx.dataSource.serverName=host\n```\n\n## quill-jdbc-monix\n\nThe `quill-jdbc-monix` module integrates the Monix asynchronous programming framework with Quill,\nsupporting all of the database vendors of the `quill-jdbc` module. \nThe Quill Monix contexts encapsulate JDBC Queries and Actions into Monix `Task`s \nand also include support for streaming queries via `Observable`.\n\n#### streaming\n\nThe `MonixJdbcContext` can stream using Monix Observables:\n\n```\nctx.stream(query[Person]) // returns: Observable[Person]\n  .foreachL(println(_))\n  .runSyncUnsafe()\n```\n\n#### transactions\n\nThe `MonixJdbcContext` provides support for transactions by storing the connection into a Monix `Local`. \nThis process is designed to be completely transparent to the user. As with the other contexts,\nif an exception is thrown anywhere inside a task or sub-task within a `transaction` block, the entire block\nwill be rolled back by the database.\n\nBasic syntax:\n```\nval trans =\n  ctx.transaction {\n    for {\n      _ <- ctx.run(query[Person].delete)\n      _ <- ctx.run(query[Person].insert(Person("Joe", 123)))\n      p <- ctx.run(query[Person])\n    } yield p\n  } //returns: Task[List[Person]]\n\nval result = trans.runSyncUnsafe() //returns: List[Person]\n```\n\nStreaming can also be done inside of `transaction` block so long as the result is converted to a task beforehand.\n```\nval trans =\n  ctx.transaction {\n    for {\n      _   <- ctx.run(query[Person].insert(Person("Joe", 123)))\n      ppl <- ctx\n              .stream(query[Person])                               // Observable[Person]\n              .foldLeftL(List[Person]())({case (l, p) => p +: l})  // ... becomes Task[List[Person]]\n    } yield ppl\n  } //returns: Task[List[Person]]\n\nval result = trans.runSyncUnsafe() //returns: List[Person]\n```\n\n#### runners\n\nUse a `Runner` object to create the different `MonixJdbcContext`s. \nThe Runner does the actual wrapping of JDBC calls into Monix Tasks.\n\n```scala\n\nimport monix.execution.Scheduler\nimport io.getquill.context.monix.Runner\n\n// You can use the default Runner when constructing a Monix jdbc contexts. \n// The resulting tasks will be wrapped with whatever Scheduler is \n// defined when you do task.syncRunUnsafe(), typically a global implicit.\nlazy val ctx = new MysqlMonixJdbcContext(SnakeCase, "ctx", Runner.default)\n\n// However...\n// Monix strongly suggests that you use a separate thread pool for database IO \n// operations. `Runner` provides a convenience method in order to do this.\nlazy val ctx = new MysqlMonixJdbcContext(SnakeCase, "ctx", Runner.using(Scheduler.io()))\n```\n\n### MySQL (quill-jdbc-monix)\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "mysql" % "mysql-connector-java" % "8.0.17",\n  "io.getquill" %% "quill-jdbc-monix" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new MysqlMonixJdbcContext(SnakeCase, "ctx", Runner.default)\n```\n\n#### application.properties\n```\nctx.dataSourceClassName=com.mysql.cj.jdbc.MysqlDataSource\nctx.dataSource.url=jdbc:mysql://host/database\nctx.dataSource.user=root\nctx.dataSource.password=root\nctx.dataSource.cachePrepStmts=true\nctx.dataSource.prepStmtCacheSize=250\nctx.dataSource.prepStmtCacheSqlLimit=2048\nctx.connectionTimeout=30000\n```\n\n### Postgres (quill-jdbc-monix)\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "org.postgresql" % "postgresql" % "42.2.8",\n  "io.getquill" %% "quill-jdbc-monix" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new PostgresMonixJdbcContext(SnakeCase, "ctx", Runner.default)\n```\n\n#### application.properties\n```\nctx.dataSourceClassName=org.postgresql.ds.PGSimpleDataSource\nctx.dataSource.user=root\nctx.dataSource.password=root\nctx.dataSource.databaseName=database\nctx.dataSource.portNumber=5432\nctx.dataSource.serverName=host\nctx.connectionTimeout=30000\n```\n\n### Sqlite (quill-jdbc-monix)\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "org.xerial" % "sqlite-jdbc" % "3.28.0",\n  "io.getquill" %% "quill-jdbc-monix" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new SqliteMonixJdbcContext(SnakeCase, "ctx", Runner.default)\n```\n\n#### application.properties\n```\nctx.driverClassName=org.sqlite.JDBC\nctx.jdbcUrl=jdbc:sqlite:/path/to/db/file.db\n```\n\n### H2 (quill-jdbc-monix)\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "com.h2database" % "h2" % "1.4.199",\n  "io.getquill" %% "quill-jdbc-monix" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new H2MonixJdbcContext(SnakeCase, "ctx", Runner.default)\n```\n\n#### application.properties\n```\nctx.dataSourceClassName=org.h2.jdbcx.JdbcDataSource\nctx.dataSource.url=jdbc:h2:mem:yourdbname\nctx.dataSource.user=sa\n```\n\n### SQL Server (quill-jdbc-monix)\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "com.microsoft.sqlserver" % "mssql-jdbc" % "7.4.1.jre8",\n  "io.getquill" %% "quill-jdbc-monix" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new SqlServerMonixJdbcContext(SnakeCase, "ctx", Runner.default)\n```\n\n#### application.properties\n```\nctx.dataSourceClassName=com.microsoft.sqlserver.jdbc.SQLServerDataSource\nctx.dataSource.user=user\nctx.dataSource.password=YourStrongPassword\nctx.dataSource.databaseName=database\nctx.dataSource.portNumber=1433\nctx.dataSource.serverName=host\n```\n\n### Oracle (quill-jdbc-monix)\n\nQuill supports Oracle version 12c and up although due to licensing restrictions, version 18c XE is used for testing.\n\nNote that the latest Oracle JDBC drivers are not publicly available. In order to get them,\nyou will need to connect to Oracle\'s private maven repository as instructed [here](https://docs.oracle.com/middleware/1213/core/MAVEN/config_maven_repo.htm#MAVEN9012).\nUnfortunately, this procedure currently does not work for SBT. There are various workarounds\navailable for this situation [here](https://stackoverflow.com/questions/1074869/find-oracle-jdbc-driver-in-maven-repository?rq=1).\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "com.oracle.jdbc" % "ojdbc8" % "18.3.0.0.0",\n  "io.getquill" %% "quill-jdbc-monix" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new OracleJdbcContext(SnakeCase, "ctx")\n```\n\n#### application.properties\n```\nctx.dataSourceClassName=oracle.jdbc.xa.client.OracleXADataSource\nctx.dataSource.databaseName=xe\nctx.dataSource.user=database\nctx.dataSource.password=YourStrongPassword\nctx.dataSource.driverType=thin\nctx.dataSource.portNumber=1521\nctx.dataSource.serverName=host\n```\n\n## NDBC Context\n\nAsync support via [NDBC driver](https://ndbc.io/) is available with Postgres database.\n\n### quill-ndbc-postgres\n\n#### transactions\n\nTransaction support is provided out of the box by NDBC:\n\n```scala\nctx.transaction {\n  ctx.run(query[Person].delete)\n  // other transactional code\n}\n```\n\nThe body of transaction can contain calls to other methods and multiple run calls since the transaction is automatically handled.\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "io.getquill" %% "quill-ndbc-postgres" % "3.4.2-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new NdbcPostgresContext(Literal, "ctx")\n```\n\n#### application.properties\n```\nctx.ndbc.dataSourceSupplierClass=io.trane.ndbc.postgres.netty4.DataSourceSupplier\nctx.ndbc.host=host\nctx.ndbc.port=1234\nctx.ndbc.user=root\nctx.ndbc.password=root\nctx.ndbc.database=database\n```\n\n## quill-async\n\nThe `quill-async` module provides simple async support for MySQL and Postgres databases.\n\n#### transactions\n\nThe async module provides transaction support based on a custom implicit execution context:\n\n```\nctx.transaction { implicit ec =>\n  ctx.run(query[Person].delete)\n  // other transactional code\n}\n```\n\nThe body of `transaction` can contain calls to other methods and multiple `run` calls, but the transactional code must be done using the provided implicit execution context. For instance:\n\n```\ndef deletePerson(name: String)(implicit ec: ExecutionContext) = \n  ctx.run(query[Person].filter(_.name == lift(name)).delete)\n\nctx.transaction { implicit ec =>\n  deletePerson("John")\n}\n```\n\nDepending on how the main execution context is imported, it is possible to produce an ambiguous implicit resolution. A way to solve this problem is shadowing the multiple implicits by using the same name:\n\n```\nimport scala.concurrent.ExecutionContext.Implicits.{ global => ec }\n\ndef deletePerson(name: String)(implicit ec: ExecutionContext) = \n  ctx.run(query[Person].filter(_.name == lift(name)).delete)\n\nctx.transaction { implicit ec =>\n  deletePerson("John")\n}\n```\n\nNote that the global execution context is renamed to ec.\n\n#### application.properties\n\n##### connection configuration\n```\nctx.host=host\nctx.port=1234\nctx.user=root\nctx.password=root\nctx.database=database\n```\n\nor use connection URL with database-specific scheme (see below):\n\n```\nctx.url=scheme://host:5432/database?user=root&password=root\n```\n\n##### connection pool configuration\n```\nctx.poolMaxQueueSize=4\nctx.poolMaxObjects=4\nctx.poolMaxIdle=999999999\nctx.poolValidationInterval=10000\n```\n\nAlso see [`PoolConfiguration` documentation](https://github.com/mauricio/postgresql-async/blob/master/db-async-common/src/main/scala/com/github/mauricio/async/db/pool/PoolConfiguration.scala).\n\n##### SSL configuration\n```\nctx.sslmode=disable # optional, one of [disable|prefer|require|verify-ca|verify-full]\nctx.sslrootcert=./path/to/cert/file # optional, required for sslmode=verify-ca or verify-full\n```\n\n##### other\n```\nctx.charset=UTF-8\nctx.maximumMessageSize=16777216\nctx.connectTimeout=5s\nctx.testTimeout=5s\nctx.queryTimeout=10m\n```\n\n### quill-async-mysql\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "io.getquill" %% "quill-async-mysql" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new MysqlAsyncContext(SnakeCase, "ctx")\n```\n\n#### application.properties\n\nSee [above](#applicationproperties-5)\n\nFor `url` property use `mysql` scheme:\n\n```\nctx.url=mysql://host:3306/database?user=root&password=root\n```\n\n### quill-async-postgres\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "io.getquill" %% "quill-async-postgres" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new PostgresAsyncContext(SnakeCase, "ctx")\n```\n\n#### application.properties\n\nSee [common properties](#applicationproperties-5)\n\nFor `url` property use `postgresql` scheme:\n\n```\nctx.url=postgresql://host:5432/database?user=root&password=root\n```\n\n## Finagle Contexts\n\nSupport for the Twitter Finagle library is available with MySQL and Postgres databases.\n\n### quill-finagle-mysql\n\n#### transactions\n\nThe finagle context provides transaction support through a `Local` value. See twitter util\'s [scaladoc](https://github.com/twitter/util/blob/ee8d3140ba0ecc16b54591bd9d8961c11b999c0d/util-core/src/main/scala/com/twitter/util/Local.scala#L96) for more details.\n\n```\nctx.transaction {\n  ctx.run(query[Person].delete)\n  // other transactional code\n}\n```\n\nThe body of `transaction` can contain calls to other methods and multiple `run` calls since the transaction is automatically propagated through the `Local` value.\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "io.getquill" %% "quill-finagle-mysql" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new FinagleMysqlContext(SnakeCase, "ctx")\n```\n\n#### application.properties\n```\nctx.dest=localhost:3306\nctx.user=root\nctx.password=root\nctx.database=database\nctx.pool.watermark.low=0\nctx.pool.watermark.high=10\nctx.pool.idleTime=5 # seconds\nctx.pool.bufferSize=0\nctx.pool.maxWaiters=2147483647\n```\n\n### quill-finagle-postgres\n\n#### transactions\n\nThe finagle context provides transaction support through a `Local` value. See twitter util\'s [scaladoc](https://github.com/twitter/util/blob/ee8d3140ba0ecc16b54591bd9d8961c11b999c0d/util-core/src/main/scala/com/twitter/util/Local.scala#L96) for more details.\n\n```\nctx.transaction {\n  ctx.run(query[Person].delete)\n  // other transactional code\n}\n```\n\nThe body of `transaction` can contain calls to other methods and multiple `run` calls since the transaction is automatically propagated through the `Local` value.\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "io.getquill" %% "quill-finagle-postgres" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### context definition\n```scala\nlazy val ctx = new FinaglePostgresContext(SnakeCase, "ctx")\n```\n\n#### application.properties\n```\nctx.host=localhost:3306\nctx.user=root\nctx.password=root\nctx.database=database\nctx.useSsl=false\nctx.hostConnectionLimit=1\nctx.numRetries=4\nctx.binaryResults=false\nctx.binaryParams=false\n```\n\n## quill-cassandra\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "io.getquill" %% "quill-cassandra" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### synchronous context\n```scala\nlazy val ctx = new CassandraSyncContext(SnakeCase, "ctx")\n```\n\n#### asynchronous context\n```scala\nlazy val ctx = new CassandraAsyncContext(SnakeCase, "ctx")\n```\n\nThe configurations are set using runtime reflection on the [`Cluster.builder`](https://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/Cluster.Builder.html) instance. It is possible to set nested structures like `queryOptions.consistencyLevel`, use enum values like `LOCAL_QUORUM`, and set multiple parameters like in `credentials`.\n\n#### application.properties\n```\nctx.keyspace=quill_test\nctx.preparedStatementCacheSize=1000\nctx.session.contactPoint=127.0.0.1\nctx.session.withPort=9042\nctx.session.queryOptions.consistencyLevel=LOCAL_QUORUM\nctx.session.withoutMetrics=true\nctx.session.withoutJMXReporting=false\nctx.session.credentials.0=root\nctx.session.credentials.1=pass\nctx.session.maxSchemaAgreementWaitSeconds=1\nctx.session.addressTranslator=com.datastax.driver.core.policies.IdentityTranslator\n```\n\n## quill-cassandra-monix\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "io.getquill" %% "quill-cassandra-monix" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### monix context\n```scala\nlazy val ctx = new CassandraMonixContext(SnakeCase, "ctx")\n```\n\n#### stream context\n```scala\nlazy val ctx = new CassandraStreamContext(SnakeCase, "ctx")\n```\n\n## OrientDB Contexts\n\n#### sbt dependencies\n```\nlibraryDependencies ++= Seq(\n  "io.getquill" %% "quill-orientdb" % "3.4.11-SNAPSHOT"\n)\n```\n\n#### synchronous context\n```scala\nlazy val ctx = new OrientDBSyncContext(SnakeCase, "ctx")\n```\n\nThe configurations are set using [`OPartitionedDatabasePool`](http://orientdb.com/javadoc/latest/com/orientechnologies/orient/core/db/OPartitionedDatabasePool.html) which creates a pool of DB connections from which an instance of connection can be acquired. It is possible to set DB credentials using the parameter called `username` and `password`.\n\n#### application.properties\n```\nctx.dbUrl=remote:127.0.0.1:2424/GratefulDeadConcerts\nctx.username=root\nctx.password=root\n```\n\n# Code Generation\n\nQuill now has a highly customizable code generator. Currently, it only supports JDBC but it will soon\nbe extended to other contexts. With a minimal amount of configuration, the code generator takes schemas like this:\n\n```sql\n-- Using schema \'public\'\n\ncreate table public.Person (\n  id int primary key auto_increment,\n  first_name varchar(255),\n  last_name varchar(255),\n  age int not null\n);\n\ncreate table public.Address (\n  person_fk int not null,\n  street varchar(255),\n  zip int\n);\n```\n\nProducing objects like this:\n\n```scala\n// src/main/scala/com/my/project/public/Person.scala\npackage com.my.project.public\n  \ncase class Person(id: Int, firstName: Option[String], lastName: Option[String], age: Int)\n```\n\n```scala\n// src/main/scala/com/my/project/public/Address.scala\npackage com.my.project.public\n  \ncase class Address(personFk: Int, street: Option[String], zip: Option[Int])\n```\n\nHave a look at the [CODEGEN.md](https://github.com/getquill/quill/blob/master/CODEGEN.md) manual page for more details.\n\n#### sbt dependencies\n\n```\nlibraryDependencies ++= Seq(\n  "io.getquill" %% "quill-codegen-jdbc" % "3.4.11-SNAPSHOT"\n)\n```\n\n\n# Logging\n\n## Compile-time\n\nTo disable logging of queries during compilation use `quill.macro.log` option:\n```\nsbt -Dquill.macro.log=false\n```\n## Runtime\n\nQuill uses SLF4J for logging. Each context logs queries which are currently executed.\nIt also logs the list of parameters that are bound into a prepared statement if any.\nTo enable that use `quill.binds.log` option:\n```\njava -Dquill.binds.log=true -jar myapp.jar\n```\n\n# Additional resources\n\n## Templates\n\nIn order to quickly start with Quill, we have setup some template projects:\n\n* [Play Framework with Quill JDBC](https://github.com/getquill/play-quill-jdbc)\n* [Play Framework with Quill async-postgres](https://github.com/jeffmath/play-quill-async-postgres-example)\n\n## Slick comparison\n\nPlease refer to [SLICK.md](https://github.com/getquill/quill/blob/master/SLICK.md) for a detailed comparison between Quill and Slick.\n\n## Cassandra libraries comparison\n\nPlease refer to [CASSANDRA.md](https://github.com/getquill/quill/blob/master/CASSANDRA.md) for a detailed comparison between Quill and other main alternatives for interaction with Cassandra in Scala.\n\n## Related Projects\n * [scala-db-codegen](https://github.com/olafurpg/scala-db-codegen) - Code/boilerplate generator from db schema\n * [quill-cache](https://github.com/mslinn/quill-cache/) - Caching layer for Quill\n * [quill-gen](https://github.com/mslinn/quill-gen/) - a DAO generator for `quill-cache`\n \n## External content\n\n### Talks\n\nScalaDays Berlin 2016 - [Scylla, Charybdis, and the mystery of Quill](https://www.youtube.com/watch?v=nqSYccoSeio)\n\n### Blog posts\n\n[quill-spark: A type-safe Scala API for Spark SQL](https://medium.com/@fwbrasil/quill-spark-a-type-safe-scala-api-for-spark-sql-2672e8582b0d)\nScalac.io blog - [Compile-time Queries with Quill](http://blog.scalac.io/2016/07/21/compile-time-queries-with-quill.html)\n\n## Code of Conduct\n\nPlease note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms. See [CODE_OF_CONDUCT.md](https://github.com/getquill/quill/blob/master/CODE_OF_CONDUCT.md) for details.\n\n## License\n\nSee the [LICENSE](https://github.com/getquill/quill/blob/master/LICENSE.txt) file for details.\n\n# Maintainers\n\n- @deusaquilus (lead maintainer)\n- @fwbrasil (creator)\n- @jilen\n- @juliano\n- @mentegy\n- @mdedetrich\n- @mxl\n\n## Former maintainers:\n\n- @gustavoamigo\n- @godenji\n- @lvicentesanchez\n\nYou can notify all current maintainers using the handle `@getquill/maintainers`.\n\n# Acknowledgments\n\nThe project was created having Philip Wadler\'s talk ["A practical theory of language-integrated query"](http://www.infoq.com/presentations/theory-language-integrated-query) as its initial inspiration. The development was heavily influenced by the following papers:\n\n* [A Practical Theory of Language-Integrated Query](http://homepages.inf.ed.ac.uk/slindley/papers/practical-theory-of-linq.pdf)\n* [Everything old is new again: Quoted Domain Specific Languages](http://homepages.inf.ed.ac.uk/wadler/papers/qdsl/qdsl.pdf)\n* [The Flatter, the Better](http://db.inf.uni-tuebingen.de/staticfiles/publications/the-flatter-the-better.pdf)'