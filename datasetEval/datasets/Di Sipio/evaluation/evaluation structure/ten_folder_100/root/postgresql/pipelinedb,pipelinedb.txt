b'PipelineDB [has joined Confluent](https://www.confluent.io/blog/pipelinedb-team-joins-confluent), read the blog post [here](https://www.pipelinedb.com/blog/pipelinedb-is-joining-confluent).\r\n\r\nPipelineDB will not have new releases beyond `1.0.0`, although critical bugs will still be fixed.\r\n\r\n# PipelineDB\r\n\r\n[![Gitter chat](https://img.shields.io/badge/gitter-join%20chat-brightgreen.svg?style=flat-square)](https://gitter.im/pipelinedb/pipelinedb)\r\n[![Twitter](https://img.shields.io/badge/twitter-@pipelinedb-55acee.svg?style=flat-square)](https://twitter.com/pipelinedb)\r\n\r\n## Overview\r\n\r\nPipelineDB is a PostgreSQL extension for high-performance time-series aggregation, designed to power realtime reporting and analytics applications.\r\n\r\nPipelineDB allows you to define [continuous SQL queries](http://docs.pipelinedb.com/continuous-views.html) that perpetually aggregate time-series data and store **only the aggregate output** in regular, queryable tables. You can think of this concept as extremely high-throughput, incrementally updated materialized views that never need to be manually refreshed.\r\n\r\nRaw time-series data is never written to disk, making PipelineDB extremely efficient for aggregation workloads.\r\n\r\nContinuous queries produce their own [output streams](http://docs.pipelinedb.com/streams.html#output-streams), and thus can be [chained together](http://docs.pipelinedb.com/continuous-transforms.html) into arbitrary networks of continuous SQL.\r\n\r\n## PostgreSQL compatibility\r\n\r\nPipelineDB runs on 64-bit architectures and currently supports the following PostgreSQL versions:\r\n\r\n* **PostgreSQL 10**: 10.1, 10.2, 10.3, 10.4, 10.5\r\n* **PostgreSQL 11**: 11.0\r\n\r\n## Getting started\r\n\r\nIf you just want to start using PipelineDB right away, head over to the [installation docs](http://docs.pipelinedb.com/installation.html) to get going.\r\n\r\nIf you\'d like to build PipelineDB from source, keep reading!\r\n\r\n## Building from source\r\n\r\nSince PipelineDB is a PostgreSQL extension, you\'ll need to have the [PostgreSQL development packages](https://www.postgresql.org/download/) installed to build PipelineDB.\r\n\r\nNext you\'ll have to install [ZeroMQ](http://zeromq.org/) which PipelineDB uses for inter-process communication. [Here\'s](https://gist.github.com/derekjn/14f95b7ceb8029cd95f5488fb04c500a) a gist with instructions to build and install ZeroMQ from source.\r\nYou\'ll also need to install some Python dependencies if you\'d like to run PipelineDB\'s Python test suite:\r\n\r\n```\r\npip install -r src/test/py/requirements.txt\r\n```\r\n\r\n#### Build PipelineDB:\r\n\r\nOnce PostgreSQL is installed, you can build PipelineDB against it:\r\n\r\n```\r\nmake USE_PGXS=1\r\nmake install\r\n```\r\n\r\n#### Test PipelineDB *(optional)*\r\nRun the following command:\r\n\r\n```\r\nmake test\r\n```\r\n\r\n#### Bootstrap the PipelineDB environment\r\nCreate PipelineDB\'s physical data directories, configuration files, etc:\r\n\r\n```\r\nmake bootstrap\r\n```\r\n\r\n**`make bootstrap` only needs to be run the first time you install PipelineDB**. The resources that `make bootstrap` creates may continue to be used as you change and rebuild PipeineDB.\r\n\r\n\r\n#### Run PipelineDB\r\nRun all of the daemons necessary for PipelineDB to operate:\r\n\r\n```\r\nmake run\r\n```\r\n\r\nEnter `Ctrl+C` to shut down PipelineDB.\r\n\r\n`make run` uses the binaries in the PipelineDB source root compiled by `make`, so you don\'t need to `make install` before running `make run` after code changes--only `make` needs to be run.\r\n\r\nThe basic development flow is:\r\n\r\n```\r\nmake\r\nmake run\r\n^C\r\n\r\n# Make some code changes...\r\nmake\r\nmake run\r\n```\r\n\r\n#### Send PipelineDB some data\r\n\r\nNow let\'s generate some test data and stream it into a simple continuous view. First, create the stream and the continuous view that reads from it:\r\n\r\n    $ psql\r\n    =# CREATE FOREIGN TABLE test_stream (key integer, value integer) SERVER pipelinedb;\r\n    CREATE FOREIGN TABLE\r\n    =# CREATE VIEW test_view WITH (action=materialize) AS SELECT key, COUNT(*) FROM test_stream GROUP BY key;\r\n    CREATE VIEW\r\n\r\nEvents can be emitted to PipelineDB streams using regular SQL `INSERTS`. Any `INSERT` target that isn\'t a table is considered a stream by PipelineDB, meaning streams don\'t need to have a schema created in advance. Let\'s emit a single event into the `test_stream` stream since our continuous view is reading from it:\r\n\r\n    $ psql\r\n    =# INSERT INTO test_stream (key, value) VALUES (0, 42);\r\n    INSERT 0 1\r\n\r\nThe 1 in the `INSERT 0 1` response means that 1 event was emitted into a stream that is actually being read by a continuous query. Now let\'s insert some random data:\r\n\r\n    =# INSERT INTO test_stream (key, value) SELECT random() * 10, random() * 10 FROM generate_series(1, 100000);\r\n    INSERT 0 100000\r\n\r\nQuery the continuous view to verify that the continuous view was properly updated. Were there actually 100,001 events counted?\r\n\r\n    $ psql -c "SELECT sum(count) FROM test_view"\r\n      sum\r\n    -------\r\n    100001\r\n    (1 row)\r\n\r\nWhat were the 10 most common randomly generated keys?\r\n\r\n    $ psql -c "SELECT * FROM test_view ORDER BY count DESC limit 10"\r\n\tkey  | count \r\n\t-----+-------\r\n\t 2   | 10124\r\n\t 8   | 10100\r\n\t 1   | 10042\r\n\t 7   |  9996\r\n\t 4   |  9991\r\n\t 5   |  9977\r\n\t 3   |  9963\r\n\t 6   |  9927\r\n\t 9   |  9915\r\n\t10   |  4997\r\n\t 0   |  4969\r\n\r\n\t(11 rows)\r\n'