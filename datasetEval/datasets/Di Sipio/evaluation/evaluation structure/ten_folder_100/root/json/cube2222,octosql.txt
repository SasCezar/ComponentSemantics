b'![OctoSQL](images/octosql.svg)OctoSQL\n=======\nOctoSQL is a query tool that allows you to join, analyse and transform data from multiple databases and file formats using SQL.\n\n[![CircleCI](https://circleci.com/gh/cube2222/octosql.svg?style=shield)](https://circleci.com/gh/cube2222/octosql)\n[![GoDoc](https://godoc.org/github.com/cube2222/octosql?status.svg)](https://godoc.org/github.com/cube2222/octosql)\n\n## Table of Contents\n- [What is OctoSQL?](#what-is-octosql)\n- [Installation](#installation)\n- [Quickstart](#quickstart)\n- [Configuration](#configuration)\n  - [JSON](#json)\n  - [CSV](#csv)\n  - [PostgreSQL](#postgresql)\n  - [MySQL](#mysql)\n  - [Redis](#redis)\n  - [Excel](#excel)\n- [Documentation](#documentation)\n- [Architecture](#architecture)\n- [Datasource Pushdown Operations](#datasource-pushdown-operations)\n- [Roadmap](#roadmap)\n\n## What is OctoSQL?\nOctoSQL is a SQL query engine which allows you to write standard SQL queries on data stored in multiple SQL databases, NoSQL databases and files in various formats trying to push down as much of the work as possible to the source databases, not transferring unnecessary data. \n\nOctoSQL does that by creating an internal representation of your query and later translating parts of it into the query languages or APIs of the source databases. Whenever a datasource doesn\'t support a given operation, OctoSQL will execute it in memory, so you don\'t have to worry about the specifics of the underlying datasources. \n\nWith OctoSQL you don\'t need O(n) client tools or a large data analysis system deployment. Everything\'s contained in a single binary.\n\n### Why the name?\nOctoSQL stems from Octopus SQL.\n\nOctopus, because octopi have many arms, so they can grasp and manipulate multiple objects, like OctoSQL is able to handle multiple datasources simultaneously.\n\n## Installation\nEither download the binary for your operating system (Linux, OS X and Windows are supported) from the [Releases page](https://github.com/cube2222/octosql/releases), or install using the go command line tool:\n```bash\ngo get -u github.com/cube2222/octosql/cmd/octosql\n```\n\n## Quickstart\nLet\'s say we have a csv file with cats, and a redis database with people (potential cat owners). Now we want to get a list of cities with the number of distinct cat names in them and the cumulative number of cat lives (as each cat has up to 9 lives left).\n\nFirst, create a configuration file ([Configuration Syntax](#configuration))\nFor example:\n```yaml\ndataSources:\n  - name: cats\n    type: csv\n    config:\n      path: "~/Documents/cats.csv"\n  - name: people\n    type: redis\n    config:\n      address: "localhost:6379"\n      password: ""\n      databaseIndex: 0\n      databaseKeyName: "id"\n```\n\nThen, set the **OCTOSQL_CONFIG** environment variable to point to the configuration file.\n```bash\nexport OCTOSQL_CONFIG=~/octosql.yaml\n```\nYou can also use the --config command line argument.\n\nFinally, query to your hearts desire:\n```bash\noctosql "SELECT p.city, FIRST(c.name), COUNT(DISTINCT c.name) cats, SUM(c.livesleft) catlives\nFROM cats c JOIN people p ON c.ownerid = p.id\nGROUP BY p.city\nORDER BY catlives DESC\nLIMIT 9"\n```\nExample output:\n```\n+---------+--------------+------+----------+\n| p.city  | c.name_first | cats | catlives |\n+---------+--------------+------+----------+\n| Warren  | Zoey         |   68 |      570 |\n| Gadsden | Snickers     |   52 |      388 |\n| Staples | Harley       |   54 |      383 |\n| Buxton  | Lucky        |   45 |      373 |\n| Bethany | Princess     |   46 |      366 |\n| Noxen   | Sheba        |   49 |      361 |\n| Yorklyn | Scooter      |   45 |      359 |\n| Tuttle  | Toby         |   57 |      356 |\n| Ada     | Jasmine      |   49 |      351 |\n+---------+--------------+------+----------+\n```\nYou can choose between table, tabbed, json and csv output formats.\n\n## Configuration\nThe configuration file has the following form\n```yaml\ndataSources:\n  - name: <table_name_in_octosql>\n    type: <datasource_type>\n    config:\n      <datasource_specific_key>: <datasource_specific_value>\n      <datasource_specific_key>: <datasource_specific_value>\n      ...\n  - name: <table_name_in_octosql>\n    type: <datasource_type>\n    config:\n      <datasource_specific_key>: <datasource_specific_value>\n      <datasource_specific_key>: <datasource_specific_value>\n      ...\n    ...\n```\n### Supported Datasources\n#### JSON\nJSON file in one of the following forms:\n- one record per line, no commas\n- JSON list of records\n##### options:\n- path - path to file containing the data, required\n- arrayFormat - if the JSON list of records format should be used, defaults to false\n\n---\n#### CSV\nCSV file separated using commas. The first row should contain column names.\n##### options:\n- path - path to file containing the data, required\n- headerRow - whether the first row of the CSV file contains column names or not, defaults to true\n- separator - columns separator, defaults to ","\n---\n#### Excel\nA single table in an Excel spreadsheet.\nThe table may or may not have column names as it\'s first row.\nThe table can be in any sheet, and start at any point, but it cannot\ncontain spaces between columns nor spaces between rows.\n##### options:\n- path - path to file, required\n- headerRow - does the first row contain column names, optional: defaults to true\n- sheet - name of the sheet in which data is stored, optional: defaults to "Sheet1"\n- rootCell - name of cell (i.e "A3", "BA14") which is the leftmost cell of the first\n- timeColumns - a list of columns to parse as datetime values with second precision\nrow, optional: defaults to "A1"\n---\n#### PostgreSQL\nSingle PostgreSQL database table.\n##### options:\n- address - address including port number, defaults to localhost:5432\n- user - required\n- password - required\n- databaseName - required\n- tableName - required\n---\n#### MySQL\nSingle MySQL database table.\n##### options:\n- address - address including port number, defaults to localhost:3306\n- user - required\n- password - required\n- databaseName - required\n- tableName - required\n---\n#### Redis\nRedis database with the given index. Currently only hashes are supported.\n##### options:\n- address - address including port number, defaults to localhost:6379\n- password - defaults to ""\n- databaseIndex - index number of Redis database, defaults to 0\n- databaseKeyName - column name of Redis key in OctoSQL records, defaults to "key"\n\n## Documentation\nDocumentation for the available functions: https://github.com/cube2222/octosql/wiki/Function-Documentation\n\nDocumentation for the available aggregates: https://github.com/cube2222/octosql/wiki/Aggregate-Documentation\n\nDocumentation for the available table valued functions: https://github.com/cube2222/octosql/wiki/Table-Valued-Functions-Documentation\n\nThe SQL dialect documentation: TODO ;) in short though:\n\nAvailable SQL constructs: Select, Where, Order By, Group By, Offset, Limit, Left Join, Right Join, Inner Join, Distinct, Union, Union All, Subqueries, Operators, Table Valued Functions.\n\nAvailable SQL types: Int, Float, String, Bool, Time, Duration, Tuple (array), Object (e.g. JSON)\n\n## Architecture\nAn OctoSQL invocation gets processed in multiple phases.\n\n### SQL AST\nFirst, the SQL query gets parsed into an abstract syntax tree. This phase only rules out syntax errors.\n\n### Logical Plan\nThe SQL AST gets converted into a logical query plan. This plan is still mostly a syntactic validation. It\'s the most naive possible translation of the SQL query. However, this plan already has more of a map-filter-reduce form.\n\nIf you wanted to add a new query language to OctoSQL, the only problem you\'d have to solve is translating it to this logical plan.\n\n### Physical Plan\nThe logical plan gets converted into a physical plan. This conversion finds any semantic errors in the query. If this phase is reached, then the input is correct and OctoSQL will be able execute it.\n\nThis phase already understands the specifics of the underlying datasources. So it\'s here where the optimizer will iteratively transform the plan, pushing computation nodes down to the datasources, and deduplicating unnecessary parts.\n\nThe optimizer uses a pattern matching approach, where it has rules for matching parts of the physical plan tree and how those patterns can be restructured into a more efficient version. The rules are meant to be as simple as possible and make the smallest possible changes. For example, pushing filters under maps, if they don\'t use any mapped variables. This way, the optimizer just keeps on iterating on the whole tree, until it can\'t change anything anymore. (each iteration tries to apply each rule in each possible place in the tree) This ensures that the plan reaches a local performance minimum, and the rules should be structured so that this local minimum is equal - or close to - the global minimum. (i.e. one optimization, shouldn\'t make another - much more useful one - impossible)\n\nHere is an example diagram of an optimized physical plan:\n![Physical Plan](images/physical.png)\n\n### Execution Plan\nThe physical plan gets materialized into an execution plan. This phase has to be able to connect to the actual datasources. It may initialize connections, open files, etc.\n\n### Stream\nStarting the execution plan creates a stream, which underneath may hold more streams, or parts of the execution plan to create streams in the future. This stream works in a pull based model.\n\n## Database Pushdown Operations\n|Datasource\t|Equality\t|In\t|> < <= =>\t|\n|---\t|---\t|---\t|---\t|\n|MySQL\t|supported\t|supported\t|supported\t|\n|PostgreSQL\t|supported\t|supported\t|supported\t|\n|Redis\t|supported\t|supported\t|scan\t|\n|JSON\t|scan\t|scan\t|scan\t|\n|CSV\t|scan\t|scan\t|scan\t|\n\nWhere scan means that the whole table needs to be scanned for each access. We are planning to add an in memory index in the future, which would allow us to store small tables in-memory, saving us a lot of unnecessary reads.\n\n## Roadmap\n- Additional Datasources.\n- SQL Constructs:\n  - JSON Query\n  - HAVING, ALL, ANY\n- Parallel expression evaluation.\n- Streams support (Kafka, Redis)\n- Push down functions, aggregates to databases that support them.\n- An in-memory index to save values of subqueries and save on rescanning tables which don\'t support a given operation, so as not to recalculate them each time.\n- MapReduce style distributed execution mode.\n- Runtime statistics\n- Server mode\n- Querying a json or csv table from standard input.\n- Integration test suite\n- Tuple splitter, returning the row for each tuple element, with the given element instead of the tuple.\n- Describe-like functionality as in the diagram above.\n'