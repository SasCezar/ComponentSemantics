b'Confluent\'s .NET Client for Apache Kafka<sup>TM</sup>\n=====================================================\n\n[![Travis Build Status](https://travis-ci.org/confluentinc/confluent-kafka-dotnet.svg?branch=master)](https://travis-ci.org/confluentinc/confluent-kafka-dotnet)\n[![Build status](https://ci.appveyor.com/api/projects/status/kux83eykufuv16cn/branch/master?svg=true)](https://ci.appveyor.com/project/ConfluentClientEngineering/confluent-kafka-dotnet/branch/master)\n[![Chat on Slack](https://img.shields.io/badge/chat-on%20slack-7A5979.svg)](https://confluentcommunity.slack.com/messages/clients)\n\n**confluent-kafka-dotnet** is Confluent\'s .NET client for [Apache Kafka](http://kafka.apache.org/) and the\n[Confluent Platform](https://www.confluent.io/product/).\n\nFeatures:\n\n- **High performance** - confluent-kafka-dotnet is a lightweight wrapper around\n[librdkafka](https://github.com/edenhill/librdkafka), a finely tuned C\nclient.\n\n- **Reliability** - There are a lot of details to get right when writing an Apache Kafka\nclient. We get them right in one place (librdkafka) and leverage this work\nacross all of our clients (also [confluent-kafka-python](https://github.com/confluentinc/confluent-kafka-python)\nand [confluent-kafka-go](https://github.com/confluentinc/confluent-kafka-go)).\n\n- **Supported** - Commercial support is offered by\n[Confluent](https://confluent.io/).\n\n- **Future proof** - Confluent, founded by the\ncreators of Kafka, is building a [streaming platform](https://www.confluent.io/product/)\nwith Apache Kafka at its core. It\'s high priority for us that client features keep\npace with core Apache Kafka and components of the [Confluent Platform](https://www.confluent.io/product/).\n\nconfluent-kafka-dotnet is derived from Andreas Heider\'s [rdkafka-dotnet](https://github.com/ah-/rdkafka-dotnet).\nWe\'re fans of his work and were very happy to have been able to leverage rdkafka-dotnet as the basis of this\nclient. Thanks Andreas!\n\n## Referencing\n\nconfluent-kafka-dotnet is distributed via NuGet. We provide three packages:\n\n- [Confluent.Kafka](https://www.nuget.org/packages/Confluent.Kafka/) *[net45, netstandard1.3, netstandard2.0]* - The core client library.\n- [Confluent.SchemaRegistry.Serdes](https://www.nuget.org/packages/Confluent.SchemaRegistry.Serdes/) *[net452, netstandard2.0]* - Provides a serializer and deserializer for working with Avro serialized data with Confluent Schema Registry integration.\n- [Confluent.SchemaRegistry](https://www.nuget.org/packages/Confluent.SchemaRegistry/) *[net452, netstandard1.4, netstandard2.0]* - Confluent Schema Registry client (a dependency of Confluent.SchemaRegistry.Serdes).\n\nTo install Confluent.Kafka from within Visual Studio, search for Confluent.Kafka in the NuGet Package Manager UI, or run the following command in the Package Manager Console:\n\n```\nInstall-Package Confluent.Kafka -Version 1.2.2\n```\n\nTo add a reference to a dotnet core project, execute the following at the command line:\n\n```\ndotnet add package -v 1.2.2 Confluent.Kafka\n```\n\nNote: `Confluent.Kafka` depends on the `librdkafka.redist` package which provides a number of different builds of `librdkafka` that are compatible with [common platforms](https://github.com/edenhill/librdkafka/wiki/librdkafka.redist-NuGet-package-runtime-libraries). If you are on one of these platforms this will all work seamlessly (and you don\'t need to explicitly reference `librdkafka.redist`). If you are on a different platform, you may need to [build librdkafka](https://github.com/edenhill/librdkafka#building) manually (or acquire it via other means) and load it using the [Library.Load](https://docs.confluent.io/current/clients/confluent-kafka-dotnet/api/Confluent.Kafka.Library.html#Confluent_Kafka_Library_Load_System_String_) method.\n\n### Branch builds\n\nNuget packages corresponding to all commits to release branches are available from the following nuget package source (Note: this is not a web URL - you \nshould specify it in the nuget package manger):\n[https://ci.appveyor.com/nuget/confluent-kafka-dotnet](https://ci.appveyor.com/nuget/confluent-kafka-dotnet). The version suffix of these nuget packages \nmatches the appveyor build number. You can see which commit a particular build number corresponds to by looking at the \n[AppVeyor build history](https://ci.appveyor.com/project/ConfluentClientEngineering/confluent-kafka-dotnet/history)\n\n\n## Usage\n\nTake a look in the [examples](examples) directory for example usage. The [integration tests](test/Confluent.Kafka.IntegrationTests/Tests) also serve as good examples.\n\nFor an overview of configuration properties, refer to the [librdkafka documentation](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md). \n\n### Basic Producer Examples\n\nYou should use the `ProduceAsync` method if you would like to wait for the result of your produce\nrequests before proceeding. You might typically want to do this in highly concurrent scenarios,\nfor example in the context of handling web requests. Behind the scenes, the client will manage \noptimizing communication with the Kafka brokers for you, batching requests as appropriate.\n\n```csharp\nusing System;\nusing System.Threading.Tasks;\nusing Confluent.Kafka;\n\nclass Program\n{\n    public static async Task Main(string[] args)\n    {\n        var config = new ProducerConfig { BootstrapServers = "localhost:9092" };\n\n        // If serializers are not specified, default serializers from\n        // `Confluent.Kafka.Serializers` will be automatically used where\n        // available. Note: by default strings are encoded as UTF8.\n        using (var p = new ProducerBuilder<Null, string>(config).Build())\n        {\n            try\n            {\n                var dr = await p.ProduceAsync("test-topic", new Message<Null, string> { Value="test" });\n                Console.WriteLine($"Delivered \'{dr.Value}\' to \'{dr.TopicPartitionOffset}\'");\n            }\n            catch (ProduceException<Null, string> e)\n            {\n                Console.WriteLine($"Delivery failed: {e.Error.Reason}");\n            }\n        }\n    }\n}\n```\n\nNote that a server round-trip is slow (3ms at a minimum; actual latency depends on many factors).\nIn highly concurrent scenarios you will achieve high overall throughput out of the producer using \nthe above approach, but there will be a delay on each `await` call. In stream processing \napplications, where you would like to process many messages in rapid succession, you would typically\nuse the `Produce` method instead:\n\n```csharp\nusing System;\nusing Confluent.Kafka;\n\nclass Program\n{\n    public static void Main(string[] args)\n    {\n        var conf = new ProducerConfig { BootstrapServers = "localhost:9092" };\n\n        Action<DeliveryReport<Null, string>> handler = r => \n            Console.WriteLine(!r.Error.IsError\n                ? $"Delivered message to {r.TopicPartitionOffset}"\n                : $"Delivery Error: {r.Error.Reason}");\n\n        using (var p = new ProducerBuilder<Null, string>(conf).Build())\n        {\n            for (int i=0; i<100; ++i)\n            {\n                p.Produce("my-topic", new Message<Null, string> { Value = i.ToString() }, handler);\n            }\n\n            // wait for up to 10 seconds for any inflight messages to be delivered.\n            p.Flush(TimeSpan.FromSeconds(10));\n        }\n    }\n}\n```\n\n### Basic Consumer Example\n\n```csharp\nusing System;\nusing System.Threading;\nusing Confluent.Kafka;\n\nclass Program\n{\n    public static void Main(string[] args)\n    {\n        var conf = new ConsumerConfig\n        { \n            GroupId = "test-consumer-group",\n            BootstrapServers = "localhost:9092",\n            // Note: The AutoOffsetReset property determines the start offset in the event\n            // there are not yet any committed offsets for the consumer group for the\n            // topic/partitions of interest. By default, offsets are committed\n            // automatically, so in this example, consumption will only start from the\n            // earliest message in the topic \'my-topic\' the first time you run the program.\n            AutoOffsetReset = AutoOffsetReset.Earliest\n        };\n\n        using (var c = new ConsumerBuilder<Ignore, string>(conf).Build())\n        {\n            c.Subscribe("my-topic");\n\n            CancellationTokenSource cts = new CancellationTokenSource();\n            Console.CancelKeyPress += (_, e) => {\n                e.Cancel = true; // prevent the process from terminating.\n                cts.Cancel();\n            };\n\n            try\n            {\n                while (true)\n                {\n                    try\n                    {\n                        var cr = c.Consume(cts.Token);\n                        Console.WriteLine($"Consumed message \'{cr.Value}\' at: \'{cr.TopicPartitionOffset}\'.");\n                    }\n                    catch (ConsumeException e)\n                    {\n                        Console.WriteLine($"Error occured: {e.Error.Reason}");\n                    }\n                }\n            }\n            catch (OperationCanceledException)\n            {\n                // Ensure the consumer leaves the group cleanly and final offsets are committed.\n                c.Close();\n            }\n        }\n    }\n}\n```\n\n### Working with Apache Avro\n\nThe `Confluent.SchemaRegistry.Serdes` nuget package provides an Avro serializer and deserializer that integrate with [Confluent\nSchema Registry](https://docs.confluent.io/current/schema-registry/docs/index.html). The `Confluent.SchemaRegistry` \nnuget package provides a client for interfacing with Schema Registry\'s REST API.\n\nYou can use the Avro serializer and deserializer with the `GenericRecord` class or with specific classes generated\nusing the `avrogen` tool, available via Nuget (.NET Core 2.1 required):\n\n```\ndotnet tool install -g Confluent.Apache.Avro.AvroGen\n```\n\nUsage:\n\n```\navrogen -s your_schema.asvc .\n```\n\nFor more information about working with Avro in .NET, refer to the the blog post [Decoupling Systems with Apache Kafka, Schema Registry and Avro](https://www.confluent.io/blog/decoupling-systems-with-apache-kafka-schema-registry-and-avro/)\n\n\n### Error Handling\n\nErrors delivered to a client\'s error handler should be considered informational except when the `IsFatal` flag\nis set to `true`, indicating that the client is in an un-recoverable state. Currently, this can only happen on\nthe producer, and only when `enable.idempotence` has been set to `true`. In all other scenarios, clients will\nattempt to recover from all errors automatically.\n\nAlthough calling most methods on the clients will result in a fatal error if the client is in an un-recoverable\nstate, you should generally only need to explicitly check for fatal errors in your error handler, and handle\nthis scenario there.\n\n#### Producer\n\nWhen using `Produce`, to determine whether a particular message has been successfully delivered to a cluster,\ncheck the `Error` field of the `DeliveryReport` during the delivery handler callback.\n\nWhen using `ProduceAsync`, any delivery result other than `NoError` will cause the returned `Task` to be in the\nfaulted state, with the `Task.Exception` field set to a `ProduceException` containing information about the message\nand error via the `DeliveryResult` and `Error` fields. Note: if you `await` the call, this means a `ProduceException`\nwill be thrown.\n\n#### Consumer\n\nAll `Consume` errors will result in a `ConsumeException` with further information about the error and context\navailable via the `Error` and `ConsumeResult` fields.\n\n\n### Confluent Cloud\n\nThe [Confluent Cloud example](examples/ConfluentCloud) demonstrates how to configure the .NET client for use with\n[Confluent Cloud](https://www.confluent.io/confluent-cloud/).\n\n\n## Build\n\nTo build the library or any test or example project, run the following from within the relevant project directory:\n\n```\ndotnet restore\ndotnet build\n```\n\nTo run an example project, run the following from within the example\'s project directory:\n\n```\ndotnet run <args>\n```\n\n## Tests\n\n### Unit Tests\n\nFrom within the test/Confluent.Kafka.UnitTests directory, run:\n\n```\ndotnet test\n```\n\n### Integration Tests\n\nFrom within the [Confluent Platform](https://www.confluent.io/product/compare/) (or Apache Kafka) distribution directory,\nrun the following two commands (in separate terminal windows) to set up a single broker test Kafka cluster:\n\n```\n./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties\n\n./bin/kafka-server-start ./etc/kafka/server.properties\n```\n\nNow use the `bootstrap-topics.sh` script in the test/Confleunt.Kafka.IntegrationTests directory to set up the\nprerequisite topics:\n\n```\n./bootstrap-topics.sh <confluent platform path> <zookeeper>\n```\n\nthen:\n\n```\ndotnet test\n```\n\nCopyright (c) \n2016-2019 [Confluent Inc.](https://www.confluent.io)\n2015-2016 [Andreas Heider](mailto:andreas@heider.io)\n'