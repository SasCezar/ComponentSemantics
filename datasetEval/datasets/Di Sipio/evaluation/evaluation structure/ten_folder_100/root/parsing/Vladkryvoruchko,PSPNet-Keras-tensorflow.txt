b'# Keras implementation of [PSPNet(caffe)](https://github.com/hszhao/PSPNet)\n\nImplemented Architecture of Pyramid Scene Parsing Network in Keras.\n\nFor the best compability please use Python3.5\n### Setup\n1. Install dependencies:\n    * Tensorflow (-gpu)\n    * Keras\n    * numpy\n    * scipy\n    * pycaffe(PSPNet)(optional for converting the weights) \n    ```bash\n    pip install -r requirements.txt --upgrade\n    ```\n2. Converted trained weights are needed to run the network.\nWeights(in ```.h5 .json``` format) have to be downloaded and placed into directory ``` weights/keras ```\n\n\nAlready converted weights can be downloaded here:\n\n * [pspnet50_ade20k.h5](https://www.dropbox.com/s/0uxn14y26jcui4v/pspnet50_ade20k.h5?dl=1)\n[pspnet50_ade20k.json](https://www.dropbox.com/s/v41lvku2lx7lh6m/pspnet50_ade20k.json?dl=1)\n * [pspnet101_cityscapes.h5](https://www.dropbox.com/s/c17g94n946tpalb/pspnet101_cityscapes.h5?dl=1)\n[pspnet101_cityscapes.json](https://www.dropbox.com/s/fswowe8e3o14tdm/pspnet101_cityscapes.json?dl=1)\n * [pspnet101_voc2012.h5](https://www.dropbox.com/s/uvqj2cjo4b9c5wg/pspnet101_voc2012.h5?dl=1)\n[pspnet101_voc2012.json](https://www.dropbox.com/s/rr5taqu19f5fuzy/pspnet101_voc2012.json?dl=1)\n\n## Convert weights by yourself(optional)\n(Note: this is **not** required if you use .h5/.json weights)\n\nRunning this needs the compiled original PSPNet caffe code and pycaffe.\n\n```bash\npython weight_converter.py <path to .prototxt> <path to .caffemodel>\n```\n\n## Usage:\n\n```bash\npython pspnet.py -m <model> -i <input_image>  -o <output_path>\npython pspnet.py -m pspnet101_cityscapes -i example_images/cityscapes.png -o example_results/cityscapes.jpg\npython pspnet.py -m pspnet101_voc2012 -i example_images/pascal_voc.jpg -o example_results/pascal_voc.jpg\n```\nList of arguments:\n```bash\n -m --model        - which model to use: \'pspnet50_ade20k\', \'pspnet101_cityscapes\', \'pspnet101_voc2012\'\n    --id           - (int) GPU Device id. Default 0\n -s --sliding      - Use sliding window\n -f --flip         - Additional prediction of flipped image\n -ms --multi_scale - Predict on multiscale images\n```\n## Keras results:\n![Original](example_images/ade20k.jpg)\n![New](example_results/ade20k_seg.jpg)\n![New](example_results/ade20k_seg_blended.jpg)\n![New](example_results/ade20k_probs.jpg)\n\n![Original](example_images/cityscapes.png)\n![New](example_results/cityscapes_seg.jpg)\n![New](example_results/cityscapes_seg_blended.jpg)\n![New](example_results/cityscapes_probs.jpg)\n\n![Original](example_images/pascal_voc.jpg)\n![New](example_results/pascal_voc_seg.jpg)\n![New](example_results/pascal_voc_seg_blended.jpg)\n![New](example_results/pascal_voc_probs.jpg)\n\n\n## Implementation details\n* The interpolation layer is implemented as custom layer "Interp"\n* Forward step takes about ~1 sec on single image\n* Memory usage can be optimized with:\n    ```python\n    config = tf.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = 0.3 \n    sess = tf.Session(config=config)\n    ```\n* ```ndimage.zoom``` can take a long time\n\n\n\n \n'