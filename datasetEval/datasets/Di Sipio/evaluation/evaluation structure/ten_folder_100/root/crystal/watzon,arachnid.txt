b'# Arachnid\n\nArachnid is a fast and powerful web scraping framework for Crystal. It provides an easy to use DSL for scraping webpages and processing all of the things you might come across.\n\n- [Arachnid](#Arachnid)\n  - [Installation](#Installation)\n  - [The CLI](#The-CLI)\n    - [Summarize](#Summarize)\n    - [Sitemap](#Sitemap)\n  - [Examples](#Examples)\n  - [Usage](#Usage)\n    - [Configuration](#Configuration)\n    - [Crawling](#Crawling)\n      - [`Arachnid#start_at(url, **options, &block : Agent ->)`](#Arachnidstartaturl-options-block--Agent)\n      - [`Arachnid#site(url, **options, &block : Agent ->)`](#Arachnidsiteurl-options-block--Agent)\n      - [`Arachnid#host(name, **options, &block : Agent ->)`](#Arachnidhostname-options-block--Agent)\n    - [Crawling Rules](#Crawling-Rules)\n    - [Events](#Events)\n      - [`every_url(&block : URI ->)`](#everyurlblock--URI)\n      - [`every_failed_url(&block : URI ->)`](#everyfailedurlblock--URI)\n      - [`every_url_like(pattern, &block : URI ->)`](#everyurllikepattern-block--URI)\n      - [`urls_like(pattern, &block : URI ->)`](#urlslikepattern-block--URI)\n      - [`all_headers(&block : HTTP::Headers)`](#allheadersblock--HTTPHeaders)\n      - [`every_resource(&block : Resource ->)`](#everyresourceblock--Resource)\n      - [`every_ok_page(&block : Resource ->)`](#everyokpageblock--Resource)\n      - [`every_redirect_page(&block : Resource ->)`](#everyredirectpageblock--Resource)\n      - [`every_timedout_page(&block : Resource ->)`](#everytimedoutpageblock--Resource)\n      - [`every_bad_request_page(&block : Resource ->)`](#everybadrequestpageblock--Resource)\n      - [`def every_unauthorized_page(&block : Resource ->)`](#def-everyunauthorizedpageblock--Resource)\n      - [`every_forbidden_page(&block : Resource ->)`](#everyforbiddenpageblock--Resource)\n      - [`every_missing_page(&block : Resource ->)`](#everymissingpageblock--Resource)\n      - [`every_internal_server_error_page(&block : Resource ->)`](#everyinternalservererrorpageblock--Resource)\n      - [`every_txt_page(&block : Resource ->)`](#everytxtpageblock--Resource)\n      - [`every_html_page(&block : Resource ->)`](#everyhtmlpageblock--Resource)\n      - [`every_xml_page(&block : Resource ->)`](#everyxmlpageblock--Resource)\n      - [`every_xsl_page(&block : Resource ->)`](#everyxslpageblock--Resource)\n      - [`every_doc(&block : Document::HTML | XML::Node ->)`](#everydocblock--DocumentHTML--XMLNode)\n      - [`every_html_doc(&block : Document::HTML | XML::Node ->)`](#everyhtmldocblock--DocumentHTML--XMLNode)\n      - [`every_xml_doc(&block : XML::Node ->)`](#everyxmldocblock--XMLNode)\n      - [`every_xsl_doc(&block : XML::Node ->)`](#everyxsldocblock--XMLNode)\n      - [`every_rss_doc(&block : XML::Node ->)`](#everyrssdocblock--XMLNode)\n      - [`every_atom_doc(&block : XML::Node ->)`](#everyatomdocblock--XMLNode)\n      - [`every_javascript(&block : Resource ->)`](#everyjavascriptblock--Resource)\n      - [`every_css(&block : Resource ->)`](#everycssblock--Resource)\n      - [`every_rss(&block : Resource ->)`](#everyrssblock--Resource)\n      - [`every_atom(&block : Resource ->)`](#everyatomblock--Resource)\n      - [`every_ms_word(&block : Resource ->)`](#everymswordblock--Resource)\n      - [`every_pdf(&block : Resource ->)`](#everypdfblock--Resource)\n      - [`every_zip(&block : Resource ->)`](#everyzipblock--Resource)\n      - [`every_image(&block : Resource ->)`](#everyimageblock--Resource)\n      - [`every_content_type(content_type : String | Regex, &block : Resource ->)`](#everycontenttypecontenttype--String--Regex-block--Resource)\n      - [`every_link(&block : URI, URI ->)`](#everylinkblock--URI-URI)\n    - [Content Types](#Content-Types)\n    - [Parsing HTML](#Parsing-HTML)\n  - [Contributing](#Contributing)\n  - [Contributors](#Contributors)\n\n## Installation\n\n1. Add the dependency to your `shard.yml`:\n\n   ```yaml\n   dependencies:\n     arachnid:\n       github: watzon/arachnid\n       version: ~> 0.1.0\n   ```\n\n2. Run `shards install`\n\nTo build the CLI\n\n1. Run `shards build --release`\n\n2. Add the `./bin` directory to your path or symlink `./bin/arachnid` with `sudo ln -s /home/path/to/arachnid /usr/local/bin`\n\n## The CLI\n\nArachnid provides a CLI for basic scanning tasks, here is what you can do with it so far:\n\n### Summarize\n\nThe `summarize` subcommand allows you to generate a report for a website. It can give you the number of pages, the internal and external links for every page, and a list of pages and their status codes (helpful for finding broken pages).\n\nYou can use it like this:\n\n```\narachnid summarize https://crystal-lang.org --ilinks --elinks -c 404 503\n```\n\nThis will generate a report for crystal-lang.org which will include every page and it\'s internal and external links, and a list of every page that returned a 404 or 503 status. For complete help use `arachnid summarize --help`\n\n### Sitemap\n\nArachnid can also generate a XML or JSON sitemap for a website by scanning the entire site, following internal links. To do so just use the `arachnid sitemap` subcommand.\n\n```\n# XML sitemap\narachnid sitemap https://crystal-lang.org --xml\n\n# JSON sitemap\narachnid sitemap https://crystal-lang.org --json\n\n# Custom output file\narachnid sitemap https://crystal-lang.org --xml -o ~/Desktop/crystal-lang.org-sitemap.xml\n```\n\nFull help is available with `arachnid sitemap --help`\n\n## Examples\n\nArachnid provides an easy to use, powerful DSL for scraping websites.\n\n```crystal\nrequire "arachnid"\nrequire "json"\n\n# Let\'s build a sitemap of crystal-lang.org\n# Links will be a hash of url to resource title\nlinks = {} of String => String\n\n# Visit a particular host, in this case `crystal-lang.org`. This will\n# not match on subdomains.\nArachnid.host("https://crystal-lang.org") do |spider|\n  # Ignore the API secion. It\'s a little big.\n  spider.ignore_urls_like(/\\/(api)\\//)\n\n  spider.every_html_page do |page|\n    puts "Visiting #{page.url.to_s}"\n\n    # Ignore redirects for our sitemap\n    unless page.redirect?\n      # Add the url of every visited page to our sitemap\n      links[page.url.to_s] = page.title.to_s.strip\n    end\n  end\nend\n\nFile.write("crystal-lang.org-sitemap.json", links.to_pretty_json)\n```\n\nWant to scan external links as well?\n\n```crystal\n# To make things interesting, this time let\'s download\n# every image we find.\nArachnid.start_at("https://crystal-lang.org") do |spider|\n  # Set a base path to store all the images at\n  base_image_dir = File.expand_path("~/Pictures/arachnid")\n  Dir.mkdir_p(base_image_dir)\n\n  # You could also use `every_image`. This allows us to\n  # track the crawler though.\n  spider.every_resource do |resource|\n    puts "Scanning #{resource.url.to_s}"\n\n    if resource.image?\n      # Since we\'re going to be saving a lot of images\n      # let\'s spawn a new fiber for each one. This\n      # makes things so much faster.\n      spawn do\n        # Output directory for images for this host\n        directory = File.join(base_image_dir, resource.url.host.to_s)\n        Dir.mkdir_p(directory)\n\n        # The name of the image\n        filename = File.basename(resource.url.path)\n\n        # Save the image using the body of the resource\n        puts "Saving #{filename} to #{directory}"\n        File.write(File.join(directory, filename), resource.body)\n      end\n    end\n  end\nend\n```\n\n## Usage\n\n### Configuration\n\nArachnid has a ton of configration options which can be passed to the mehthods listed below in [Crawling](#crawling) and to the constructor for `Arachnid::Agent`. They are as follows:\n\n- **read_timeout** - Read timeout\n- **connect_timeout** - Connect timeout\n- **max_redirects** - Maximum amount of redirects to follow\n- **default_headers** - Default HTTP headers to use for all hosts\n- **host_header** - HTTP host header to use\n- **host_headers** - HTTP headers to use for specific hosts\n- **user_agent** - sets the user agent for the crawler\n- **referer** - Referer to use\n- **fetch_delay** - Delay in between fetching resources\n- **queue** - Preload the queue with urls\n- **fibers** - Maximum amount of fibers to spin up for asynchronous processing\n- **history** - Links that should not be visited\n- **limit** - Maximum number of resources to visit\n- **max_depth** - Maximum crawl depth\n\nThere are also a few class properties on `Arachnid` itself which are used as the defaults, unless overrided.\n\n- **max_redirects**\n- **connect_timeout**\n- **read_timeout**\n- **user_agent**\n\n### Crawling\n\nArachnid provides 3 interfaces to use for crawling:\n\n#### `Arachnid#start_at(url, **options, &block : Agent ->)`\n\n`start_at` is what you want to use if you\'re going to be doing a full crawl of multiple sites. It doesn\'t filter any urls by default and will scan every link it encounters.\n\n#### `Arachnid#site(url, **options, &block : Agent ->)`\n\n`site` constrains the crawl to a specific site. "site" in this case is defined as all paths within a domain and it\'s subdomains.\n\n#### `Arachnid#host(name, **options, &block : Agent ->)`\n\n`host` is similar to site, but stays within the domain, not crawling subdomains.\n\n*Maybe `site` and `host` should be swapped? I don\'t know what is more intuitive.*\n\n### Crawling Rules\n\nArachnid has the concept of **filters** for the purpose of filtering urls before visiting them. They are as follows:\n\n- **hosts**\n  - [visit_hosts_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#visit_hosts_like%28pattern%29-instance-method)\n  - [ignore_hosts_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#ignore_hosts_like%28pattern%29-instance-method)\n- **ports**\n  - [visit_ports_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#visit_ports-instance-method)\n  - [ignore_ports_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#ignore_ports-instance-method)\n- **ports**\n  - [visit_ports_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#visit_ports_like%28pattern%29-instance-method)\n  - [ignore_ports_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#ignore_ports_like%28pattern%29-instance-method)\n- **links**\n  - [visit_links_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#visit_links_like(pattern)-instance-method)\n  - [ignore_links_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#ignore_links_like(pattern)-instance-method)\n- **urls**\n  - [visit_urls_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#visit_urls_like%28pattern%29-instance-method)\n  - [ignore_urls_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#ignore_urls_like%28pattern%29-instance-method)\n- **exts**\n  - [visit_exts_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#visit_exts_like%28pattern%29-instance-method)\n  - [ignore_exts_like(pattern : String | Regex)](https://watzon.github.io/arachnid/Arachnid/Agent.html#ignore_exts_like%28pattern%29-instance-method)\n\nAll of these methods have the ability to also take a block instead of a pattern, where the block returns true or false. The only difference between `links` and `urls` in this case is with the block argument. `links` receives a `String` and `urls` a `URI`. Honestly I\'ll probably get rid of `links` soon and just make it `urls`.\n\n`exts` looks at the extension, if it exists, and fiters base on that.\n\n### Events\n\nEvery crawled "page" is referred to as a resource, since sometimes they will be html/xml, sometimes javascript or css, and sometimes images, videos, zip files, etc. Every time a resource is scanned one of several events is called. They are:\n\n#### `every_url(&block : URI ->)`\nPass each URL from each resource visited to the given block.\n\n#### `every_failed_url(&block : URI ->)`\nPass each URL that could not be requested to the given block.\n\n#### `every_url_like(pattern, &block : URI ->)`\nPass every URL that the agent visits, and matches a given pattern, to a given block.\n\n#### `urls_like(pattern, &block : URI ->)`\nSame as `every_url_like`\n\n#### `all_headers(&block : HTTP::Headers)`\nPass the headers from every response the agent receives to a given block.\n\n#### `every_resource(&block : Resource ->)`\nPass every resource that the agent visits to a given block.\n\n#### `every_ok_page(&block : Resource ->)`\nPass every OK resource that the agent visits to a given block.\n\n#### `every_redirect_page(&block : Resource ->)`\nPass every Redirect resource that the agent visits to a given block.\n\n#### `every_timedout_page(&block : Resource ->)`\nPass every Timeout resource that the agent visits to a given block.\n\n#### `every_bad_request_page(&block : Resource ->)`\nPass every Bad Request resource that the agent visits to a given block.\n\n#### `def every_unauthorized_page(&block : Resource ->)`\nPass every Unauthorized resource that the agent visits to a given block.\n\n#### `every_forbidden_page(&block : Resource ->)`\nPass every Forbidden resource that the agent visits to a given block.\n\n#### `every_missing_page(&block : Resource ->)`\nPass every Missing resource that the agent visits to a given block.\n\n#### `every_internal_server_error_page(&block : Resource ->)`\nPass every Internal Server Error resource that the agent visits to a given block.\n\n#### `every_txt_page(&block : Resource ->)`\nPass every Plain Text resource that the agent visits to a given block.\n\n#### `every_html_page(&block : Resource ->)`\nPass every HTML resource that the agent visits to a given block.\n\n#### `every_xml_page(&block : Resource ->)`\nPass every XML resource that the agent visits to a given block.\n\n#### `every_xsl_page(&block : Resource ->)`\nPass every XML Stylesheet (XSL) resource that the agent visits to a given block.\n\n#### `every_doc(&block : Document::HTML | XML::Node ->)`\nPass every HTML or XML document that the agent parses to a given block.\n\n#### `every_html_doc(&block : Document::HTML | XML::Node ->)`\nPass every HTML document that the agent parses to a given block.\n\n#### `every_xml_doc(&block : XML::Node ->)`\nPass every XML document that the agent parses to a given block.\n\n#### `every_xsl_doc(&block : XML::Node ->)`\nPass every XML Stylesheet (XSL) that the agent parses to a given block.\n\n#### `every_rss_doc(&block : XML::Node ->)`\nPass every RSS document that the agent parses to a given block.\n\n#### `every_atom_doc(&block : XML::Node ->)`\nPass every Atom document that the agent parses to a given block.\n\n#### `every_javascript(&block : Resource ->)`\nPass every JavaScript resource that the agent visits to a given block.\n\n#### `every_css(&block : Resource ->)`\nPass every CSS resource that the agent visits to a given block.\n\n#### `every_rss(&block : Resource ->)`\nPass every RSS feed that the agent visits to a given block.\n\n#### `every_atom(&block : Resource ->)`\nPass every Atom feed that the agent visits to a given block.\n\n#### `every_ms_word(&block : Resource ->)`\nPass every MS Word resource that the agent visits to a given block.\n\n#### `every_pdf(&block : Resource ->)`\nPass every PDF resource that the agent visits to a given block.\n\n#### `every_zip(&block : Resource ->)`\nPass every ZIP resource that the agent visits to a given block.\n\n#### `every_image(&block : Resource ->)`\nPasses every image resource to the given block.\n\n#### `every_content_type(content_type : String | Regex, &block : Resource ->)`\nPasses every resource with a matching content type to the given block.\n\n#### `every_link(&block : URI, URI ->)`\nPasses every origin and destination URI of each link to a given block.\n\n### Content Types\n\nEvery resource has an associated content type and the `Resource` class itself provides several easy methods to check it. You can find all of them [here](https://watzon.github.io/arachnid/Arachnid/Resource/ContentTypes.html).\n\n### Parsing HTML\n\nEvery HTML/XML resource has full access to the suite of methods provided by [Crystagiri](https://github.com/madeindjs/Crystagiri/) allowing you to more easily search by css selector.\n\n## Contributing\n\n1. Fork it (<https://github.com/watzon/arachnid/fork>)\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am \'Add some feature\'`)\n4. Push to the branch (`git push origin my-new-feature`)\n5. Create a new Pull Request\n\n## Contributors\n\n- [Chris Watson](https://github.com/watzon) - creator and maintainer\n'