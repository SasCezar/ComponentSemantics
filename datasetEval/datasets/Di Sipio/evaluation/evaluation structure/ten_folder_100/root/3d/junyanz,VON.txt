b"# Visual Object Networks\n\n\n<img src='imgs/teaser.jpg' width=820>\n\n[Project Page](http://von.csail.mit.edu) |  [Paper](http://arxiv.org/abs/1812.02725)\n\nWe present Visual Object Networks (VON), an end-to-end adversarial learning framework that jointly models 3D shapes and 2D images.  Our model can synthesize a 3D shape, its intermediate 2.5D depth representation, and a 2D image all at once. The VON not only generates realistic images but also enables several 3D operations.\n\n\n\nVisual Object Networks: Image Generation with Disentangled 3D Representation.<br/>\n[Jun-Yan Zhu](http://people.csail.mit.edu/junyanz/),\n [Zhoutong Zhang](https://www.csail.mit.edu/person/zhoutong-zhang), [Chengkai Zhang](https://scholar.google.com/citations?user=rChGGwgAAAAJ&hl=en), [Jiajun Wu](https://jiajunwu.com/), [Antonio Torralba](http://web.mit.edu/torralba/www/), [    Joshua B. Tenenbaum](http://web.mit.edu/cocosci/josh.html), [William T. Freeman](http://billf.mit.edu/).<br/>\nMIT CSAIL and Google Research.<br/>\nIn NeurIPS 2018.\n\n## Example results\n(a) Typical examples produced by a recent GAN model [Gulrajani et al., 2017].<br/>\n(b) Our model produces three outputs: a 3D shape, its 2.5D projection given a viewpoint, and a final image with realistic texture.<br/>\n(c) Our model allows several 3D applications including editing viewpoint, shape, or texture independently.\n\n<img src='imgs/overview.jpg' width=800>\n\n## More samples\nBelow we show more samples from DCGAN [Radford et al., 2016], LSGAN [Mao et al., 2017], WGAN-GP [Gulrajani et al., 2017], and our VON. For our method, we show both 3D shapes and 2D images. The learned 3D prior helps produce better samples.\n\n<img src='imgs/samples.jpg' width=820>\n\n## 3D object manipulations\nOur VON allows several 3D applications such as (left) changing the viewpoint, texture, or shape independently, and (right) interpolating between two objects in shape space, texture space, or both.\n\n<img src='imgs/app.jpg' width=820>\n\n## Transfer texture across objects and viewpoints\nVON can transfer the texture of a real image to different shapes and viewpoints\n\n<img src='imgs/transfer.jpg' width=820>\n\n## Prerequisites\n- Linux (only tested on Ubuntu 16.04)\n- Python3 (only tested with python 3.6)\n- Anaconda3\n- NVCC & GCC (only tested with gcc 6.3.0)\n- PyTorch 0.4.1 (does not support 0.4.0)\n- Currently not tested with Nvidia RTX GPU series\n- Docker Engine and Nvidia-Docker2 if using Docker container.\n\n\n## Getting Started ###\n### Installation\n- Clone this repo:\n```bash\ngit clone -b master --single-branch https://github.com/junyanz/VON.git\ncd VON\n```\n- Install PyTorch 0.4.1+ and torchvision from http://pytorch.org and other dependencies (e.g., [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate)). You can install all the dependencies by the following:\n```bash\nconda create --name von --file pkg_specs.txt\nsource activate von\n```\n\n- Compile our rendering kernel by running the following:\n```bash\nbash install.sh\n```\nWe only test this step with `gcc 6.3.0`. If you need to recompile the kernel, please run `bash clean.sh` first before you recompile it.\n\n- If you can not compile the custom kernels, we provide a Dockerfile for building a working container. To use the Dockerfile, you need to install [Docker Engine](https://www.docker.com) and [Nvidia-Docker2](https://github.com/NVIDIA/nvidia-docker) for using Nvidia GPUs inside the docker container. To build the docker image, run:\n```bash\nsudo docker build ./../von -t von\n```\nTo access the container, run:\n```bash\nsudo docker run -it --runtime=nvidia --ipc=host von /bin/bash\n```\nThen, to compile the kernels, simply:\n```bash\ncd /app/von\nsource activate von\n./install.sh\n```\n\n- (Optional) Install [blender](https://www.blender.org/) for visualizing generated 3D shapes. After installation, please add blender to the PATH environment variable.\n\n### Generate 3D shapes, 2.5D sketches, and images\n- Download our pretrained models:\n```bash\nbash ./scripts/download_model.sh\n```\n\n- Generate results with the model\n```\nbash ./scripts/figures.sh 0 car df\n```\n\nThe test results will be saved to an HTML file here: `./results/*/*/index.html`.\n\n### Model Training\n- To train a model, download the training dataset (distance functions and images). For example, if we would like to train a car model with distance function representation on GPU 0.\n```bash\nbash ./scripts/download_dataset.sh\n```\n- To train a 3D generator:\n```bash\nbash ./scripts/train_shape.sh 0 car df\n```\n- To train a 2D texture network using ShapeNet real shapes:\n```bash\nbash ./scripts/train_texture_real.sh 0 car df 0\n```\n\n- To train a 2D texture network using pre-trained 3D generator:\n```bash\nbash ./scripts/train_texture.sh 0 car df 0\n```\n\n- Jointly finetune 3D and 2D generative models:\n```bash\nbash ./scripts/train_full.sh 0 car df 0\n```\n\n- To view training results and loss plots, go to http://localhost:8097 in a web browser. To see more intermediate results, check out  `./checkpoints/*/web/index.html`\n\n\n### Citation\n\nIf you find this useful for your research, please cite the following paper.\n```\n@inproceedings{VON,\n  title={Visual Object Networks: Image Generation with Disentangled 3{D} Representations},\n  author={Jun-Yan Zhu and Zhoutong Zhang and Chengkai Zhang and Jiajun Wu and Antonio Torralba and Joshua B. Tenenbaum and William T. Freeman},\n  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},\n  year={2018}\n}\n\n```\n\n### Acknowledgements\nThis work is supported by NSF #1231216, NSF #1524817, ONR MURI N00014-16-1-2007, Toyota Research Institute, Shell, and Facebook. We thank Xiuming Zhang, Richard Zhang, David Bau, and Zhuang Liu for valuable discussions. This code borrows from the [CycleGAN & pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) repo.\n"