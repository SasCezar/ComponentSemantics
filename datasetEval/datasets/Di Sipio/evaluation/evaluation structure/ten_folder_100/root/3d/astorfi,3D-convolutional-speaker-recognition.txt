b'\n=========================================================================================================================\nTensorFlow implementation of 3D Convolutional Neural Networks for Speaker Verification - `Official Project Page`_ - `Pytorch Implementation`_\n=========================================================================================================================\n\n\n.. image:: https://travis-ci.org/astorfi/3D-convolutional-speaker-recognition.svg?branch=master\n    :target: https://travis-ci.org/astorfi/3D-convolutional-speaker-recognition\n.. image:: https://codecov.io/gh/astorfi/3D-convolutional-speaker-recognition/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/astorfi/3D-convolutional-speaker-recognition\n.. image:: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\n    :target: https://github.com/astorfi/3D-convolutional-speaker-recognition/pulls\n.. image:: https://badges.frapsoft.com/os/v2/open-source.svg?v=102\n    :target: https://github.com/ellerbrock/open-source-badge/\n.. image:: https://zenodo.org/badge/94718341.svg\n   :target: https://zenodo.org/badge/latestdoi/94718341\n.. image:: https://img.shields.io/twitter/follow/amirsinatorfi.svg?label=Follow&style=social\n      :target: https://twitter.com/amirsinatorfi\n\n\nThis repository contains the code release for our paper titled as *"Text-Independent\nSpeaker Verification Using 3D Convolutional Neural Networks"*. The link to the paper_ is\nprovided as well.\n\n\n.. _Official Project Page: https://codeocean.com/2017/08/01/3d-convolutional-neural-networks-for-speaker-recognition/code\n.. _Pytorch Implementation: https://github.com/astorfi/3D-convolutional-speaker-recognition-pytorch\n\n.. _paper: https://arxiv.org/abs/1705.09422\n.. _TensorFlow: https://www.tensorflow.org/\n\nThe code has been developed using TensorFlow_. The input pipeline must be prepared by the users.\nThis code is aimed to provide the implementation for Speaker Verification (SR) by using 3D convolutional neural networks\nfollowing the SR protocol.\n\n.. image:: readme_images/conv_gif.gif\n    :target: https://github.com/astorfi/3D-convolutional-speaker-recognition/blob/master/_images/conv_gif.gif\n\n------------\nCitation\n------------\n\nIf you used this code, please kindly consider citing the following paper:\n\n.. code:: shell\n\n    @article{torfi2017text,\n      title={Text-independent speaker verification using 3d convolutional neural networks},\n      author={Torfi, Amirsina and Nasrabadi, Nasser M and Dawson, Jeremy},\n      journal={arXiv preprint arXiv:1705.09422},\n      year={2017}\n    }\n\n-----\nDEMO\n-----\n\nFor running a demo, after forking the repository, run the following scrit:\n\n.. code:: shell\n\n    ./run.sh\n\n|speakerrecognition|\n\n.. |speakerrecognition| image:: readme_images/speakerrecognition.png\n    :target: https://asciinema.org/a/yfy6FryUAWWMl1vgylrRagMdw\n\n\n\n--------------\nGeneral View\n--------------\n\nWe leveraged 3D convolutional architecture for creating the speaker model in order to simultaneously\ncapturing the speech-related and temporal information from the speakers\' utterances.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nSpeaker Verification Protocol(SVP)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn this work, a 3D Convolutional Neural Network (3D-CNN)\narchitecture has been utilized for text-independent speaker\nverification in three phases.\n\n     1. At the **development phase**, a CNN is trained\n     to classify speakers at the utterance-level.\n\n     2. In the **enrollment stage**, the trained network is utilized to directly create a\n     speaker model for each speaker based on the extracted features.\n\n     3. Finally, in the **evaluation phase**, the extracted features\n     from the test utterance will be compared to the stored speaker\n     model to verify the claimed identity.\n\nThe aforementioned three phases are usually considered as the SV protocol. One of the main\nchallenges is the creation of the speaker models. Previously-reported approaches create\nspeaker models based on averaging the extracted features from utterances of the speaker,\nwhich is known as the d-vector system.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nHow to leverage 3D Convolutional Neural Networks?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn our paper, we propose the implementation of 3D-CNNs for direct speaker model creation\nin which, for both development and enrollment phases, an identical number of\nspeaker utterances is fed to the network for representing the spoken utterances\nand creation of the speaker model. This leads to simultaneously capturing the\nspeaker-related information and building a more robust system to cope with\nwithin-speaker variation. We demonstrate that the proposed method significantly\noutperforms the d-vector verification system.\n\n\n--------------------\nCode Implementation\n--------------------\n\nThe input pipeline must be provided by the user. **Please refer to ``code/0-input/input_feature.py`` for having an idea about how the input pipeline works.**\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nInput Pipeline for this work\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. image:: readme_images/Speech_GIF.gif\n    :target: https://github.com/astorfi/3D-convolutional-speaker-recognition/blob/master/_images/Speech_GIF.gif\n\nThe MFCC features can be used as the data representation of the spoken utterances at the frame level. However, a\ndrawback is their non-local characteristics due to the last DCT 1 operation for generating MFCCs. This operation disturbs the locality property and is in contrast with the local characteristics of the convolutional operations. The employed approach in this work is to use the log-energies, which we\ncall MFECs. The extraction of MFECs is similar to MFCCs\nby discarding the DCT operation. The temporal features are\noverlapping 20ms windows with the stride of 10ms, which are\nused for the generation of spectrum features. From a 0.8-\nsecond sound sample, 80 temporal feature sets (each forms\na 40 MFEC features) can be obtained which form the input\nspeech feature map. Each input feature map has the dimen-\nsionality of \xce\xb6 \xc3\x97 80 \xc3\x97 40 which is formed from 80 input\nframes and their corresponding spectral features, where \xce\xb6 is\nthe number of utterances used in modeling the speaker during\nthe development and enrollment stages.\n\n\n\nThe **speech features** have been extracted using [SpeechPy]_ package.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nImplementation of 3D Convolutional Operation\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. _Slim: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\n\nThe Slim_ high-level API made our life very easy. The following script has been used for our\nimplementation:\n\n.. code:: python\n\n        net = slim.conv2d(inputs, 16, [3, 1, 5], stride=[1, 1, 1], scope=\'conv11\')\n        net = PReLU(net, \'conv11_activation\')\n        net = slim.conv2d(net, 16, [3, 9, 1], stride=[1, 2, 1], scope=\'conv12\')\n        net = PReLU(net, \'conv12_activation\')\n        net = tf.nn.max_pool3d(net, strides=[1, 1, 1, 2, 1], ksize=[1, 1, 1, 2, 1], padding=\'VALID\', name=\'pool1\')\n\n        ############ Conv-2 ###############\n        ############ Conv-1 ###############\n        net = slim.conv2d(net, 32, [3, 1, 4], stride=[1, 1, 1], scope=\'conv21\')\n        net = PReLU(net, \'conv21_activation\')\n        net = slim.conv2d(net, 32, [3, 8, 1], stride=[1, 2, 1], scope=\'conv22\')\n        net = PReLU(net, \'conv22_activation\')\n        net = tf.nn.max_pool3d(net, strides=[1, 1, 1, 2, 1], ksize=[1, 1, 1, 2, 1], padding=\'VALID\', name=\'pool2\')\n\n        ############ Conv-3 ###############\n        ############ Conv-1 ###############\n        net = slim.conv2d(net, 64, [3, 1, 3], stride=[1, 1, 1], scope=\'conv31\')\n        net = PReLU(net, \'conv31_activation\')\n        net = slim.conv2d(net, 64, [3, 7, 1], stride=[1, 1, 1], scope=\'conv32\')\n        net = PReLU(net, \'conv32_activation\')\n        # net = slim.max_pool2d(net, [1, 1], stride=[4, 1], scope=\'pool1\')\n\n        ############ Conv-4 ###############\n        net = slim.conv2d(net, 128, [3, 1, 3], stride=[1, 1, 1], scope=\'conv41\')\n        net = PReLU(net, \'conv41_activation\')\n        net = slim.conv2d(net, 128, [3, 7, 1], stride=[1, 1, 1], scope=\'conv42\')\n        net = PReLU(net, \'conv42_activation\')\n        # net = slim.max_pool2d(net, [1, 1], stride=[4, 1], scope=\'pool1\')\n\n        ############ Conv-5 ###############\n        net = slim.conv2d(net, 128, [4, 3, 3], stride=[1, 1, 1], normalizer_fn=None, scope=\'conv51\')\n        net = PReLU(net, \'conv51_activation\')\n\n        # net = slim.conv2d(net, 256, [1, 1], stride=[1, 1], scope=\'conv52\')\n        # net = PReLU(net, \'conv52_activation\')\n\n        # Last layer which is the logits for classes\n        logits = tf.contrib.layers.conv2d(net, num_classes, [1, 1, 1], activation_fn=None, scope=\'fc\')\n\n\nAs it can be seen, ``slim.conv2d`` has been used. However, simply by using 3D kernels as ``[k_x, k_y, k_z]``\nand ``stride=[a, b, c]`` it can be turned into a 3D-conv operation. The base of the ``slim.conv2d`` is\n``tf.contrib.layers.conv2d``. Please refer to official Documentation_ for further details.\n\n.. _Documentation: https://www.tensorflow.org/api_docs/python/tf/contrib/layers\n\n\n-----------\nDisclaimer\n-----------\n\n.. _link: https://github.com/tensorflow/models/tree/master/slim\n\nThe code architecture part has been heavily inspired by Slim_ and Slim image classification\nlibrary. Please refer to this link_ for further details.\n\n---------\nCitation\n---------\n\nIf you used this code please kindly cite the following paper:\n\n.. code:: shell\n\n  @article{torfi2017text,\n    title={Text-Independent Speaker Verification Using 3D Convolutional Neural Networks},\n    author={Torfi, Amirsina and Nasrabadi, Nasser M and Dawson, Jeremy},\n    journal={arXiv preprint arXiv:1705.09422},\n    year={2017}\n  }\n\n--------\nLicense\n--------\n\nThe license is as follows:\n\n.. code:: shell\n\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets "{}"\n      replaced with your own identifying information. (Don\'t include the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same "printed page" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {2017} {Amirsina Torfi}\n\n   Licensed under the Apache License, Version 2.0 (the "License");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an "AS IS" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n\nPlease refer to LICENSE_ file for further detail.\n\n.. _LICENSE: https://github.com/astorfi/3D-convolutional-speaker-recognition/blob/master/LICENSE\n\n\n-------------\nContribution\n-------------\n\nWe are looking forward to your kind feedback. Please help us to improve the code and make\nour work better. For contribution, please create the pull request and we will investigate it promptly.\nOnce again, we appreciate your feedback and code inspections.\n\n\n.. rubric:: references\n\n.. [SpeechPy] Amirsina Torfi. 2017. astorfi/speech_feature_extraction: SpeechPy. Zenodo. doi:10.5281/zenodo.810392.\n'