b'<p align="center"><img width="55%" src="docs/_static/img/logo.svg" /></p>\n\n<h3 align="center">Basic Utilities for PyTorch Natural Language Processing (NLP)</h3>\n\nPyTorch-NLP, or `torchnlp` for short, is a library of basic utilities for PyTorch\nNLP. `torchnlp` extends PyTorch to provide you with\nbasic text data processing functions.\n\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pytorch-nlp.svg?style=flat-square)\n[![Codecov](https://img.shields.io/codecov/c/github/PetrochukM/PyTorch-NLP/master.svg?style=flat-square)](https://codecov.io/gh/PetrochukM/PyTorch-NLP)\n[![Downloads](http://pepy.tech/badge/pytorch-nlp)](http://pepy.tech/project/pytorch-nlp)\n[![Documentation Status](https://img.shields.io/readthedocs/pytorchnlp/latest.svg?style=flat-square)](http://pytorchnlp.readthedocs.io/en/latest/?badge=latest&style=flat-square)\n[![Build Status](https://img.shields.io/travis/PetrochukM/PyTorch-NLP/master.svg?style=flat-square)](https://travis-ci.org/PetrochukM/PyTorch-NLP)\n[![Twitter: PetrochukM](https://img.shields.io/twitter/follow/MPetrochuk.svg?style=social)](https://twitter.com/MPetrochuk)\n\n_Logo by [Chloe Yeo](http://www.yeochloe.com/), Corporate Sponsorship by [WellSaid Labs](https://wellsaidlabs.com/)_\n\n## Installation \xf0\x9f\x90\xbe\n\nMake sure you have Python 3.5+ and PyTorch 1.0+. You can then install `pytorch-nlp` using\npip:\n\n```python\npip install pytorch-nlp\n```\n\nOr to install the latest code via:\n\n```python\npip install git+https://github.com/PetrochukM/PyTorch-NLP.git\n```\n\n## Docs\n\nThe complete documentation for PyTorch-NLP is available\nvia [our ReadTheDocs website](https://pytorchnlp.readthedocs.io).\n\n## Get Started\n\nWithin an NLP data pipeline, you\'ll want to implement these basic steps:\n\n### 1. Load your Data \xf0\x9f\x90\xbf\n\nLoad the IMDB dataset, for example:\n\n```python\nfrom torchnlp.datasets import imdb_dataset\n\n# Load the imdb training dataset\ntrain = imdb_dataset(train=True)\ntrain[0]  # RETURNS: {\'text\': \'For a movie that gets..\', \'sentiment\': \'pos\'}\n```\n\nLoad a custom dataset, for example:\n\n```python\nfrom pathlib import Path\n\nfrom torchnlp.download import download_file_maybe_extract\n\ndirectory_path = Path(\'data/\')\ntrain_file_path = Path(\'trees/train.txt\')\n\ndownload_file_maybe_extract(\n    url=\'http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\',\n    directory=directory_path,\n    check_files=[train_file_path])\n\nopen(directory_path / train_file_path)\n```\n\nDon\'t worry we\'ll handle caching for you!\n\n### 2. Text to Tensor\n\nTokenize and encode your text as a tensor.\n\nFor example, a `WhitespaceEncoder` breaks\ntext into tokens whenever it encounters a whitespace character.\n\n```python\nfrom torchnlp.encoders.text import WhitespaceEncoder\n\nloaded_data = ["now this ain\'t funny", "so don\'t you dare laugh"]\nencoder = WhitespaceEncoder(loaded_data)\nencoded_data = [encoder.encode(example) for example in loaded_data]\n```\n\n### 3. Tensor to Batch\n\nWith your loaded and encoded data in hand, you\'ll want to batch your dataset.\n\n```python\nimport torch\nfrom torchnlp.samplers import BucketBatchSampler\nfrom torchnlp.utils import collate_tensors\nfrom torchnlp.encoders.text import stack_and_pad_tensors\n\nencoded_data = [torch.randn(2), torch.randn(3), torch.randn(4), torch.randn(5)]\n\ntrain_sampler = torch.utils.data.sampler.SequentialSampler(encoded_data)\ntrain_batch_sampler = BucketBatchSampler(\n    train_sampler, batch_size=2, drop_last=False, sort_key=lambda i: encoded_data[i].shape[0])\n\nbatches = [[encoded_data[i] for i in batch] for batch in train_batch_sampler]\nbatches = [collate_tensors(batch, stack_tensors=stack_and_pad_tensors) for batch in batches]\n```\n\nPyTorch-NLP builds on top of PyTorch\'s existing `torch.utils.data.sampler`, `torch.stack`\nand `default_collate` to support sequential inputs of varying lengths!\n\n### 4. Training and Inference\n\nWith your batch in hand, you can use PyTorch to develop and train your model using gradient descent.\nFor example, check out [this example code](examples/snli/train.py) for training on the Stanford\nNatural Language Inference (SNLI) Corpus.\n\n## Last But Not Least\n\nPyTorch-NLP has a couple more NLP focused utility packages to support you! \xf0\x9f\xa4\x97\n\n### Deterministic Functions\n\nNow you\'ve setup your pipeline, you may want to ensure that some functions run deterministically.\nWrap any code that\'s random, with `fork_rng` and you\'ll be good to go, like so:\n\n```python\nimport random\nimport numpy\nimport torch\n\nfrom torchnlp.random import fork_rng\n\nwith fork_rng(seed=123):  # Ensure determinism\n    print(\'Random:\', random.randint(1, 2**31))\n    print(\'Numpy:\', numpy.random.randint(1, 2**31))\n    print(\'Torch:\', int(torch.randint(1, 2**31, (1,))))\n```\n\nThis will always print:\n\n```text\nRandom: 224899943\nNumpy: 843828735\nTorch: 843828736\n```\n\n### Pre-Trained Word Vectors\n\nNow that you\'ve computed your vocabulary, you may want to make use of\npre-trained word vectors to set your embeddings, like so:\n\n```python\nimport torch\nfrom torchnlp.encoders.text import WhitespaceEncoder\nfrom torchnlp.word_to_vector import GloVe\n\nencoder = WhitespaceEncoder(["now this ain\'t funny", "so don\'t you dare laugh"])\n\npretrained_embedding = GloVe(name=\'6B\', dim=100, is_include=lambda w: w in set(encoder.vocab))\nembedding_weights = torch.Tensor(encoder.vocab_size, pretrained_embedding.dim)\nfor i, token in enumerate(encoder.vocab):\n    embedding_weights[i] = pretrained_embedding[token]\n```\n\n### Neural Networks Layers\n\nFor example, from the neural network package, apply the state-of-the-art `LockedDropout`:\n\n```python\nimport torch\nfrom torchnlp.nn import LockedDropout\n\ninput_ = torch.randn(6, 3, 10)\ndropout = LockedDropout(0.5)\n\n# Apply a LockedDropout to `input_`\ndropout(input_) # RETURNS: torch.FloatTensor (6x3x10)\n```\n\n### Metrics\n\nCompute common NLP metrics such as the BLEU score.\n\n```python\nfrom torchnlp.metrics import get_moses_multi_bleu\n\nhypotheses = ["The brown fox jumps over the dog \xe7\xac\x91"]\nreferences = ["The quick brown fox jumps over the lazy dog \xe7\xac\x91"]\n\n# Compute BLEU score with the official BLEU perl script\nget_moses_multi_bleu(hypotheses, references, lowercase=True)  # RETURNS: 47.9\n```\n\n### Help :question:\n\nMaybe looking at longer examples may help you at [`examples/`](examples/).\n\nNeed more help? We are happy to answer your questions via [Gitter Chat](https://gitter.im/PyTorch-NLP)\n\n## Contributing\n\nWe\'ve released PyTorch-NLP because we found a lack of basic toolkits for NLP in PyTorch. We hope\nthat other organizations can benefit from the project. We are thankful for any contributions from\nthe community.\n\n### Contributing Guide\n\nRead our [contributing guide](https://github.com/PetrochukM/PyTorch-NLP/blob/master/CONTRIBUTING.md)\nto learn about our development process, how to propose bugfixes and improvements, and how to build\nand test your changes to PyTorch-NLP.\n\n## Related Work\n\n### [torchtext](https://github.com/pytorch/text)\n\ntorchtext and PyTorch-NLP differ in the architecture and feature set; otherwise, they are similar.\ntorchtext and PyTorch-NLP provide pre-trained word vectors, datasets, iterators and text encoders.\nPyTorch-NLP also provides neural network modules and metrics. From an architecture standpoint,\ntorchtext is object orientated with external coupling while PyTorch-NLP is object orientated with\nlow coupling.\n\n### [AllenNLP](https://github.com/allenai/allennlp)\n\nAllenNLP is designed to be a platform for research. PyTorch-NLP is designed to be a lightweight toolkit.\n\n## Authors\n\n- [Michael Petrochuk](https://github.com/PetrochukM/) \xe2\x80\x94 Developer\n- [Chloe Yeo](http://www.yeochloe.com/) \xe2\x80\x94 Logo Design\n\n## Citing\n\nIf you find PyTorch-NLP useful for an academic publication, then please use the following BibTeX to\ncite it:\n\n```\n@misc{pytorch-nlp,\n  author = {Petrochuk, Michael},\n  title = {PyTorch-NLP: Rapid Prototyping with PyTorch Natural Language Processing (NLP) Tools},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/PetrochukM/PyTorch-NLP}},\n}\n```\n'