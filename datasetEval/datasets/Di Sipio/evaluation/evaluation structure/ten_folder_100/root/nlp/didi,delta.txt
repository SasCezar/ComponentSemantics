b'<div align="center">\n  <img src="docs/_static/delta_logo.png">\n</div>\n\n\n[![Build Status](https://travis-ci.org/didi/delta.svg?branch=master)](https://travis-ci.org/didi/delta)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![GitHub top language](https://img.shields.io/github/languages/top/didi/delta)](https://img.shields.io/github/languages/top/didi/delta)\n[![GitHub Issues](https://img.shields.io/github/issues/didi/delta.svg)](https://github.com/didi/delta/issues)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/didi/delta/blob/master/LICENSE)\n\n# DELTA - A DEep learning Language Technology plAtform\n\n\n## What is DELTA?\n\n**DELTA** is a deep learning based end-to-end **natural language and speech processing** platform. \nDELTA aims to provide easy and fast experiences for using, deploying, and developing natural language processing and speech models for both academia and industry use cases. DELTA is mainly implemented using TensorFlow and Python 3.\n\nFor details of DELTA, please refer to this [paper](https://arxiv.org/abs/1908.01853).\n\n## What can DELTA do?\n\nDELTA has been used for developing several state-of-the-art algorithms for publications and delivering real production to serve millions of users. \nIt helps you to train, develop, and deploy NLP and/or speech models, featuring:\n\n - Easy-to-use\n   - One command to train NLP and speech models, including:\n     - NLP: text classification, named entity recognition, question and answering, text summarization, etc\n     - Speech: speech recognition, speaker verification, emotion recognition, etc\n   - Use configuration files to easily tune parameters and network structures\n - Easy-to-deploy\n   - What you see in training is what you get in serving: all data processing and features extraction are integrated into a model graph\n   - Uniform I/O interfaces and no changes for new models\n - Easy-to-develop\n   - Easily build state-of-the-art models using modularized components\n   - All modules are reliable and fully-tested\n\n## Table of Contents\n\n- [Installation](#installation)\n- [Quick Start](#quick-start)\n- [Benchmarks](#benchmarks)\n- [FAQ](#faq)\n- [Contributing](#contributing)\n- [References](#references)\n- [License](#license)\n- [Acknowledgement](#acknowledgement)\n\n## Installation\n\nWe provide several approach to install DELTA:\n\n- If you are only interested in NLP tasks, you can [use `pip` to install](#install-from-pip\n) DELTA.\n\n- If you are interested in both NLP and speech tasks, you can install DELTA [from the source code](#install-from-source-code).\n\n- If you are interested in model deployment, you may install DELTA [from the source code](#install-from-source-code) or [from `docker`](#install-from-docker).\n\n### Install from pip\n\nWe provide the pip install support for `nlp` version of DELTA.\n\n**Note**: Users can still install DELTA from the source for both `nlp` and `speech` tasks.\n\nWe recommend to create [conda](https://conda.io/) or\n[virtualenv](https://virtualenv.pypa.io/en/latest/) and install DELTA\nfrom pip in the virtual environment. For example\n```bash\nconda create -n delta-pip-py3.6 python=3.6\nconda activate delta-pip-py3.6\n```\n\nPlease install TensorFlow 2.x if you have not installed it in your system.\n```bash\npip install tensorflow\n```\n\nThen, simply install DELTA use the following command:\n```bash\npip install delta-nlp\n```\n\nAfter install DELTA, you can follow this example to train NLP models or develop new models.\n[A Text Classification Usage Example for pip users](docs/tutorials/training/text_class_pip_example.md)\n\n### Install from Source Code\n\nTo install from the source code, we use [conda](https://conda.io/) to\ninstall required packages. Please\n[install conda](https://conda.io/en/latest/miniconda.html) if you do not\nhave it in your system.\n\nAlso, we provide two options to install DELTA, `nlp` version or `full`\nversion. `nlp` version needs minimal requirements and only installs NLP\nrelated packages:\n\n```shell\n# Run the installation script for NLP version, with CPU or GPU.\ncd tools\n./install/install-delta.sh nlp [cpu|gpu]\n```\n\n**Note**: Users from mainland China may need to set up conda mirror sources, see [./tools/install/install-delta.sh](tools/install/install-delta.sh) for details.\n\nIf you want to use both NLP and speech packages, you can install the `full` version. The full version needs [Kaldi](https://github.com/kaldi-asr/kaldi) library, which can be pre-installed or installed using our installation script.\n\n```shell\ncd tools\n# If you have installed Kaldi\nKALDI=/your/path/to/Kaldi ./install/install-delta.sh full [cpu|gpu]\n# If you have not installed Kaldi, use the following command\n# ./install/install-delta.sh full [cpu|gpu]\n```\n\nTo verify the installation, run:\n\n```shell\n# Activate conda environment\nconda activate delta-py3.6-tf2.0.0\n# Or use the following command if your conda version is < 4.6\n# source activate delta-py3.6-tf2.0.0\n\n# Add DELTA environment\nsource env.sh\n\n# Generate mock data for text classification.\npushd egs/mock_text_cls_data/text_cls/v1\n./run.sh\npopd\n\n# Train the model\npython3 delta/main.py --cmd train_and_eval --config egs/mock_text_cls_data/text_cls/v1/config/han-cls.yml\n```\n\n### Manual installation\n\nFor advanced installation, full version users, or more details, please refer to [manual installation](docs/installation/manual_setup.md).\n\n### Install from Docker\n\nFor Docker users, we provide images with DELTA installed. Please refer to [docker installation](docs/installation/using_docker.md).\n\n\n## Quick Start\n\n### Existing Examples\n\nDELTA organizes many commonly-used tasks as examples in [egs](egs/) directory. \nEach example is a NLP or speech task using a public dataset.\nWe provide the whole pipeline including data processing, model training, evaluation, and deployment.\n\nYou can simply use the `run.sh` under each directory to prepare the dataset, and then train or evaluate a model. \nFor example, you can use the following command to download the CONLL2003 dataset and train and evaluate a BLSTM-CRF model for NER: \n\n```shell\npushd ./egs/conll2003/seq_label/v1/\n./run.sh\npopd\npython3 delta/main.py --cmd train --config egs/conll2003/seq_label/v1/config/seq-label.yml\npython3 delta/main.py --cmd eval --config egs/conll2003/seq_label/v1/config/seq-label.yml\n```\n\n### Modeling\n\nThere are several modes to start a DELTA pipeline:\n\n- train_and_eval\n- train\n- eval\n- infer\n- export_model\n\n**Note**: Before run any command, please make sure you need to `source env.sh` in the current command prompt or a shell script.\n\nYou can use `train_and_eval` to start the model training and evaluation:\n\n```shell\npython3 delta/main.py --cmd train_and_eval --config <your configuration file>.yml\n```\n\nThis is equivalent to:\n\n```shell\npython3 delta/main.py --cmd train --config <your configuration file>.yml \npython3 delta/main.py --cmd eval --config <your configuration file>.yml \n```\n\nFor evaluation, you need to prepare a data file with features and labels. \nIf you only want to do inference with feature only, you can use the `infer` mode:\n\n```shell\npython3 delta/main.py --cmd infer --config <your configuration file>.yml \n```\n\nWhen the training is done, you can export a model `checkpoint` to `SavedModel`:\n\n```shell\npython3 delta/main.py --cmd export_model --config <your configuration file>.yml \n```\n\n### Deployment\n\nFor model deployment, we provide many tools in the DELTA-NN package.\nWe organize the model deployment scripts under `./dpl` directory.\n\n* Put `SavedModel` and configure `model.yaml` into `dpl/model`.\n* Use scripts under `dpl/run.sh` to convert model to other deployment model, and compile libraries.\n* All compiled `tensorflow` libs and `delta-nn` libs are in `dpl/lib`.\n* All things need for deployment are under `dpl/output` dir.\n* Test, benchmark or serve under docker.\n\nFor more information, please see [dpl/README.md](dpl/README.md).\n\n## Benchmarks\n\nIn DELTA, we provide experimental results for each task on public datasets as benchmarks. \nFor each task, we compare our implementation with a similar model chosen from a highly-cited publication.\nYou can reproduce the experimental results using the scripts and configuration in the `./egs` directory. \nFor more details, please refer to [released models](docs/released_models.md).\n\n### NLP tasks\n\n| Task | Model | DataSet | Metric | DELTA | Baseline | Baseline reference |\n|---|---|---|---|---|---|---|\n| Sentence Classification | CNN | TREC | Acc | 92.2 | 91.2 | Kim (2014) |\n| Document Classification | HAN | Yahoo Answer | Acc | 75.1 | 75.8 | Yang et al. (2016) |\n| Named Entity Recognition | BiLSTM-CRF | CoNLL 2003 | F1 | 84.6 | 84.7 | Huang et al. (2015) |\n| Intent Detection (joint) | BiLSTM-CRF-Attention | ATIS | Acc | 97.4 | 98.2| Liu and Lane (2016) |\n| Slots Filling (joint) | BiLSTM-CRF-Attention | ATIS  | F1 | 95.2 | 95.9 | Liu and Lane (2016) |\n| Natural Language Inference | LSTM | SNLI | Acc | 80.7 | 80.6 | Bowman et al. (2016) | \n| Summarization | Seq2seq-LSTM | CNN/Daily Mail | RougeL | 27.3 | 28.1 | See et al. (2017) |\n| Pretrain-NER | ELMO | CoNLL 2003 | F1 | 92.2 | 92.2 | Peters et al. (2018) |\n| Pretrain-NER | BERT | CoNLL 2003 | F1 | 94.6 | 94.9 | Devlin et al. (2019) |\n\n### Speech tasks\n\n| Task | Model | DataSet | Metric | DELTA | Baseline | Baseline reference |\n|---|---|---|---|---|---|---|\n| Speech recognition | CTC | HKUST | CER | 36.49 | 38.67 | Miao et al. (2016) |\n| Speaker verfication | TDNN | VoxCeleb | EER | 3.028 | 3.138 | Kaldi |\n| Emotion recognition | RNN-mean pool | IEMOCAP | Acc | 59.44 | 56.90 | Mirsamadi et al. (2017) |\n\n\n## FAQ\n\nSee [FAQ](docs/faq.md) for more information.\n\n## Contributing\n\nAny contribution is welcome. All issues and pull requests are highly appreciated!\nFor more details, please refer to [the contribution guide](CONTRIBUTING.md).\n\n## References\n\nPlease cite this [paper](https://arxiv.org/abs/1908.01853) when referencing DELTA.\n```bibtex\n@ARTICLE{delta,\n       author = {{Han}, Kun and {Chen}, Junwen and {Zhang}, Hui and {Xu}, Haiyang and\n         {Peng}, Yiping and {Wang}, Yun and {Ding}, Ning and {Deng}, Hui and\n         {Gao}, Yonghu and {Guo}, Tingwei and {Zhang}, Yi and {He}, Yahao and\n         {Ma}, Baochang and {Zhou}, Yulong and {Zhang}, Kangli and {Liu}, Chao and\n         {Lyu}, Ying and {Wang}, Chenxi and {Gong}, Cheng and {Wang}, Yunbo and\n         {Zou}, Wei and {Song}, Hui and {Li}, Xiangang},\n       title = "{DELTA: A DEep learning based Language Technology plAtform}",\n       journal = {arXiv e-prints},\n       year = "2019",\n       url = {https://arxiv.org/abs/1908.01853},\n}\n\n```\n\n## License\n\nThe DELTA platform is licensed under the terms of the Apache license.\nSee [LICENSE](LICENSE) for more information.\n\n## Acknowledgement\n\nThe DELTA platform depends on many open source repos.\nSee [References](docs/references.md) for more information.\n\n'