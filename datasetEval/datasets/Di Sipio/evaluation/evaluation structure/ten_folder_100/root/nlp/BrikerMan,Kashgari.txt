b'<h1 align="center">\n    <a href=\'https://en.wikipedia.org/wiki/Mahmud_al-Kashgari\'>Kashgari</a>\n</h1>\n\n<p align="center">\n    <a href="https://github.com/BrikerMan/kashgari/blob/master/LICENSE">\n        <img alt="GitHub" src="https://img.shields.io/github/license/BrikerMan/kashgari.svg?color=blue&style=popout">\n    </a>\n    <a href="https://travis-ci.com/BrikerMan/Kashgari">\n        <img src="https://travis-ci.com/BrikerMan/Kashgari.svg?branch=master"/>\n    </a>\n    <a href=\'https://coveralls.io/github/BrikerMan/Kashgari?branch=master\'>\n        <img src=\'https://coveralls.io/repos/github/BrikerMan/Kashgari/badge.svg?branch=master\' alt=\'Coverage Status\'/>\n    </a>\n     <a href="https://pepy.tech/project/kashgari">\n        <img src="https://pepy.tech/badge/kashgari"/>\n    </a>\n    <a href="https://pypi.org/project/kashgari/">\n        <img alt="PyPI" src="https://img.shields.io/pypi/v/kashgari.svg">\n    </a>\n</p>\n\n<h4 align="center">\n    <a href="#overview">Overview</a> |\n    <a href="#performance">Performance</a> |\n    <a href="#quick-start">Quick start</a> |\n    <a href="https://kashgari.bmio.net/">Documentation</a> |\n    <a href="https://kashgari-zh.bmio.net/">\xe4\xb8\xad\xe6\x96\x87\xe6\x96\x87\xe6\xa1\xa3</a> |\n    <a href="https://kashgari.bmio.net/about/contributing/">Contributing</a>\n</h4>\n\n\xf0\x9f\x8e\x89\xf0\x9f\x8e\x89\xf0\x9f\x8e\x89 We are proud to announce that we entirely rewrote Kashgari with tf.keras, now Kashgari comes with easier to understand API and is faster! \xf0\x9f\x8e\x89\xf0\x9f\x8e\x89\xf0\x9f\x8e\x89\n\n## Overview\n\nKashgari is a simple and powerful NLP Transfer learning framework, build a state-of-art model in 5 minutes for named entity recognition (NER), part-of-speech tagging (PoS), and text classification tasks.\n\n- **Human-friendly**. Kashgari\'s code is straightforward, well documented and tested, which makes it very easy to understand and modify.\n- **Powerful and simple**. Kashgari allows you to apply state-of-the-art natural language processing (NLP) models to your text, such as named entity recognition (NER), part-of-speech tagging (PoS) and classification.\n- **Built-in transfer learning**. Kashgari built-in pre-trained BERT and Word2vec embedding models, which makes it very simple to transfer learning to train your model.\n- **Fully scalable**. Kashgari provides a simple, fast, and scalable environment for fast experimentation, train your models and experiment with new approaches using different embeddings and model structure. \n- **Production Ready**. Kashgari could export model with `SavedModel` format for tensorflow serving, you could directly deploy it on the cloud. \n\n## Our Goal\n\n- **Academic users** Easier experimentation to prove their hypothesis without coding from scratch.\n- **NLP beginners** Learn how to build an NLP project with production level code quality.\n- **NLP developers** Build a production level classification/labeling model within minutes.\n\n## Performance\n\n| Task                     | Language | Dataset                   | Score          | Detail                                                                                                             |\n| ------------------------ | -------- | ------------------------- | -------------- | ------------------------------------------------------------------------------------------------------------------ |\n| Named Entity Recognition | Chinese  | People\'s Daily Ner Corpus | **94.46** (F1) | [Text Labeling Performance Report](https://kashgari.bmio.net/tutorial/text-labeling/#performance-report) |\n\n## Tutorials\n\nHere is a set of quick tutorials to get you started with the library:\n\n- [Tutorial 1: Text Classification](https://kashgari.bmio.net/tutorial/text-classification/)\n- [Tutorial 2: Text Labeling](https://kashgari.bmio.net/tutorial/text-labeling/)\n- [Tutorial 3: Language Embedding](https://kashgari.bmio.net/embeddings/)\n\nThere are also articles and posts that illustrate how to use Kashgari:\n\n- [15 \xe5\x88\x86\xe9\x92\x9f\xe6\x90\xad\xe5\xbb\xba\xe4\xb8\xad\xe6\x96\x87\xe6\x96\x87\xe6\x9c\xac\xe5\x88\x86\xe7\xb1\xbb\xe6\xa8\xa1\xe5\x9e\x8b](https://eliyar.biz/nlp_chinese_text_classification_in_15mins/)\n- [\xe5\x9f\xba\xe4\xba\x8e BERT \xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe5\x91\xbd\xe5\x90\x8d\xe5\xae\x9e\xe4\xbd\x93\xe8\xaf\x86\xe5\x88\xab\xef\xbc\x88NER)](https://eliyar.biz/nlp_chinese_bert_ner/)\n- [BERT/ERNIE \xe6\x96\x87\xe6\x9c\xac\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe9\x83\xa8\xe7\xbd\xb2](https://eliyar.biz/nlp_train_and_deploy_bert_text_classification/)\n- [\xe4\xba\x94\xe5\x88\x86\xe9\x92\x9f\xe6\x90\xad\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9f\xba\xe4\xba\x8eBERT\xe7\x9a\x84NER\xe6\xa8\xa1\xe5\x9e\x8b](https://www.jianshu.com/p/1d6689851622)\n- [Multi-Class Text Classification with Kashgari in 15 minutes](https://medium.com/@BrikerMan/multi-class-text-classification-with-kashgari-in-15mins-c3e744ce971d)\n\n## Quick start\n\n### Requirements and Installation\n\n\xf0\x9f\x8e\x89\xf0\x9f\x8e\x89\xf0\x9f\x8e\x89 We renamed again for consistency and clarity. From now on, it is all `kashgari`. \xf0\x9f\x8e\x89\xf0\x9f\x8e\x89\xf0\x9f\x8e\x89\n\nThe project is based on Python 3.6+, because it is 2019 and type hinting is cool.\n\n| Backend          | pypi version                           | desc            |\n| ---------------- | -------------------------------------- | --------------- |\n| TensorFlow 2.x   | `pip install \'kashgari>=2.0.0\'`        | coming soon     |\n| TensorFlow 1.14+ | `pip install \'kashgari>=1.0.0,<2.0.0\'` | current version |\n| Keras            | `pip install \'kashgari<1.0.0\'`         | legacy version  |\n\n[Find more info about the name changing.](https://github.com/BrikerMan/Kashgari/releases/tag/v1.0.0)\n\n### Example Usage\n\nLet\'s run an NER labeling model with Bi_LSTM Model.\n\n```python\nfrom kashgari.corpus import ChineseDailyNerCorpus\nfrom kashgari.tasks.labeling import BiLSTM_Model\n\ntrain_x, train_y = ChineseDailyNerCorpus.load_data(\'train\')\ntest_x, test_y = ChineseDailyNerCorpus.load_data(\'test\')\nvalid_x, valid_y = ChineseDailyNerCorpus.load_data(\'valid\')\n\nmodel = BiLSTM_Model()\nmodel.fit(train_x, train_y, valid_x, valid_y, epochs=50)\n\n"""\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ninput (InputLayer)           (None, 97)                0\n_________________________________________________________________\nlayer_embedding (Embedding)  (None, 97, 100)           320600\n_________________________________________________________________\nlayer_blstm (Bidirectional)  (None, 97, 256)           235520\n_________________________________________________________________\nlayer_dropout (Dropout)      (None, 97, 256)           0\n_________________________________________________________________\nlayer_time_distributed (Time (None, 97, 8)             2056\n_________________________________________________________________\nactivation_7 (Activation)    (None, 97, 8)             0\n=================================================================\nTotal params: 558,176\nTrainable params: 558,176\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 20864 samples, validate on 2318 samples\nEpoch 1/50\n20864/20864 [==============================] - 9s 417us/sample - loss: 0.2508 - acc: 0.9333 - val_loss: 0.1240 - val_acc: 0.9607\n\n"""\n```\n\n### Run with GPT-2 Embedding\n\n```python\nfrom kashgari.embeddings import GPT2Embedding\nfrom kashgari.corpus import ChineseDailyNerCorpus\nfrom kashgari.tasks.labeling import BiGRU_Model\n\ntrain_x, train_y = ChineseDailyNerCorpus.load_data(\'train\')\nvalid_x, valid_y = ChineseDailyNerCorpus.load_data(\'valid\')\n\ngpt2_embedding = GPT2Embedding(\'<path-to-gpt-model-folder>\', sequence_length=30)\nmodel = BiGRU_Model(gpt2_embedding)\nmodel.fit(train_x, train_y, valid_x, valid_y, epochs=50)\n```\n\n### Run with Bert Embedding\n\n```python\nfrom kashgari.embeddings import BERTEmbedding\nfrom kashgari.tasks.labeling import BiGRU_Model\nfrom kashgari.corpus import ChineseDailyNerCorpus\n\nbert_embedding = BERTEmbedding(\'<bert-model-folder>\', sequence_length=30)\nmodel = BiGRU_Model(bert_embedding)\n\ntrain_x, train_y = ChineseDailyNerCorpus.load_data()\nmodel.fit(train_x, train_y)\n```\n\n## Sponsors\n\nSupport this project by becoming a sponsor. Your issues and feature request will be prioritized.[[Become a sponsor](https://www.patreon.com/join/brikerman?)]\n\n## Contributing\n\nThanks for your interest in contributing! There are many ways to get involved; start with the [contributor guidelines](https://kashgari.bmio.net/about/contributing/) and then check these open issues for specific tasks.\n\nFeel free to join the WeChat group if you want to more involved in Kashgari\'s development.\n\n![](http://s3.bmio.net/kashgari-qr-code.jpeg)\n\n## Reference\n\nThis library is inspired by and references following frameworks and papers.\n\n- [flair - A very simple framework for state-of-the-art Natural Language Processing (NLP)](https://github.com/zalandoresearch/flair)\n- [anago - Bidirectional LSTM-CRF and ELMo for Named-Entity Recognition, Part-of-Speech Tagging](https://github.com/Hironsan/anago)\n- [Chinese-Word-Vectors](https://github.com/Embedding/Chinese-Word-Vectors)\n'