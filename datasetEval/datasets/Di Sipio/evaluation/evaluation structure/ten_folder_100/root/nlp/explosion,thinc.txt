b'<a href="https://explosion.ai"><img src="https://explosion.ai/assets/img/logo.svg" width="125" height="125" align="right" /></a>\n\n# Thinc: Practical Machine Learning for NLP in Python\n\n**Thinc** is the machine learning library powering [spaCy](https://spacy.io). It\nfeatures a battle-tested linear model designed for large sparse learning\nproblems, and a flexible neural network model under development for\n[spaCy v2.0](https://spacy.io/usage/v2).\n\nThinc is a practical toolkit for implementing models that follow the\n["Embed, encode, attend, predict"](https://explosion.ai/blog/deep-learning-formula-nlp)\narchitecture. It\'s designed to be easy to install, efficient for CPU usage and\noptimised for NLP and deep learning with text \xe2\x80\x93 in particular, hierarchically\nstructured input and variable-length sequences.\n\n\xf0\x9f\x94\xae [Read the release notes here.](https://github.com/explosion/thinc/releases/)\n\n[![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/7/master.svg?logo=azure-pipelines&style=flat-square)](https://dev.azure.com/explosion-ai/public/_build?definitionId=7)\n[![Current Release Version](https://img.shields.io/github/release/explosion/thinc.svg?style=flat-square&logo=github)](https://github.com/explosion/thinc/releases)\n[![PyPi Version](https://img.shields.io/pypi/v/thinc.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.python.org/pypi/thinc)\n[![conda Version](https://img.shields.io/conda/vn/conda-forge/thinc.svg?style=flat-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/thinc)\n[![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://github.com/explosion/wheelwright/releases)\n\n## What\'s where (as of v7.0.0)\n\n| Module                 | Description                                                         |\n| ---------------------- | ------------------------------------------------------------------- |\n| `thinc.v2v.Model`      | Base class.                                                         |\n| `thinc.v2v`            | Layers transforming vectors to vectors.                             |\n| `thinc.i2v`            | Layers embedding IDs to vectors.                                    |\n| `thinc.t2v`            | Layers pooling tensors to vectors.                                  |\n| `thinc.t2t`            | Layers transforming tensors to tensors (e.g. CNN, LSTM).            |\n| `thinc.api`            | Higher-order functions, for building networks. Will be renamed.     |\n| `thinc.extra`          | Datasets and utilities.                                             |\n| `thinc.neural.ops`     | Container classes for mathematical operations. Will be reorganized. |\n| `thinc.linear.avgtron` | Legacy efficient Averaged Perceptron implementation.                |\n\n## Development status\n\nThinc\'s deep learning functionality is still under active development: APIs are\nunstable, and we\'re not yet ready to provide usage support. However, if you\'re\nalready quite familiar with neural networks, there\'s a lot here you might find\ninteresting. Thinc\'s conceptual model is quite different from TensorFlow\'s.\nThinc also implements some novel features, such as a small DSL for concisely\nwiring up models, embedding tables that support pre-computation and the hashing\ntrick, dynamic batch sizes, a concatenation-based approach to variable-length\nsequences, and support for model averaging for the Adam solver (which performs\nvery well).\n\n## No computational graph \xe2\x80\x93 just higher order functions\n\nThe central problem for a neural network implementation is this: during the\nforward pass, you compute results that will later be useful during the backward\npass. How do you keep track of this arbitrary state, while making sure that\nlayers can be cleanly composed?\n\nMost libraries solve this problem by having you declare the forward\ncomputations, which are then compiled into a graph somewhere behind the scenes.\nThinc doesn\'t have a "computational graph". Instead, we just use the stack,\nbecause we put the state from the forward pass into callbacks.\n\nAll nodes in the network have a simple signature:\n\n```\nf(inputs) -> {outputs, f(d_outputs)->d_inputs}\n```\n\nTo make this less abstract, here\'s a ReLu activation, following this signature:\n\n```python\ndef relu(inputs):\n    mask = inputs > 0\n    def backprop_relu(d_outputs, optimizer):\n        return d_outputs * mask\n    return inputs * mask, backprop_relu\n```\n\nWhen you call the `relu` function, you get back an output variable, and a\ncallback. This lets you calculate a gradient using the output, and then pass it\ninto the callback to perform the backward pass.\n\nThis signature makes it easy to build a complex network out of smaller pieces,\nusing arbitrary higher-order functions you can write yourself. To make this\nclearer, we need a function for a weights layer. Usually this will be\nimplemented as a class \xe2\x80\x94 but let\'s continue using closures, to keep things\nconcise, and to keep the simplicity of the interface explicit.\n\nThe main complication for the weights layer is that we now have a side-effect to\nmanage: we would like to update the weights. There are a few ways to handle\nthis. In Thinc we currently pass a callable into the backward pass. (I\'m not\nconvinced this is best.)\n\n```python\nimport numpy\n\ndef create_linear_layer(n_out, n_in):\n    W = numpy.zeros((n_out, n_in))\n    b = numpy.zeros((n_out, 1))\n\n    def forward(X):\n        Y = W @ X + b\n        def backward(dY, optimizer):\n            dX = W.T @ dY\n            dW = numpy.einsum(\'ik,jk->ij\', dY, X)\n            db = dY.sum(axis=0)\n\n            optimizer(W, dW)\n            optimizer(b, db)\n\n            return dX\n        return Y, backward\n    return forward\n```\n\nIf we call `Wb = create_linear_layer(5, 4)`, the variable `Wb` will be the\n`forward()` function, implemented inside the body of `create_linear_layer()`.\nThe `Wb` instance will have access to the `W` and `b` variable defined in its\nouter scope. If we invoke `create_linear_layer()` again, we get a new instance,\nwith its own internal state.\n\nThe `Wb` instance and the `relu` function have exactly the same signature. This\nmakes it easy to write higher order functions to compose them. The most obvious\nthing to do is chain them together:\n\n```python\ndef chain(*layers):\n    def forward(X):\n        backprops = []\n        Y = X\n        for layer in layers:\n            Y, backprop = layer(Y)\n            backprops.append(backprop)\n        def backward(dY, optimizer):\n            for backprop in reversed(backprops):\n                dY = backprop(dY, optimizer)\n            return dY\n        return Y, backward\n    return forward\n```\n\nWe could now chain our linear layer together with the `relu` activation, to\ncreate a simple feed-forward network:\n\n```python\nWb1 = create_linear_layer(10, 5)\nWb2 = create_linear_layer(3, 10)\n\nmodel = chain(Wb1, relu, Wb2)\n\nX = numpy.random.uniform(size=(5, 4))\n\ny, bp_y = model(X)\n\ndY = y - truth\ndX = bp_y(dY, optimizer)\n```\n\nThis conceptual model makes Thinc very flexible. The trade-off is that Thinc is\nless convenient and efficient at workloads that fit exactly into what\n[TensorFlow](https://www.tensorflow.org/) etc. are designed for. If your graph\nreally is static, and your inputs are homogenous in size and shape,\n[Keras](https://keras.io/) will likely be faster and simpler. But if you want to\npass normal Python objects through your network, or handle sequences and\nrecursions of arbitrary length or complexity, you might find Thinc\'s design a\nbetter fit for your problem.\n\n## Quickstart\n\nThinc should install cleanly with both [pip](http://pypi.python.org/pypi/thinc)\nand [conda](https://anaconda.org/conda-forge/thinc), for **Pythons 2.7+ and\n3.5+**, on **Linux**, **macOS / OSX** and **Windows**. Its only system\ndependency is a compiler tool-chain (e.g. `build-essential`) and the Python\ndevelopment headers (e.g. `python-dev`).\n\n```bash\npip install thinc\n```\n\nFor GPU support, we\'re grateful to use the work of Chainer\'s `cupy` module,\nwhich provides a numpy-compatible interface for GPU arrays. However, installing\nChainer when no GPU is available currently causes an error. We therefore do not\nlist Chainer as an explicit dependency \xe2\x80\x94 so building Thinc for GPU requires some\nextra steps:\n\n```bash\nexport CUDA_HOME=/usr/local/cuda-8.0 # Or wherever your CUDA is\nexport PATH=$PATH:$CUDA_HOME/bin\npip install chainer\npython -c "import cupy; assert cupy" # Check it installed\npip install thinc_gpu_ops thinc # Or `thinc[cuda]`\npython -c "import thinc_gpu_ops" # Check the GPU ops were built\n```\n\nThe rest of this section describes how to build Thinc from source. If you have\n[Fabric](http://www.fabfile.org) installed, you can use the shortcut:\n\n```bash\ngit clone https://github.com/explosion/thinc\ncd thinc\nfab clean env make test\n```\n\nYou can then run the examples as follows:\n\n```bash\nfab eg.mnist\nfab eg.basic_tagger\nfab eg.cnn_tagger\n```\n\nOtherwise, you can build and test explicitly with:\n\n```bash\ngit clone https://github.com/explosion/thinc\ncd thinc\n\nvirtualenv .env\nsource .env/bin/activate\n\npip install -r requirements.txt\npython setup.py build_ext --inplace\npy.test thinc/\n```\n\nAnd then run the examples as follows:\n\n```bash\npython examples/mnist.py\npython examples/basic_tagger.py\npython examples/cnn_tagger.py\n```\n\n## Usage\n\nThe Neural Network API is still subject to change, even within minor versions.\nYou can get a feel for the current API by checking out the examples. Here are a\nfew quick highlights.\n\n### 1. Shape inference\n\nModels can be created with some dimensions unspecified. Missing dimensions are\ninferred when pre-trained weights are loaded or when training begins. This\neliminates a common source of programmer error:\n\n```python\n# Invalid network \xe2\x80\x94 shape mismatch\nmodel = chain(ReLu(512, 748), ReLu(512, 784), Softmax(10))\n\n# Leave the dimensions unspecified, and you can\'t be wrong.\nmodel = chain(ReLu(512), ReLu(512), Softmax())\n```\n\n### 2. Operator overloading\n\nThe `Model.define_operators()` classmethod allows you to bind arbitrary binary\nfunctions to Python operators, for use in any `Model` instance. The method can\n(and should) be used as a context-manager, so that the overloading is limited to\nthe immediate block. This allows concise and expressive model definition:\n\n```python\nwith Model.define_operators({\'>>\': chain}):\n    model = ReLu(512) >> ReLu(512) >> Softmax()\n```\n\nThe overloading is cleaned up at the end of the block. A fairly arbitrary zoo of\nfunctions are currently implemented. Some of the most useful:\n\n-   `chain(model1, model2)`: Compose two models `f(x)` and `g(x)` into a single\n    model computing `g(f(x))`.\n-   `clone(model1, int)`: Create `n` copies of a model, each with distinct\n    weights, and chain them together.\n-   `concatenate(model1, model2)`: Given two models with output dimensions\n    `(n,)` and `(m,)`, construct a model with output dimensions `(m+n,)`.\n-   `add(model1, model2)`: `add(f(x), g(x)) = f(x)+g(x)`\n-   `make_tuple(model1, model2)`: Construct tuples of the outputs of two models,\n    at the batch level. The backward pass expects to receive a tuple of\n    gradients, which are routed through the appropriate model, and summed.\n\nPutting these things together, here\'s the sort of tagging model that Thinc is\ndesigned to make easy.\n\n```python\nwith Model.define_operators({\'>>\': chain, \'**\': clone, \'|\': concatenate}):\n    model = (\n        add_eol_markers(\'EOL\')\n        >> flatten\n        >> memoize(\n            CharLSTM(char_width)\n            | (normalize >> str2int >> Embed(word_width)))\n        >> ExtractWindow(nW=2)\n        >> BatchNorm(ReLu(hidden_width)) ** 3\n        >> Softmax()\n    )\n```\n\nNot all of these pieces are implemented yet, but hopefully this shows where\nwe\'re going. The `memoize` function will be particularly important: in any batch\nof text, the common words will be very common. It\'s therefore important to\nevaluate models such as the `CharLSTM` once per word type per minibatch, rather\nthan once per token.\n\n### 3. Callback-based backpropagation\n\nMost neural network libraries use a computational graph abstraction. This takes\nthe execution away from you, so that gradients can be computed automatically.\nThinc follows a style more like the `autograd` library, but with larger\noperations. Usage is as follows:\n\n```python\ndef explicit_sgd_update(X, y):\n    sgd = lambda weights, gradient: weights - gradient * 0.001\n    yh, finish_update = model.begin_update(X, drop=0.2)\n    finish_update(y-yh, sgd)\n```\n\nSeparating the backpropagation into three parts like this has many advantages.\nThe interface to all models is completely uniform \xe2\x80\x94 there is no distinction\nbetween the top-level model you use as a predictor and the internal models for\nthe layers. We also make concurrency simple, by making the `begin_update()` step\na pure function, and separating the accumulation of the gradient from the action\nof the optimizer.\n\n### 4. Class annotations\n\nTo keep the class hierarchy shallow, Thinc uses class decorators to reuse code\nfor layer definitions. Specifically, the following decorators are available:\n\n-   `describe.attributes()`: Allows attributes to be specified by keyword\n    argument. Used especially for dimensions and parameters.\n-   `describe.on_init()`: Allows callbacks to be specified, which will be called\n    at the end of the `__init__.py`.\n-   `describe.on_data()`: Allows callbacks to be specified, which will be called\n    on `Model.begin_training()`.\n\n## \xf0\x9f\x9b\xa0 Changelog\n\n| Version   | Date         | Description                                                               |\n| --------- | ------------ | ------------------------------------------------------------------------- |\n| [v7.3.1]  | `2019-10-30` | Relax dependecy requirements                                              |\n| [v7.3.0]  | `2019-10-28` | Mish activation and experimental optimizers                               |\n| [v7.2.0]  | `2019-10-20` | Simpler GPU install and bug fixes                                         |\n| [v7.1.1]  | `2019-09-10` | Support `preshed` v3.0.0                                                  |\n| [v7.1.0]  | `2019-08-23` | Support other CPUs, read-only arrays                                      |\n| [v7.0.8]  | `2019-07-11` | Fix version for PyPi                                                      |\n| [v7.0.7]  | `2019-07-11` | Avoid allocating a negative shape for ngrams                              |\n| [v7.0.6]  | `2019-07-11` | Fix `LinearModel` regression                                              |\n| [v7.0.5]  | `2019-07-10` | Bug fixes for pickle, threading, unflatten and consistency                |\n| [v7.0.4]  | `2019-03-19` | Don\'t require `thinc_gpu_ops`                                             |\n| [v7.0.3]  | `2019-03-15` | Fix pruning in beam search                                                |\n| [v7.0.2]  | `2019-02-23` | Fix regression in linear model class                                      |\n| [v7.0.1]  | `2019-02-16` | Fix import errors                                                         |\n| [v7.0.0]  | `2019-02-15` | Overhaul package dependencies                                             |\n| [v6.12.1] | `2018-11-30` | Fix `msgpack` pin                                                         |\n| [v6.12.0] | `2018-10-15` | Wheels and separate GPU ops                                               |\n| [v6.10.3] | `2018-07-21` | Python 3.7 support and dependency updates                                 |\n| [v6.11.2] | `2018-05-21` | Improve GPU installation                                                  |\n| [v6.11.1] | `2018-05-20` | Support direct linkage to BLAS libraries                                  |\n| v6.11.0   | `2018-03-16` | _n/a_                                                                     |\n| [v6.10.2] | `2017-12-06` | Efficiency improvements and bug fixes                                     |\n| [v6.10.1] | `2017-11-15` | Fix GPU install and minor memory leak                                     |\n| [v6.10.0] | `2017-10-28` | CPU efficiency improvements, refactoring                                  |\n| [v6.9.0]  | `2017-10-03` | Reorganize layers, bug fix to Layer Normalization                         |\n| [v6.8.2]  | `2017-09-26` | Fix packaging of `gpu_ops`                                                |\n| [v6.8.1]  | `2017-08-23` | Fix Windows support                                                       |\n| [v6.8.0]  | `2017-07-25` | SELU layer, attention, improved GPU/CPU compatibility                     |\n| [v6.7.3]  | `2017-06-05` | Fix convolution on GPU                                                    |\n| [v6.7.2]  | `2017-06-02` | Bug fixes to serialization                                                |\n| [v6.7.1]  | `2017-06-02` | Improve serialization                                                     |\n| [v6.7.0]  | `2017-06-01` | Fixes to serialization, hash embeddings and flatten ops                   |\n| [v6.6.0]  | `2017-05-14` | Improved GPU usage and examples                                           |\n| v6.5.2    | `2017-03-20` | _n/a_                                                                     |\n| [v6.5.1]  | `2017-03-20` | Improved linear class and Windows fix                                     |\n| [v6.5.0]  | `2017-03-11` | Supervised similarity, fancier embedding and improvements to linear model |\n| v6.4.0    | `2017-02-15` | _n/a_                                                                     |\n| [v6.3.0]  | `2017-01-25` | Efficiency improvements, argument checking and error messaging            |\n| [v6.2.0]  | `2017-01-15` | Improve API and introduce overloaded operators                            |\n| [v6.1.3]  | `2017-01-10` | More neural network functions and training continuation                   |\n| v6.1.2    | `2017-01-09` | _n/a_                                                                     |\n| v6.1.1    | `2017-01-09` | _n/a_                                                                     |\n| v6.1.0    | `2017-01-09` | _n/a_                                                                     |\n| [v6.0.0]  | `2016-12-31` | Add `thinc.neural` for NLP-oriented deep learning                         |\n\n[v7.3.1]: https://github.com/explosion/thinc/releases/tag/v7.3.1\n[v7.3.0]: https://github.com/explosion/thinc/releases/tag/v7.3.0\n[v7.2.0]: https://github.com/explosion/thinc/releases/tag/v7.2.0\n[v7.1.1]: https://github.com/explosion/thinc/releases/tag/v7.1.1\n[v7.1.0]: https://github.com/explosion/thinc/releases/tag/v7.1.0\n[v7.0.8]: https://github.com/explosion/thinc/releases/tag/v7.0.8\n[v7.0.7]: https://github.com/explosion/thinc/releases/tag/v7.0.7\n[v7.0.6]: https://github.com/explosion/thinc/releases/tag/v7.0.6\n[v7.0.5]: https://github.com/explosion/thinc/releases/tag/v7.0.5\n[v7.0.4]: https://github.com/explosion/thinc/releases/tag/v7.0.4\n[v7.0.3]: https://github.com/explosion/thinc/releases/tag/v7.0.3\n[v7.0.2]: https://github.com/explosion/thinc/releases/tag/v7.0.2\n[v7.0.1]: https://github.com/explosion/thinc/releases/tag/v7.0.1\n[v7.0.0]: https://github.com/explosion/thinc/releases/tag/v7.0.0\n[v6.12.1]: https://github.com/explosion/thinc/releases/tag/v6.12.1\n[v6.12.0]: https://github.com/explosion/thinc/releases/tag/v6.12.0\n[v6.11.2]: https://github.com/explosion/thinc/releases/tag/v6.11.2\n[v6.11.1]: https://github.com/explosion/thinc/releases/tag/v6.11.1\n[v6.10.3]: https://github.com/explosion/thinc/releases/tag/v6.10.3\n[v6.10.2]: https://github.com/explosion/thinc/releases/tag/v6.10.2\n[v6.10.1]: https://github.com/explosion/thinc/releases/tag/v6.10.1\n[v6.10.0]: https://github.com/explosion/thinc/releases/tag/v6.10.0\n[v6.9.0]: https://github.com/explosion/thinc/releases/tag/v6.9.0\n[v6.8.2]: https://github.com/explosion/thinc/releases/tag/v6.8.2\n[v6.8.1]: https://github.com/explosion/thinc/releases/tag/v6.8.1\n[v6.8.0]: https://github.com/explosion/thinc/releases/tag/v6.8.0\n[v6.7.3]: https://github.com/explosion/thinc/releases/tag/v6.7.3\n[v6.7.2]: https://github.com/explosion/thinc/releases/tag/v6.7.2\n[v6.7.1]: https://github.com/explosion/thinc/releases/tag/v6.7.1\n[v6.7.0]: https://github.com/explosion/thinc/releases/tag/v6.7.0\n[v6.6.0]: https://github.com/explosion/thinc/releases/tag/v6.6.0\n[v6.5.1]: https://github.com/explosion/thinc/releases/tag/v6.5.1\n[v6.5.0]: https://github.com/explosion/thinc/releases/tag/v6.5.0\n[v6.3.0]: https://github.com/explosion/thinc/releases/tag/v6.3.0\n[v6.2.0]: https://github.com/explosion/thinc/releases/tag/v6.2.0\n[v6.1.3]: https://github.com/explosion/thinc/releases/tag/v6.1.3\n[v6.0.0]: https://github.com/explosion/thinc/releases/tag/v6.0.0\n'