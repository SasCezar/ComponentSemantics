b'#  This repository is to collect BERT related resources. \n\nAD:  a repository for graph convolutional networks at https://github.com/Jiakui/awesome-gcn (resources for graph convolutional networks  \xef\xbc\x88\xe5\x9b\xbe\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9b\xb8\xe5\x85\xb3\xe8\xb5\x84\xe6\xba\x90\xef\xbc\x89). \n\n\n# Papers: \n\n1. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805), BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n, Authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n\n<details>\n\n<summary><b> Click to see more </b></summary>\n\n2. [arXiv:1812.06705](https://arxiv.org/abs/1812.06705), Conditional BERT Contextual Augmentation, Authors: Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, Songlin Hu\n\n3. [arXiv:1812.03593](https://arxiv.org/pdf/1812.03593), SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering, Authors: Chenguang Zhu, Michael Zeng, Xuedong Huang\n\n4. [arXiv:1901.02860](https://arxiv.org/abs/1901.02860), Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,  Authors: Zihang Dai, Zhilin Yang, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V. Le and Ruslan Salakhutdinov.\n\n5. [arXiv:1901.04085](https://arxiv.org/pdf/1901.04085.pdf), Passage Re-ranking with BERT, Authors: Rodrigo Nogueira, Kyunghyun Cho\n\n6. [arXiv:1902.02671](https://arxiv.org/pdf/1902.02671.pdf), BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning, Authors: Asa Cooper Stickland, Iain Murray\n\n7. [arXiv:1904.02232](https://arxiv.org/abs/1904.02232), BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis, Authors: Hu Xu, Bing Liu, Lei Shu, Philip S. Yu, [[code](https://github.com/howardhsu/BERT-for-RRC-ABSA)]\n\n</details>\n\n\n\n\n# Github Repositories: \n\n## official implement:\n\n1.  [google-research/bert](https://github.com/google-research/bert),  **officical** TensorFlow code and pre-trained models for BERT ,\n![](https://img.shields.io/github/stars/google-research/bert.svg)\n\n\n## implement of BERT besides tensorflow: \n\n\n\n\n1. [codertimo/BERT-pytorch](https://github.com/codertimo/BERT-pytorch),   Google AI 2018 BERT pytorch implementation,\n![](https://img.shields.io/github/stars/codertimo/BERT-pytorch.svg)\n\n2. [huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT),   A PyTorch implementation of Google AI\'s BERT model with script to load Google\'s pre-trained models,\n![](https://img.shields.io/github/stars/huggingface/pytorch-pretrained-BERT.svg)\n\n\n\n3. [dmlc/gluon-nlp](https://github.com/dmlc/gluon-nlp), Gluon + MXNet implementation that reproduces BERT pretraining and finetuning on GLUE benchmark, SQuAD, etc,\n![](https://img.shields.io/github/stars/dmlc/gluon-nlp.svg)\n\n4. [dbiir/UER-py](https://github.com/dbiir/UER-py),  UER-py is a toolkit for pre-training on general-domain corpus and fine-tuning on downstream task. UER-py maintains model modularity and supports research extensibility. It facilitates the use of different pre-training models (e.g. BERT), and provides interfaces for users to further extend upon.  \n![](https://img.shields.io/github/stars/dbiir/UER-py.svg)\n\n\n5. [BrikerMan/Kashgari](https://github.com/BrikerMan/Kashgari), Simple, Keras-powered multilingual NLP framework, allows you to build your models in 5 minutes for named entity recognition (NER), part-of-speech tagging (PoS) and text classification tasks. Includes BERT, GPT-2 and word2vec embedding. \n![](https://img.shields.io/github/stars/BrikerMan/Kashgari.svg)\n\n6. [kaushaltrivedi/fast-bert](https://github.com/kaushaltrivedi/fast-bert), Super easy library for BERT based NLP models, \n![](https://img.shields.io/github/stars/kaushaltrivedi/fast-bert.svg)\n\n\n\n\n<details>\n \n<summary><b> Click to see more </b></summary>\n\n\n7. [Separius/BERT-keras](https://github.com/Separius/BERT-keras), Keras implementation of BERT with pre-trained weights, \n![](https://img.shields.io/github/stars/Separius/BERT-keras.svg)\n\n8. [soskek/bert-chainer](https://github.com/soskek/bert-chainer),  Chainer implementation of "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",\n![](https://img.shields.io/github/stars/soskek/bert-chainer.svg)\n\n9. [innodatalabs/tbert](https://github.com/innodatalabs/tbert), PyTorch port of BERT ML model\n![](https://img.shields.io/github/stars/innodatalabs/tbert.svg)\n\n10. [guotong1988/BERT-tensorflow](https://github.com/guotong1988/BERT-tensorflow), BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n![](https://img.shields.io/github/stars/guotong1988/BERT-tensorflow.svg)\n\n11. [dreamgonfly/BERT-pytorch](https://github.com/dreamgonfly/BERT-pytorch), \nPyTorch implementation of BERT in "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" \n![](https://img.shields.io/github/stars/dreamgonfly/BERT-pytorch.svg)\n\n12. [CyberZHG/keras-bert](https://github.com/CyberZHG/keras-bert), Implementation of BERT that could load official pre-trained models for feature extraction and prediction\n![](https://img.shields.io/github/stars/CyberZHG/keras-bert.svg)\n\n13. [soskek/bert-chainer](https://github.com/soskek/bert-chainer), Chainer implementation of "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"\n![](https://img.shields.io/github/stars/soskek/bert-chainer.svg)\n\n14. [MaZhiyuanBUAA/bert-tf1.4.0](https://github.com/MaZhiyuanBUAA/bert-tf1.4.0), bert-tf1.4.0\n![](https://img.shields.io/github/stars/MaZhiyuanBUAA/bert-tf1.4.0.svg)\n\n15. [dhlee347/pytorchic-bert](https://github.com/dhlee347/pytorchic-bert), Pytorch Implementation of Google BERT,\n![](https://img.shields.io/github/stars/dhlee347/pytorchic-bert.svg)\n\n16. [kpot/keras-transformer](https://github.com/kpot/keras-transformer), Keras library for building (Universal) Transformers, facilitating BERT and GPT models,\n![](https://img.shields.io/github/stars/kpot/keras-transformer.svg)\n\n17. [miroozyx/BERT_with_keras](https://github.com/miroozyx/BERT_with_keras), A Keras version of Google\'s BERT model,\n![](https://img.shields.io/github/stars/miroozyx/BERT_with_keras.svg)\n\n18. [conda-forge/pytorch-pretrained-bert-feedstock](https://github.com/conda-forge/pytorch-pretrained-bert-feedstock), A conda-smithy repository for pytorch-pretrained-bert. ,\n![](https://img.shields.io/github/stars/conda-forge/pytorch-pretrained-bert-feedstock.svg)\n\n\n19. [Rshcaroline/BERT_Pytorch_fastNLP](https://github.com/Rshcaroline/BERT_Pytorch_fastNLP), A PyTorch & fastNLP implementation of Google AI\'s BERT model.\n![](https://img.shields.io/github/stars/Rshcaroline/BERT_Pytorch_fastNLP.svg)\n\n20. [nghuyong/ERNIE-Pytorch](https://github.com/nghuyong/ERNIE-Pytorch), ERNIE Pytorch Version,\n![](https://img.shields.io/github/stars/nghuyong/ERNIE-Pytorch.svg)\n\n</details>\n\n\n\n##  Pretrained BERT weights: \n1. [brightmart/roberta_zh](https://github.com/brightmart/roberta_zh), RoBERTa for Chinese, \xe4\xb8\xad\xe6\x96\x87\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83RoBERTa\xe6\xa8\xa1\xe5\x9e\x8b, \n![](https://img.shields.io/github/stars/brightmart/roberta_zh.svg)\n\n2. [ymcui/Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm), Pre-Training with Whole Word Masking for Chinese BERT\xef\xbc\x88\xe4\xb8\xad\xe6\x96\x87BERT-wwm\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x89 https://arxiv.org/abs/1906.08101, \n![](https://img.shields.io/github/stars/ymcui/Chinese-BERT-wwm.svg)\n\n3. [thunlp/OpenCLaP](https://github.com/thunlp/OpenCLaP),Open Chinese Language Pre-trained Model Zoo, OpenCLaP\xef\xbc\x9a\xe5\xa4\x9a\xe9\xa2\x86\xe5\x9f\x9f\xe5\xbc\x80\xe6\xba\x90\xe4\xb8\xad\xe6\x96\x87\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe8\xaf\xad\xe8\xa8\x80\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbb\x93\xe5\xba\x93, \n![](https://img.shields.io/github/stars/thunlp/OpenCLaP.svg)\n\n4. [ymcui/Chinese-PreTrained-XLNet](https://github.com/ymcui/Chinese-PreTrained-XLNet), Pre-Trained Chinese XLNet\xef\xbc\x88\xe4\xb8\xad\xe6\x96\x87XLNet\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x89, \n![](https://img.shields.io/github/stars/ymcui/Chinese-PreTrained-XLNet.svg)\n\n5. [brightmart/xlnet_zh](https://github.com/brightmart/xlnet_zh), \xe4\xb8\xad\xe6\x96\x87\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83XLNet\xe6\xa8\xa1\xe5\x9e\x8b: Pre-Trained Chinese XLNet_Large, \n![](https://img.shields.io/github/stars/brightmart/xlnet_zh.svg)\n\n\n## improvement over BERT:\n1. [thunlp/ERNIE](https://github.com/https://github.com/thunlp/ERNIE), Source code and dataset for ACL 2019 paper "ERNIE: Enhanced Language Representation with Informative Entities", imporove bert with heterogeneous information fusion. \n![](https://img.shields.io/github/stars/thunlp/ERNIE.svg)\n\n2. [PaddlePaddle/LARK](https://github.com/PaddlePaddle/LARK),  LAnguage Representations Kit, PaddlePaddle implementation of BERT. It also contains an improved version of BERT, ERNIE, for chinese NLP tasks.  BERT \xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe6\x94\xb9\xe8\xbf\x9b\xe7\x89\x88 ERNIE, \n![](https://img.shields.io/github/stars/PaddlePaddle/LARK.svg)\n \n3. [zihangdai/xlnet](https://github.com/zihangdai/xlnet), XLNet: Generalized Autoregressive Pretraining for Language Understanding, \n![](https://img.shields.io/github/stars/zihangdai/xlnet.svg)\n\n4. [kimiyoung/transformer-xl](https://github.com/kimiyoung/transformer-xl), Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, This repository contains the code in both PyTorch and TensorFlow for our paper. \n![](https://img.shields.io/github/stars/kimiyoung/transformer-xl.svg)\n\n5. [GaoPeng97/transformer-xl-chinese](https://github.com/GaoPeng97/transformer-xl-chinese), transformer xl\xe5\x9c\xa8\xe4\xb8\xad\xe6\x96\x87\xe6\x96\x87\xe6\x9c\xac\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x8a\xe7\x9a\x84\xe5\xb0\x9d\xe8\xaf\x95\xe3\x80\x82\xef\xbc\x88transformer xl for text generation of chinese\xef\xbc\x89, \n![](https://img.shields.io/github/stars/GaoPeng97/transformer-xl-chinese.svg)\n\n6. [PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE),  An Implementation of ERNIE For Language Understanding (including Pre-training models and Fine-tuning tools)  BERT \xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe6\x94\xb9\xe8\xbf\x9b\xe7\x89\x88 ERNIE, \n![](https://img.shields.io/github/stars/PaddlePaddle/ERNIE.svg)\n\n7. [pytorch/fairseq](https://github.com/pytorch/fairseq),  Facebook AI Research Sequence-to-Sequence Toolkit written in Python. RoBERTa: A Robustly Optimized BERT Pretraining Approach, \n![](https://img.shields.io/github/stars/pytorch/fairseq.svg)\n\n\n8. [facebookresearch/SpanBERT](https://github.com/facebookresearch/SpanBERT), Code for using and evaluating SpanBERT.\n, This repository contains code and models for the paper: SpanBERT: Improving Pre-training by Representing and Predicting Spans., \n![](https://img.shields.io/github/stars/facebookresearch/SpanBERT.svg)\n\n9. [brightmart/albert_zh](https://github.com/brightmart/albert_zh), \xe6\xb5\xb7\xe9\x87\x8f\xe4\xb8\xad\xe6\x96\x87\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83ALBERT\xe6\xa8\xa1\xe5\x9e\x8b, A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS https://arxiv.org/pdf/1909.11942.pdf, \n![](https://img.shields.io/github/stars/brightmart/albert_zh.svg)\n\n10. [lonePatient/albert_pytorch](https://github.com/lonePatient/albert_pytorch), A Lite Bert For Self-Supervised Learning Language Representations, \n![](https://img.shields.io/github/stars/lonePatient/albert_pytorch.svg)\n\n\n11. [kpe/bert-for-tf2](https://github.com/kpe/bert-for-tf2), A Keras TensorFlow 2.0 implementation of BERT, ALBERT and adapter-BERT. https://github.com/kpe/bert-for-tf2, \n![](https://img.shields.io/github/stars/kpe/bert-for-tf2.svg)\n\n\n\n## other resources for BERT: \n\n1. [brightmart/bert_language_understanding](https://github.com/brightmart/bert_language_understanding), Pre-training of Deep Bidirectional Transformers for Language Understanding: pre-train TextCNN,\n![](https://img.shields.io/github/stars/brightmart/bert_language_understanding.svg)\n\n2. [Y1ran/NLP-BERT--ChineseVersion](https://github.com/Y1ran/NLP-BERT--ChineseVersion), \xe8\xb0\xb7\xe6\xad\x8c\xe8\x87\xaa\xe7\x84\xb6\xe8\xaf\xad\xe8\xa8\x80\xe5\xa4\x84\xe7\x90\x86\xe6\xa8\xa1\xe5\x9e\x8bBERT\xef\xbc\x9a\xe8\xae\xba\xe6\x96\x87\xe8\xa7\xa3\xe6\x9e\x90\xe4\xb8\x8epython\xe4\xbb\xa3\xe7\xa0\x81,\n![](https://img.shields.io/github/stars/Y1ran/NLP-BERT--ChineseVersion.svg)\n\n<details>\n \n<summary><b> Click to see more </b></summary>\n\n\n3. [yangbisheng2009/cn-bert](https://github.com/yangbisheng2009/cn-bert), BERT\xe5\x9c\xa8\xe4\xb8\xad\xe6\x96\x87NLP\xe7\x9a\x84\xe5\xba\x94\xe7\x94\xa8, \xe8\xaf\xad\xe6\xb3\x95\xe6\xa3\x80\xe6\x9f\xa5\n![](https://img.shields.io/github/stars/yangbisheng2009/cn-bert.svg)\n\n4. [JayYip/bert-multiple-gpu](https://github.com/JayYip/bert-multiple-gpu), A multiple GPU support version of BERT,\n![](https://img.shields.io/github/stars/JayYip/bert-multiple-gpu.svg)\n\n5. [HighCWu/keras-bert-tpu](https://github.com/HighCWu/keras-bert-tpu), Implementation of BERT that could load official pre-trained models for feature extraction and prediction on TPU, \n![](https://img.shields.io/github/stars/HighCWu/keras-bert-tpu.svg)\n\n6. [Willyoung2017/Bert_Attempt](https://github.com/Willyoung2017/Bert_Attempt), PyTorch Pretrained Bert,\n![](https://img.shields.io/github/stars/Willyoung2017/Bert_Attempt.svg)\n\n7. [Pydataman/bert_examples](https://github.com/Pydataman/bert_examples), some examples of bert, run_classifier.py \xe6\x98\xaf\xe5\x9f\xba\xe4\xba\x8e\xe8\xb0\xb7\xe6\xad\x8cbert\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86Quora Insincere Questions Classification\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe6\xaf\x94\xe8\xb5\x9b\xe3\x80\x82run_ner.py\xe6\x98\xaf\xe5\x9f\xba\xe4\xba\x8e\xe7\x91\x9e\xe9\x87\x91\xe5\x8c\xbb\xe9\x99\xa2AI\xe5\xa4\xa7\xe8\xb5\x9b \xe7\xac\xac\xe4\xb8\x80\xe8\xb5\x9b\xe5\xad\xa3\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8cbert\xe5\x86\x99\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\x91\xbd\xe5\x90\x8d\xe5\xae\x9e\xe4\xbd\x93\xe8\xaf\x86\xe5\x88\xab\xe3\x80\x82\n![](https://img.shields.io/github/stars/Pydataman/bert_examples.svg)\n\n8. [guotong1988/BERT-chinese](https://github.com/guotong1988/BERT-chinese), BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \xe4\xb8\xad\xe6\x96\x87 \xe6\xb1\x89\xe8\xaf\xad\n![](https://img.shields.io/github/stars/guotong1988/BERT-chinese.svg)\n\n9. [zhongyunuestc/bert_multitask](https://github.com/zhongyunuestc/bert_multitask), \xe5\xa4\x9a\xe4\xbb\xbb\xe5\x8a\xa1task\n![](https://img.shields.io/github/stars/zhongyunuestc/bert_multitask.svg)\n\n10. [Microsoft/AzureML-BERT](https://github.com/Microsoft/AzureML-BERT), End-to-end walk through for fine-tuning BERT using Azure Machine Learning , \n![](https://img.shields.io/github/stars/Microsoft/AzureML-BERT.svg)\n\n11. [bigboNed3/bert_serving](https://github.com/bigboNed3/bert_serving), export bert model for serving, \n![](https://img.shields.io/github/stars/nghuyong/ERNIE-Pytorch.svg)\n\n12. [yoheikikuta/bert-japanese](https://github.com/yoheikikuta/bert-japanese), BERT with SentencePiece for Japanese text. \n![](https://img.shields.io/github/stars/bigboNed3/bert_serving.svg)\n\n13. [whqwill/seq2seq-keyphrase-bert](https://github.com/whqwill/seq2seq-keyphrase-bert), add BERT to encoder part for https://github.com/memray/seq2seq-keyphrase-pytorch,\n![](https://img.shields.io/github/stars/whqwill/seq2seq-keyphrase-bert.svg)\n\n14. [algteam/bert-examples](https://github.com/algteam/bert-examples), bert-demo, \n![](https://img.shields.io/github/stars/algteam/bert-examples.svg)\n\n15. [cedrickchee/awesome-bert-nlp](https://github.com/cedrickchee/awesome-bert-nlp), A curated list of NLP resources focused on BERT, attention mechanism, Transformer networks, and transfer learning. \n![](https://img.shields.io/github/stars/cedrickchee/awesome-bert-nlp.svg)\n\n16. [cnfive/cnbert](https://github.com/cnfive/cnbert), \xe4\xb8\xad\xe6\x96\x87\xe6\xb3\xa8\xe9\x87\x8a\xe4\xb8\x80\xe4\xb8\x8bbert\xe4\xbb\xa3\xe7\xa0\x81\xe5\x8a\x9f\xe8\x83\xbd, \n![](https://img.shields.io/github/stars/cnfive/cnbert.svg)\n\n17. [brightmart/bert_customized](https://github.com/brightmart/bert_customized), bert with customized features,\n![](https://img.shields.io/github/stars/brightmart/bert_customized.svg)\n\n\n19. [JayYip/bert-multitask-learning](https://github.com/JayYip/bert-multitask-learning), BERT for Multitask Learning, \n![](https://img.shields.io/github/stars/JayYip/bert-multitask-learning.svg)\n\n20. [yuanxiaosc/BERT_Paper_Chinese_Translation](https://github.com/yuanxiaosc/BERT_Paper_Chinese_Translation), BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \xe8\xae\xba\xe6\x96\x87\xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe7\xbf\xbb\xe8\xaf\x91\xe3\x80\x82Chinese Translation! https://yuanxiaosc.github.io/2018/12/\xe2\x80\xa6, \n![](https://img.shields.io/github/stars/yuanxiaosc/BERT_Paper_Chinese_Translation.svg)\n\n21. [yaserkl/BERTvsULMFIT](https://github.com/yaserkl/BERTvsULMFIT), Comparing Text Classification results using BERT embedding and ULMFIT embedding,\n![](https://img.shields.io/github/stars/yaserkl/BERTvsULMFIT.svg)\n\n22. [kpot/keras-transformer](https://github.com/kpot/keras-transformer), Keras library for building (Universal) Transformers, facilitating BERT and GPT models, \n![](https://img.shields.io/github/stars/kpot/keras-transformer.svg)\n\n23. [1234560o/Bert-model-code-interpretation](https://github.com/1234560o/Bert-model-code-interpretation), \xe8\xa7\xa3\xe8\xaf\xbbtensorflow\xe7\x89\x88\xe6\x9c\xacBert\xe4\xb8\xadmodeling.py\xe6\x95\xb0\xe6\x8d\xae\xe6\xb5\x81\n![](https://img.shields.io/github/stars/1234560o/Bert-model-code-interpretation.svg)\n\n24. [cdathuraliya/bert-inference](https://github.com/cdathuraliya/bert-inference), A helper class for Google BERT (Devlin et al., 2018) to support online prediction and model pipelining. \n![](https://img.shields.io/github/stars/cdathuraliya/bert-inference.svg)\n\n\n25. [gameofdimension/java-bert-predict](https://github.com/gameofdimension/java-bert-predict), turn bert pretrain checkpoint into saved model for a feature extracting demo in java\n![](https://img.shields.io/github/stars/gameofdimension/java-bert-predict.svg)\n\n26. [1234560o/Bert-model-code-interpretation](https://github.com/1234560o/Bert-model-code-interpretation), \xe8\xa7\xa3\xe8\xaf\xbbtensorflow\xe7\x89\x88\xe6\x9c\xacBert\xe4\xb8\xadmodeling.py\xe6\x95\xb0\xe6\x8d\xae\xe6\xb5\x81\n![](https://img.shields.io/github/stars/1234560o/Bert-model-code-interpretation.svg)\n\n</details>\n\n\n## domain specific BERT: \n\n1. [allenai/scibert](https://github.com/allenai/scibert), A BERT model for scientific text. https://arxiv.org/abs/1903.10676,\n![](https://img.shields.io/github/stars/allenai/scibert.svg)\n\n2. [MeRajat/SolvingAlmostAnythingWithBert](https://github.com/MeRajat/SolvingAlmostAnythingWithBert), BioBert Pytorch\n![](https://img.shields.io/github/stars/MeRajat/SolvingAlmostAnythingWithBert.svg)\n\n3. [kexinhuang12345/clinicalBERT](https://github.com/kexinhuang12345/clinicalBERT), ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission https://arxiv.org/abs/1904.05342\n![](https://img.shields.io/github/stars/kexinhuang12345/clinicalBERT.svg)\n\n4. [EmilyAlsentzer/clinicalBERT](https://github.com/EmilyAlsentzer/clinicalBERT), repository for Publicly Available Clinical BERT Embeddings\n![](https://img.shields.io/github/stars/EmilyAlsentzer/clinicalBERT.svg)\n\n\n## BERT Deploy Tricks:\n\n1. [zhihu/cuBERT](https://github.com/zhihu/cuBERT), Fast implementation of BERT inference directly on NVIDIA (CUDA, CUBLAS) and Intel MKL\n![](https://img.shields.io/github/stars/zhihu/cuBERT.svg)\n\n2. [xmxoxo/BERT-train2deploy](https://github.com/xmxoxo/BERT-train2deploy), Bert Model training and deploy, BERT\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbb\x8e\xe8\xae\xad\xe7\xbb\x83\xe5\x88\xb0\xe9\x83\xa8\xe7\xbd\xb2, \n![](https://img.shields.io/github/stars/xmxoxo/BERT-train2deploy.svg)\n\n\n3. [https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT](https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT), BERT For TensorFlow, This repository provides a script and recipe to train BERT to achieve state of the art accuracy, and is tested and maintained by NVIDIA.\n![](https://img.shields.io/github/stars/NVIDIA/DeepLearningExamples.svg)\n\n4. [qiangsiwei/bert_distill](https://github.com/qiangsiwei/bert_distill), BERT distillation\xef\xbc\x88\xe5\x9f\xba\xe4\xba\x8eBERT\xe7\x9a\x84\xe8\x92\xb8\xe9\xa6\x8f\xe5\xae\x9e\xe9\xaa\x8c \xef\xbc\x89, \n![](https://img.shields.io/github/stars/qiangsiwei/bert_distill.svg)\n\n5. [kevinmtian/distill-bert](https://github.com/kevinmtian/distill-bert), Knowledge Distillation from BERT, \n![](https://img.shields.io/github/stars/kevinmtian/distill-bert.svg)\n\n\n\n\n\n## BERT QA & RC task:\n\n1. [sogou/SMRCToolkit](https://github.com/sogou/SMRCToolkit), This toolkit was designed for the fast and efficient development of modern machine comprehension models, including both published models and original prototypes., \n![](https://img.shields.io/github/stars/sogou/SMRCToolkit.svg)\n\n\n2. [benywon/ChineseBert](https://github.com/benywon/ChineseBert), This is a chinese Bert model specific for question answering,\n![](https://img.shields.io/github/stars/benywon/ChineseBert.svg)\n\n3. [matthew-z/R-net](https://github.com/matthew-z/R-net), R-net in PyTorch, with BERT and ELMo,\n![](https://img.shields.io/github/stars/matthew-z/R-net.svg)\n\n4. [nyu-dl/dl4marco-bert](https://github.com/nyu-dl/dl4marco-bert), Passage Re-ranking with BERT,\n![](https://img.shields.io/github/stars/nyu-dl/dl4marco-bert.svg)\n\n5. [xzp27/BERT-for-Chinese-Question-Answering](https://github.com/xzp27/BERT-for-Chinese-Question-Answering), \n![](https://img.shields.io/github/stars/xzp27/BERT-for-Chinese-Question-Answering.svg)\n\n6. [chiayewken/bert-qa](https://github.com/chiayewken/bert-qa), BERT for question answering starting with HotpotQA,\n![](https://img.shields.io/github/stars/chiayewken/bert-qa.svg)\n\n7. [ankit-ai/BertQA-Attention-on-Steroids](https://github.com/ankit-ai/BertQA-Attention-on-Steroids), BertQA - Attention on Steroids,\n![](https://img.shields.io/github/stars/ankit-ai/BertQA-Attention-on-Steroids.svg)\n\n8. [NoviScl/BERT-RACE](https://github.com/NoviScl/BERT-RACE), This work is based on Pytorch implementation of BERT (https://github.com/huggingface/pytorch-pretrained-BERT). I adapted the original BERT model to work on multiple choice machine comprehension.\n![](https://img.shields.io/github/stars/NoviScl/BERT-RACE.svg)\n\n9. [eva-n27/BERT-for-Chinese-Question-Answering](https://github.com/eva-n27/BERT-for-Chinese-Question-Answering), \n![](https://img.shields.io/github/stars/eva-n27/BERT-for-Chinese-Question-Answering.svg)\n\n10. [allenai/allennlp-bert-qa-wrapper](https://github.com/allenai/allennlp-bert-qa-wrapper),  This is a simple wrapper on top of pretrained BERT based QA models from pytorch-pretrained-bert to make AllenNLP model archives, so that you can serve demos from AllenNLP.\n![](https://img.shields.io/github/stars/allenai/allennlp-bert-qa-wrapper.svg)\n\n11. [edmondchensj/ChineseQA-with-BERT](https://github.com/edmondchensj/ChineseQA-with-BERT), EECS 496: Advanced Topics in Deep Learning Final Project: Chinese Question Answering with BERT (Baidu DuReader Dataset)\n![](https://img.shields.io/github/stars/edmondchensj/ChineseQA-with-BERT.svg)\n\n12. [graykode/toeicbert](https://github.com/graykode/toeicbert), TOEIC(Test of English for International Communication) solving using pytorch-pretrained-BERT model.,\n![](https://img.shields.io/github/starsgraykode/toeicbert.svg)\n\n13. [graykode/KorQuAD-beginner](https://github.com/graykode/KorQuAD-beginner), https://github.com/graykode/KorQuAD-beginner\n![](https://img.shields.io/github/stars/graykode/KorQuAD-beginner.svg)\n\n14. [krishna-sharma19/SBU-QA](https://github.com/krishna-sharma19/SBU-QA), This repository uses pretrain BERT embeddings for transfer learning in QA domain\n![](https://img.shields.io/github/stars/krishna-sharma19/SBU-QA.svg)\n\n15. [basketballandlearn/Dureader-Bert](https://github.com/basketballandlearn/Dureader-Bert), BERT Dureader\xe5\xa4\x9a\xe6\x96\x87\xe6\xa1\xa3\xe9\x98\x85\xe8\xaf\xbb\xe7\x90\x86\xe8\xa7\xa3 \xe6\x8e\x92\xe5\x90\x8d\xe7\xac\xac\xe4\xb8\x83, 2019 Dureader\xe6\x9c\xba\xe5\x99\xa8\xe9\x98\x85\xe8\xaf\xbb\xe7\x90\x86\xe8\xa7\xa3 \xe5\x8d\x95\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbb\xa3\xe7\xa0\x81\xe3\x80\x82, \n![](https://img.shields.io/github/stars/basketballandlearn/Dureader-Bert.svg)\n\n\n\n## BERT classification task:\n\n1. [zhpmatrix/Kaggle-Quora-Insincere-Questions-Classification](https://github.com/zhpmatrix/Kaggle-Quora-Insincere-Questions-Classification), Kaggle\xe6\x96\xb0\xe8\xb5\x9b(baseline)-\xe5\x9f\xba\xe4\xba\x8eBERT\xe7\x9a\x84fine-tuning\xe6\x96\xb9\xe6\xa1\x88+\xe5\x9f\xba\xe4\xba\x8etensor2tensor\xe7\x9a\x84Transformer Encoder\xe6\x96\xb9\xe6\xa1\x88\n![](https://img.shields.io/github/stars/zhpmatrix/Kaggle-Quora-Insincere-Questions-Classification.svg)\n\n2. [maksna/bert-fine-tuning-for-chinese-multiclass-classification](https://github.com/maksna/bert-fine-tuning-for-chinese-multiclass-classification), use google pre-training model bert to fine-tuning for the chinese multiclass classification\n![](https://img.shields.io/github/stars/maksna/bert-fine-tuning-for-chinese-multiclass-classification.svg)\n\n3. [NLPScott/bert-Chinese-classification-task](https://github.com/NLPScott/bert-Chinese-classification-task), bert\xe4\xb8\xad\xe6\x96\x87\xe5\x88\x86\xe7\xb1\xbb\xe5\xae\x9e\xe8\xb7\xb5,\n![](https://img.shields.io/github/stars/NLPScott/bert-Chinese-classification-task.svg)\n\n4. [Socialbird-AILab/BERT-Classification-Tutorial](https://github.com/Socialbird-AILab/BERT-Classification-Tutorial),\n![](https://img.shields.io/github/stars/Socialbird-AILab/BERT-Classification-Tutorial.svg)\n\n5. [fooSynaptic/BERT_classifer_trial](https://github.com/fooSynaptic/BERT_classifer_trial), BERT trial for chinese corpus classfication\n![](https://img.shields.io/github/stars/fooSynaptic/BERT_classifer_trial.svg)\n\n6. [xiaopingzhong/bert-finetune-for-classfier](https://github.com/xiaopingzhong/bert-finetune-for-classfier), \xe5\xbe\xae\xe8\xb0\x83BERT\xe6\xa8\xa1\xe5\x9e\x8b,\xe5\x90\x8c\xe6\x97\xb6\xe6\x9e\x84\xe5\xbb\xba\xe8\x87\xaa\xe5\xb7\xb1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\xae\x9e\xe7\x8e\xb0\xe5\x88\x86\xe7\xb1\xbb\n![](https://img.shields.io/github/stars/xiaopingzhong/bert-finetune-for-classfier.svg)\n\n7. [pengming617/bert_classification](https://github.com/pengming617/bert_classification), \xe5\x88\xa9\xe7\x94\xa8bert\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x96\x87\xe6\x9c\xac\xe5\x88\x86\xe7\xb1\xbb,\n![](https://img.shields.io/github/stars/pengming617/bert_classification.svg)\n\n8. [xieyufei1993/Bert-Pytorch-Chinese-TextClassification](https://github.com/xieyufei1993/Bert-Pytorch-Chinese-TextClassification), Pytorch Bert Finetune in Chinese Text Classification,\n![](https://img.shields.io/github/stars/xieyufei1993/Bert-Pytorch-Chinese-TextClassification.svg)\n\n9. [liyibo/text-classification-demos](https://github.com/liyibo/text-classification-demos), Neural models for Text Classification in Tensorflow, such as cnn, dpcnn, fasttext, bert ...,\n![](https://img.shields.io/github/stars/liyibo/text-classification-demos.svg)\n\n10. [circlePi/BERT_Chinese_Text_Class_By_pytorch](https://github.com/circlePi/BERT_Chinese_Text_Class_By_pytorch), A Pytorch implements of Chinese text class based on BERT_Pretrained_Model,\n![](https://img.shields.io/github/stars/circlePi/BERT_Chinese_Text_Class_By_pytorch.svg)\n\n11. [kaushaltrivedi/bert-toxic-comments-multilabel](https://github.com/kaushaltrivedi/bert-toxic-comments-multilabel), Multilabel classification for Toxic comments challenge using Bert,\n![](https://img.shields.io/github/stars/kaushaltrivedi/bert-toxic-comments-multilabel.svg)\n\n12. [lonePatient/BERT-chinese-text-classification-pytorch](https://github.com/lonePatient/BERT-chinese-text-classification-pytorch), This repo contains a PyTorch implementation of a pretrained BERT model for text classification.,\n![](https://img.shields.io/github/stars/lonePatient/BERT-chinese-text-classification-pytorch.svg)\n\n\n\n## BERT Sentiment Analysis\n\n1. [Chung-I/Douban-Sentiment-Analysis](https://github.com/Chung-I/Douban-Sentiment-Analysis), Sentiment Analysis on Douban Movie Short Comments Dataset using BERT.\n![](https://img.shields.io/github/stars/Chung-I/Douban-Sentiment-Analysis.svg)\n\n2. [lynnna-xu/bert_sa](https://github.com/lynnna-xu/bert_sa), bert sentiment analysis tensorflow serving with RESTful API\n![](https://img.shields.io/github/stars/lynnna-xu/bert_sa.svg)\n\n3. [HSLCY/ABSA-BERT-pair](https://github.com/HSLCY/ABSA-BERT-pair), Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence (NAACL 2019) https://arxiv.org/abs/1903.09588,\n![](https://img.shields.io/github/stars/HSLCY/ABSA-BERT-pair.svg)\n\n4. [songyouwei/ABSA-PyTorch](https://github.com/songyouwei/ABSA-PyTorch), Aspect Based Sentiment Analysis, PyTorch Implementations. \xe5\x9f\xba\xe4\xba\x8e\xe6\x96\xb9\xe9\x9d\xa2\xe7\x9a\x84\xe6\x83\x85\xe6\x84\x9f\xe5\x88\x86\xe6\x9e\x90\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8PyTorch\xe5\xae\x9e\xe7\x8e\xb0\xe3\x80\x82,\n![](https://img.shields.io/github/stars/songyouwei/ABSA-PyTorch.svg)\n\n5. [howardhsu/BERT-for-RRC-ABSA](https://github.com/howardhsu/BERT-for-RRC-ABSA), code for our NAACL 2019 paper: "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis",\n![](https://img.shields.io/github/stars/howardhsu/BERT-for-RRC-ABSA.svg)\n\n6. [brightmart/sentiment_analysis_fine_grain](https://github.com/brightmart/sentiment_analysis_fine_grain), Multi-label Classification with BERT; Fine Grained Sentiment Analysis from AI challenger,\n![](https://img.shields.io/github/stars/brightmart/sentiment_analysis_fine_grain.svg)\n\n\n## BERT  NER  task:  \n\n1. [zhpmatrix/bert-sequence-tagging](https://github.com/zhpmatrix/bert-sequence-tagging), \xe5\x9f\xba\xe4\xba\x8eBERT\xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe5\xba\x8f\xe5\x88\x97\xe6\xa0\x87\xe6\xb3\xa8\n![](https://img.shields.io/github/stars/zhpmatrix/bert-sequence-tagging.svg)\n\n2. [kyzhouhzau/BERT-NER](https://github.com/kyzhouhzau/BERT-NER), Use google BERT to do CoNLL-2003 NER ! ,\n![](https://img.shields.io/github/stars/kyzhouhzau/BERT-NER.svg)\n\n3. [king-menin/ner-bert](https://github.com/king-menin/ner-bert), NER task solution (bert-Bi-LSTM-CRF) with google bert https://github.com/google-research.\n![](https://img.shields.io/github/stars/king-menin/ner-bert.svg)\n\n4. [macanv/BERT-BiLSMT-CRF-NER](https://github.com/macanv/BERT-BiLSMT-CRF-NER), Tensorflow solution of NER task Using BiLSTM-CRF model with Google BERT Fine-tuning  ,\n![](https://img.shields.io/github/stars/macanv/BERT-BiLSMT-CRF-NER.svg)\n\n5. [FuYanzhe2/Name-Entity-Recognition](https://github.com/FuYanzhe2/Name-Entity-Recognition), Lstm-crf,Lattice-CRF,bert-ner\xe5\x8f\x8a\xe8\xbf\x91\xe5\xb9\xb4ner\xe7\x9b\xb8\xe5\x85\xb3\xe8\xae\xba\xe6\x96\x87follow,\n![](https://img.shields.io/github/stars/FuYanzhe2/Name-Entity-Recognition.svg)\n\n6. [mhcao916/NER_Based_on_BERT](https://github.com/mhcao916/NER_Based_on_BERT), this project is based on google bert model, which is a Chinese NER\n![](https://img.shields.io/github/stars/mhcao916/NER_Based_on_BERT.svg)\n\n7. [ProHiryu/bert-chinese-ner](https://github.com/ProHiryu/bert-chinese-ner), \xe4\xbd\xbf\xe7\x94\xa8\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe8\xaf\xad\xe8\xa8\x80\xe6\xa8\xa1\xe5\x9e\x8bBERT\xe5\x81\x9a\xe4\xb8\xad\xe6\x96\x87NER,\n![](https://img.shields.io/github/stars/ProHiryu/bert-chinese-ner.svg)\n\n8. [sberbank-ai/ner-bert](https://github.com/sberbank-ai/ner-bert), BERT-NER (nert-bert) with google bert,\n![](https://img.shields.io/github/stars/sberbank-ai/ner-bert.svg)\n\n9. [kyzhouhzau/Bert-BiLSTM-CRF](https://github.com/kyzhouhzau/Bert-BiLSTM-CRF), This model base on bert-as-service. Model structure : bert-embedding bilstm crf. ,\n![](https://img.shields.io/github/stars/kyzhouhzau/Bert-BiLSTM-CRF.svg)\n\n10. [Hoiy/berserker](https://github.com/Hoiy/berserker), Berserker - BERt chineSE woRd toKenizER, Berserker (BERt chineSE woRd toKenizER) is a Chinese tokenizer built on top of Google\'s BERT model. ,\n![](https://img.shields.io/github/stars/Hoiy/berserker.svg)\n\n11. [Kyubyong/bert_ner](https://github.com/Kyubyong/bert_ner), Ner with Bert,\n![](https://img.shields.io/github/stars/Kyubyong/bert_ner.svg)\n\n12. [jiangpinglei/BERT_ChineseWordSegment](https://github.com/jiangpinglei/BERT_ChineseWordSegment),  A Chinese word segment model based on BERT, F1-Score 97%,\n![](https://img.shields.io/github/stars/jiangpinglei/BERT_ChineseWordSegment.svg)\n\n13. [yanwii/ChineseNER](https://github.com/yanwii/ChineseNER), \xe5\x9f\xba\xe4\xba\x8eBi-GRU + CRF \xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe6\x9c\xba\xe6\x9e\x84\xe5\x90\x8d\xe3\x80\x81\xe4\xba\xba\xe5\x90\x8d\xe8\xaf\x86\xe5\x88\xab \xe4\xb8\xad\xe6\x96\x87\xe5\xae\x9e\xe4\xbd\x93\xe8\xaf\x86\xe5\x88\xab, \xe6\x94\xaf\xe6\x8c\x81google bert\xe6\xa8\xa1\xe5\x9e\x8b\n![](https://img.shields.io/github/stars/yanwii/ChineseNER.svg)\n\n14. [lemonhu/NER-BERT-pytorch](https://github.com/lemonhu/NER-BERT-pytorch), PyTorch solution of NER task Using Google AI\'s pre-trained BERT model.\n![](https://img.shields.io/github/stars/lemonhu/NER-BERT-pytorch.svg)\n\n\n## BERT Text Summarization Task: \n\n1. [nlpyang/BertSum](https://github.com/nlpyang/BertSum), Code for paper Fine-tune BERT for Extractive Summarization,\n![](https://img.shields.io/github/stars/nlpyang/BertSum.svg)\n\n2. [santhoshkolloju/Abstractive-Summarization-With-Transfer-Learning](https://github.com/santhoshkolloju/Abstractive-Summarization-With-Transfer-Learning), Abstractive summarisation using Bert as encoder and Transformer Decoder,\n![](https://img.shields.io/github/stars/santhoshkolloju/Abstractive-Summarization-With-Transfer-Learning.svg)\n\n3. [nayeon7lee/bert-summarization](https://github.com/nayeon7lee/bert-summarization), Implementation of \'Pretraining-Based Natural Language Generation for Text Summarization\', Paper: https://arxiv.org/pdf/1902.09243.pdf\n![](https://img.shields.io/github/stars/nayeon7lee/bert-summarization.svg)\n\n4. [dmmiller612/lecture-summarizer](https://github.com/dmmiller612/lecture-summarizer), Lecture summarizer with BERT\n![](https://img.shields.io/github/stars/dmmiller612/lecture-summarizer.svg)\n\n\n\n\n## BERT Text Generation Task: \n1. [asyml/texar](https://github.com/asyml/texar), Toolkit for Text Generation and Beyond https://texar.io, Texar is a general-purpose text generation toolkit, has also implemented BERT here for classification, and text generation applications by combining with Texar\'s other modules.\n![](https://img.shields.io/github/stars/asyml/texar.svg)\n\n2. [voidful/BertGenerate](https://github.com/voidful/BertGenerate), Fine tuning bert for text generation, Bert \xe5\x81\x9a \xe6\x96\x87\xe6\x9c\xac\xe7\x94\x9f\xe6\x88\x90 \xe7\x9a\x84\xe4\xb8\x80\xe4\xba\x9b\xe5\xaf\xa6\xe9\xa9\x97\n![](https://img.shields.io/github/stars/voidful/BertGenerate.svg)\n\n3. [Tiiiger/bert_score](https://github.com/Tiiiger/bert_score), BERT score for language generation,\n![](https://img.shields.io/github/stars/Tiiiger/bert_score.svg)\n\n\n\n## BERT  Knowledge Graph Task : \n\n1. [lvjianxin/Knowledge-extraction](https://github.com/lvjianxin/Knowledge-extraction), \xe5\x9f\xba\xe4\xba\x8e\xe4\xb8\xad\xe6\x96\x87\xe7\x9a\x84\xe7\x9f\xa5\xe8\xaf\x86\xe6\x8a\xbd\xe5\x8f\x96\xef\xbc\x8cBaseLine\xef\xbc\x9aBi-LSTM+CRF \xe5\x8d\x87\xe7\xba\xa7\xe7\x89\x88\xef\xbc\x9aBert\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\n![](https://img.shields.io/github/stars/lvjianxin/Knowledge-extraction.svg)\n\n2. [sakuranew/BERT-AttributeExtraction](https://github.com/sakuranew/BERT-AttributeExtraction), USING BERT FOR Attribute Extraction in KnowledgeGraph. fine-tuning and feature extraction. \xe4\xbd\xbf\xe7\x94\xa8\xe5\x9f\xba\xe4\xba\x8ebert\xe7\x9a\x84\xe5\xbe\xae\xe8\xb0\x83\xe5\x92\x8c\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe6\x96\xb9\xe6\xb3\x95\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x9f\xa5\xe8\xaf\x86\xe5\x9b\xbe\xe8\xb0\xb1\xe7\x99\xbe\xe5\xba\xa6\xe7\x99\xbe\xe7\xa7\x91\xe4\xba\xba\xe7\x89\xa9\xe8\xaf\x8d\xe6\x9d\xa1\xe5\xb1\x9e\xe6\x80\xa7\xe6\x8a\xbd\xe5\x8f\x96\xe3\x80\x82,\n![](https://img.shields.io/github/stars/sakuranew/BERT-AttributeExtraction.svg)\n\n3. [aditya-AI/Information-Retrieval-System-using-BERT](https://github.com/aditya-AI/Information-Retrieval-System-using-BERT),\n![](https://img.shields.io/github/stars/aditya-AI/Information-Retrieval-System-using-BERT.svg)\n\n4. [jkszw2014/bert-kbqa-NLPCC2017](https://github.com/jkszw2014/bert-kbqa-NLPCC2017), A trial of kbqa based on bert for NLPCC2016/2017 Task 5  (\xe5\x9f\xba\xe4\xba\x8eBERT\xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe7\x9f\xa5\xe8\xaf\x86\xe5\xba\x93\xe9\x97\xae\xe7\xad\x94\xe5\xae\x9e\xe8\xb7\xb5\xef\xbc\x8c\xe4\xbb\xa3\xe7\xa0\x81\xe5\x8f\xaf\xe8\xb7\x91\xe9\x80\x9a\xef\xbc\x89\xef\xbc\x8c\xe5\x8d\x9a\xe5\xae\xa2\xe4\xbb\x8b\xe7\xbb\x8d https://blog.csdn.net/ai_1046067944/article/details/86707784  ,\n![](https://img.shields.io/github/stars/jkszw2014/bert-kbqa-NLPCC2017.svg)\n\n5. [yuanxiaosc/Schema-based-Knowledge-Extraction](https://github.com/yuanxiaosc/Schema-based-Knowledge-Extraction), Code for http://lic2019.ccf.org.cn/kg \xe4\xbf\xa1\xe6\x81\xaf\xe6\x8a\xbd\xe5\x8f\x96\xe3\x80\x82\xe4\xbd\xbf\xe7\x94\xa8\xe5\x9f\xba\xe4\xba\x8e BERT \xe7\x9a\x84\xe5\xae\x9e\xe4\xbd\x93\xe6\x8a\xbd\xe5\x8f\x96\xe5\x92\x8c\xe5\x85\xb3\xe7\xb3\xbb\xe6\x8a\xbd\xe5\x8f\x96\xe7\x9a\x84\xe7\xab\xaf\xe5\x88\xb0\xe7\xab\xaf\xe7\x9a\x84\xe8\x81\x94\xe5\x90\x88\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82\xef\xbc\x88\xe5\xb0\x86\xe5\x9c\xa8\xe6\xaf\x94\xe8\xb5\x9b\xe7\xbb\x93\xe6\x9d\x9f\xe5\x90\x8e\xef\xbc\x8c\xe5\xae\x8c\xe5\x96\x84\xe4\xbb\xa3\xe7\xa0\x81\xe5\x92\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe8\xaf\xb4\xe6\x98\x8e\xef\xbc\x89,\n![](https://img.shields.io/github/stars/yuanxiaosc/Schema-based-Knowledge-Extraction.svg)\n\n6. [yuanxiaosc/Entity-Relation-Extraction](https://github.com/yuanxiaosc/Entity-Relation-Extraction),  Entity and Relation Extraction Based on TensorFlow. \xe5\x9f\xba\xe4\xba\x8eTensorFlow\xe7\x9a\x84\xe7\xae\xa1\xe9\x81\x93\xe5\xbc\x8f\xe5\xae\x9e\xe4\xbd\x93\xe5\x8f\x8a\xe5\x85\xb3\xe7\xb3\xbb\xe6\x8a\xbd\xe5\x8f\x96\xef\xbc\x8c2019\xe8\xaf\xad\xe8\xa8\x80\xe4\xb8\x8e\xe6\x99\xba\xe8\x83\xbd\xe6\x8a\x80\xe6\x9c\xaf\xe7\xab\x9e\xe8\xb5\x9b\xe4\xbf\xa1\xe6\x81\xaf\xe6\x8a\xbd\xe5\x8f\x96\xe4\xbb\xbb\xe5\x8a\xa1\xe8\xa7\xa3\xe5\x86\xb3\xe6\x96\xb9\xe6\xa1\x88\xef\xbc\x88\xe6\xaf\x94\xe8\xb5\x9b\xe7\xbb\x93\xe6\x9d\x9f\xe5\x90\x8e\xe5\xae\x8c\xe5\x96\x84\xe4\xbb\xa3\xe7\xa0\x81\xef\xbc\x89\xe3\x80\x82Schema based Knowledge Extraction, SKE 2019 http://lic2019.ccf.org.cn,\n![](https://img.shields.io/github/stars/yuanxiaosc/Entity-Relation-Extraction.svg)\n\n7. [WenRichard/KBQA-BERT](https://github.com/WenRichard/KBQA-BERT), \xe5\x9f\xba\xe4\xba\x8e\xe7\x9f\xa5\xe8\xaf\x86\xe5\x9b\xbe\xe8\xb0\xb1\xe7\x9a\x84\xe9\x97\xae\xe7\xad\x94\xe7\xb3\xbb\xe7\xbb\x9f\xef\xbc\x8cBERT\xe5\x81\x9a\xe5\x91\xbd\xe5\x90\x8d\xe5\xae\x9e\xe4\xbd\x93\xe8\xaf\x86\xe5\x88\xab\xe5\x92\x8c\xe5\x8f\xa5\xe5\xad\x90\xe7\x9b\xb8\xe4\xbc\xbc\xe5\xba\xa6\xef\xbc\x8c\xe5\x88\x86\xe4\xb8\xbaonline\xe5\x92\x8coutline\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe5\x8d\x9a\xe5\xae\xa2\xe4\xbb\x8b\xe7\xbb\x8d https://zhuanlan.zhihu.com/p/62946533  ,\n![](https://img.shields.io/github/stars/WenRichard/KBQA-BERT.svg)\n\n8. [zhpmatrix/BERTem](https://github.com/zhpmatrix/BERTem), ACL2019\xe8\xae\xba\xe6\x96\x87\xe5\xae\x9e\xe7\x8e\xb0\xe3\x80\x8aMatching the Blanks: Distributional Similarity for Relation Learning\xe3\x80\x8b ,\n![](https://img.shields.io/github/stars/zhpmatrix/BERTem.svg)\n\n\n\n\n## BERT  Coreference Resolution \n1. [ianycxu/RGCN-with-BERT](https://github.com/ianycxu/RGCN-with-BERT), Gated-Relational Graph Convolutional Networks (RGCN) with BERT for Coreference Resolution Task\n![](https://img.shields.io/github/stars/ianycxu/RGCN-with-BERT.svg)\n\n2. [isabellebouchard/BERT_for_GAP-coreference](https://github.com/isabellebouchard/BERT_for_GAP-coreference),  BERT finetuning for GAP unbiased pronoun resolution\n![](https://img.shields.io/github/stars/isabellebouchard/BERT_for_GAP-coreference.svg)\n\n\n\n## BERT  visualization toolkit: \n1. [jessevig/bertviz](https://github.com/jessevig/bertviz), Tool for visualizing BERT\'s attention,\n![](https://img.shields.io/github/stars/jessevig/bertviz.svg)\n\n\n## BERT chatbot :\n1. [GaoQ1/rasa_nlu_gq](https://github.com/GaoQ1/rasa_nlu_gq), turn natural language into structured data(\xe6\x94\xaf\xe6\x8c\x81\xe4\xb8\xad\xe6\x96\x87\xef\xbc\x8c\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe4\xba\x86N\xe7\xa7\x8d\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\x94\xaf\xe6\x8c\x81\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x9c\xba\xe6\x99\xaf\xe5\x92\x8c\xe4\xbb\xbb\xe5\x8a\xa1),\n![](https://img.shields.io/github/stars/GaoQ1/rasa_nlu_gq.svg)\n\n2. [GaoQ1/rasa_chatbot_cn](https://github.com/GaoQ1/rasa_chatbot_cn), \xe5\x9f\xba\xe4\xba\x8erasa-nlu\xe5\x92\x8crasa-core \xe6\x90\xad\xe5\xbb\xba\xe7\x9a\x84\xe5\xaf\xb9\xe8\xaf\x9d\xe7\xb3\xbb\xe7\xbb\x9fdemo,\n![](https://img.shields.io/github/stars/GaoQ1/rasa_chatbot_cn.svg)\n\n3. [GaoQ1/rasa-bert-finetune](https://github.com/GaoQ1/rasa-bert-finetune), \xe6\x94\xaf\xe6\x8c\x81rasa-nlu \xe7\x9a\x84bert finetune,\n![](https://img.shields.io/github/stars/GaoQ1/rasa-bert-finetune.svg)\n\n4. [geodge831012/bert_robot](https://github.com/geodge831012/bert_robot), \xe7\x94\xa8\xe4\xba\x8e\xe6\x99\xba\xe8\x83\xbd\xe5\x8a\xa9\xe6\x89\x8b\xe5\x9b\x9e\xe7\xad\x94\xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe5\x9f\xba\xe4\xba\x8eBERT\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xe6\x94\xb9\xe9\x80\xa0\n![](https://img.shields.io/github/stars/geodge831012/bert_robot.svg)\n\n5. [yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification](https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification), This is the template code to use BERT for sequence lableing and text classification, in order to facilitate BERT for more tasks. Currently, the template code has included conll-2003 named entity identification, Snips Slot Filling and Intent Prediction.\n![](https://img.shields.io/github/stars/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification.svg)\n\n6. [guillaume-chevalier/ReuBERT](https://github.com/guillaume-chevalier/ReuBERT), A question-answering chatbot, simply.\n![](https://img.shields.io/github/stars/guillaume-chevalier/ReuBERT.svg)\n\n\n\n## BERT language model and embedding: \n\n1.  [hanxiao/bert-as-service](https://github.com/hanxiao/bert-as-service),    Mapping a variable-length sentence to a fixed-length vector using pretrained BERT model,\n![](https://img.shields.io/github/stars/hanxiao/bert-as-service.svg)\n\n2. [YC-wind/embedding_study](https://github.com/YC-wind/embedding_study), \xe4\xb8\xad\xe6\x96\x87\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x94\x9f\xe6\x88\x90\xe5\xad\x97\xe5\x90\x91\xe9\x87\x8f\xe5\xad\xa6\xe4\xb9\xa0\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95BERT\xef\xbc\x8cELMO\xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe6\x95\x88\xe6\x9e\x9c,\n![](https://img.shields.io/github/stars/YC-wind/embedding_study.svg)\n\n3. [Kyubyong/bert-token-embeddings](https://github.com/Kyubyong/bert-token-embeddings), Bert Pretrained Token Embeddings,\n![](https://img.shields.io/github/stars/Kyubyong/bert-token-embeddings.svg)\n\n4. [xu-song/bert_as_language_model](https://github.com/xu-song/bert_as_language_model), bert as language model, fork from https://github.com/google-research/bert,\n![](https://img.shields.io/github/stars/xu-song/bert_as_language_model.svg)\n\n5. [yuanxiaosc/Deep_dynamic_word_representation](https://github.com/yuanxiaosc/Deep_dynamic_word_representation), TensorFlow code and pre-trained models for deep dynamic word representation (DDWR). It combines the BERT model and ELMo\'s deep context word representation.,\n![](https://img.shields.io/github/stars/yuanxiaosc/Deep_dynamic_word_representation.svg)\n\n6. [imgarylai/bert-embedding](https://github.com/imgarylai/bert-embedding), Token level embeddings from BERT model on mxnet and gluonnlp http://bert-embedding.readthedocs.io/,\n![](https://img.shields.io/github/stars/imgarylai/bert-embedding.svg)\n\n7. [terrifyzhao/bert-utils](https://github.com/terrifyzhao/bert-utils), BERT\xe7\x94\x9f\xe6\x88\x90\xe5\x8f\xa5\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8cBERT\xe5\x81\x9a\xe6\x96\x87\xe6\x9c\xac\xe5\x88\x86\xe7\xb1\xbb\xe3\x80\x81\xe6\x96\x87\xe6\x9c\xac\xe7\x9b\xb8\xe4\xbc\xbc\xe5\xba\xa6\xe8\xae\xa1\xe7\xae\x97,\n![](https://img.shields.io/github/stars/terrifyzhao/bert-utils.svg)\n\n8. [fennuDetudou/BERT_implement](https://github.com/fennuDetudou/BERT_implement), \xe4\xbd\xbf\xe7\x94\xa8BERT\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x96\x87\xe6\x9c\xac\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c\xe7\x9b\xb8\xe4\xbc\xbc\xe5\x8f\xa5\xe5\xad\x90\xe5\x88\xa4\xe6\x96\xad\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8a\xe8\xaf\x8d\xe6\x80\xa7\xe6\xa0\x87\xe6\xb3\xa8,\n![](https://img.shields.io/github/stars/fennuDetudou/BERT_implement.svg)\n\n9. [whqwill/seq2seq-keyphrase-bert](https://github.com/whqwill/seq2seq-keyphrase-bert), add BERT to encoder part for https://github.com/memray/seq2seq-keyphrase-pytorch,\n![](https://img.shields.io/github/stars/whqwill/seq2seq-keyphrase-bert.svg)\n\n10. [charles9n/bert-sklearn](https://github.com/charles9n/bert-sklearn), a sklearn wrapper for Google\'s BERT model,\n![](https://img.shields.io/github/stars/charles9n/bert-sklearn.svg)\n\n\n11. [NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM), Ongoing research training transformer language models at scale, including: BERT,\n![](https://img.shields.io/github/stars/NVIDIA/Megatron-LM.svg)\n\n12. [hankcs/BERT-token-level-embedding](https://github.com/hankcs/BERT-token-level-embedding), Generate BERT token level embedding without pain\n![](https://img.shields.io/github/stars/hankcs/BERT-token-level-embedding.svg)\n\n13. [facebookresearch/LAMA](https://github.com/facebookresearch/LAMA), LAMA: LAnguage Model Analysis, LAMA is a set of connectors to pre-trained language models.\n![](https://img.shields.io/github/stars/facebookresearch/LAMA.svg)\n\n\n\n\n## BERT Text Match: \n\n1. [pengming617/bert_textMatching](https://github.com/pengming617/bert_textMatching), \xe5\x88\xa9\xe7\x94\xa8\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xae\x9e\xe7\x8e\xb0\xe5\x9f\xba\xe4\xba\x8ebert\xe7\x9a\x84\xe8\xaf\xad\xe4\xb9\x89\xe5\x8c\xb9\xe9\x85\x8d\xe6\xa8\xa1\xe5\x9e\x8b \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xbaLCQMC\xe5\xae\x98\xe6\x96\xb9\xe6\x95\xb0\xe6\x8d\xae\n![](https://img.shields.io/github/stars/pengming617/bert_textMatching.svg)\n\n2. [Brokenwind/BertSimilarity](https://github.com/Brokenwind/BertSimilarity), Computing similarity of two sentences with google\'s BERT algorithm\n![](https://img.shields.io/github/stars/Brokenwind/BertSimilarity.svg)\n\n3. [policeme/chinese_bert_similarity](https://github.com/policeme/chinese_bert_similarity), bert chinese similarity\n![](https://img.shields.io/github/stars/policeme/chinese_bert_similarity.svg)\n\n4. [lonePatient/bert-sentence-similarity-pytorch](https://github.com/lonePatient/bert-sentence-similarity-pytorch), This repo contains a PyTorch implementation of a pretrained BERT model for sentence similarity task.\n![](https://img.shields.io/github/stars/lonePatient/bert-sentence-similarity-pytorch.svg)\n\n5. [nouhadziri/DialogEntailment](https://github.com/nouhadziri/DialogEntailment), The implementation of the paper "Evaluating Coherence in Dialogue Systems using Entailment" https://arxiv.org/abs/1904.03371\n![](https://img.shields.io/github/stars/nouhadziri/DialogEntailment.svg)\n\n6. [UKPLab/sentence-transformers](https://github.com/UKPLab/sentence-transformers), Sentence Embeddings with BERT & XLNet, \nSentence Transformers: Sentence Embeddings using BERT / RoBERTa / XLNet with PyTorch, \n![](https://img.shields.io/github/stars/UKPLab/sentence-transformers.svg)\n\n\n## BERT tutorials: \n\n1. [graykode/nlp-tutorial](https://github.com/graykode/nlp-tutorial), Natural Language Processing Tutorial for Deep Learning Researchers https://www.reddit.com/r/MachineLearn\xe2\x80\xa6,\n![](https://img.shields.io/github/stars/graykode/nlp-tutorial.svg)\n\n2. [dragen1860/TensorFlow-2.x-Tutorials](https://github.com/dragen1860/TensorFlow-2.x-Tutorials), TensorFlow 2.x version\'s Tutorials and Examples, including CNN, RNN, GAN, Auto-Encoders, FasterRCNN, GPT, BERT examples, etc. TF 2.0\xe7\x89\x88\xe5\x85\xa5\xe9\x97\xa8\xe5\xae\x9e\xe4\xbe\x8b\xe4\xbb\xa3\xe7\xa0\x81\xef\xbc\x8c\xe5\xae\x9e\xe6\x88\x98\xe6\x95\x99\xe7\xa8\x8b\xe3\x80\x82,\n![](https://img.shields.io/github/stars/dragen1860/TensorFlow-2.x-Tutorials.svg)\n\n\n'