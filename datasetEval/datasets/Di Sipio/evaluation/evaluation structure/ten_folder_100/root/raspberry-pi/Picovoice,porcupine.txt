b'# Porcupine\n\n[![GitHub release](https://img.shields.io/github/release/Picovoice/Porcupine.svg)](https://github.com/Picovoice/Porcupine/releases)\n\nMade in Vancouver, Canada by [Picovoice](https://picovoice.ai)\n\nPorcupine is a highly-accurate and lightweight **wake word** (a.k.a. **keyword spotting**, **trigger word detection**,\n**hotword detection**, and **voice command**) engine. It enables developers to build always-listening voice-enabled\napplications. It is\n\n* using **deep neural networks** trained in **real-world situations**.\n* compact and computationally-efficient making it suitable for **IoT**. It can run with as low as **20 KB RAM** on an MCU.\n* **cross-platform**. It is implemented in fixed-point ANSI C. Currently **Raspberry Pi (all variants)**, **Beagle Bone**,\n**Android**, **iOS**, **watchOS**, **Linux (x86_64)**, **Mac**, **Windows**, and **web browsers** (**WebAssembly**) are\nsupported. Furthermore, Support for various **ARM Cortex-A** and **ARM Cortex-M** processors and **DSP cores** is available\nfor commercial customers.\n* **scalable**. It can detect multiple (possibly many) voice commands concurrently with no added CPU/memory footprint.\n* self-service. Developers are empowered to choose from a set of predefined wake phrases on different platforms and use\nthem for free. In addition, developers can generate custom wake phrases using\n[Picovoice Console](https://console.picovoice.ai) (subject to certain limitations and only on Linux, Mac, or Windows)\nfor non-commercial, personal, and evaluation-only purposes.  \n\n## Picovoice Console\n\nAnnouncing the [Picovoice Console](https://console.picovoice.ai). The Console is a web-based platform for\nbuilding voice applications. You can sign up for an account with your email address or with your GitHub account.\n\nThe console succeeds the (now retired) optimizer tool, as it can be used to train custom wake-words (Porcupine .ppn files). If you cloned\nthis repository prior to August 31st, 2019, we highly recommend to make a new clone and use the Console for training custom\nwake-words. The size of the repository is reduced from ~3 GB to 30 MB (100X reduction), which should make working with it easier as well as reduce cloning timeout errors from GitHub that some people have reported.\n\n\n## Table of Contents\n\n* [Try It Out](#try-it-out)\n* [Performance](#performance)\n* [Model Variants](#model-variants)\n* [Structure of Repository](#structure-of-repository)\n* [Running Demo Applications](#running-demo-applications)\n    * [Python Demo Application](#python-demo-application)\n    * [Android Demo Application](#android-demo-application)\n    * [iOS Demo Application](#ios-demo-application)\n* [Evaluating Keyword Files](#evaluating-keyword-files)\n* [Integration](#integration)\n    * [C](#c)\n    * [Python](#python)\n    * [C#](#csharp)\n    * [Android](#android)\n    * [iOS](#ios)\n    * [Javascript](#javascript)\n* [Contributing](#contributing)\n* [Releases](#releases)\n* [License](#license)\n* [FAQ](#faq)\n\n## Try It Out\n\nTry out Porcupine using its [interactive web demo](https://picovoice.ai/demos/porcupine_wasm_smart_lighting.html).\nYou need a working microphone.\n\nTry out Porcupine by downloading its\n[Android demo application](https://play.google.com/store/apps/details?id=ai.picovoice.porcupine.demo&hl=en). The demo\napplication allows you to test Porcupine on a variety of wake words in any environment.\n\n![Android Demo](resources/images/demo.gif)\n\nTry out Porcupine by installing its [PIP package](https://pypi.org/project/pvporcupine/).\n\nSee Porcupine in action on an ARM Cortex-M7 (accompanied by [rhino](https://github.com/Picovoice/rhino) for intent inference).\n\n[![Porcupine in Action](https://img.youtube.com/vi/WadKhfLyqTQ/0.jpg)](https://www.youtube.com/watch?v=WadKhfLyqTQ)\n\nSee Porcupine in action on an ARM Cortex-M4 (accompanied by [rhino](https://github.com/Picovoice/rhino) for intent inference).\n\n[![Porcupine in Action](https://img.youtube.com/vi/T0tAnh8tUQg/0.jpg)](https://www.youtube.com/watch?v=T0tAnh8tUQg)\n\n[![Porcupine in Action](https://img.youtube.com/vi/dH8Q-JplpSo/0.jpg)](https://www.youtube.com/watch?v=dH8Q-JplpSo)\n\n## Performance\n\nA comparison between accuracy and runtime metrics of Porcupine and two other widely-used libraries, PocketSphinx and\nSnowboy, is provided [here](https://github.com/Picovoice/wakeword-benchmark). Compared to the best-performing engine of these two,\nPorcupine\'s standard model is 3.34 times more accurate, 4.38 times faster (on Raspberry Pi 3). \n\n## Model Variants\n\nPorcupine has two flavours: **standard** and **compressed**. The compressed model is specifically designed\nfor deeply-embedded applications (MCUs and DSPs). Its accuracy is slightly lower than the standard model, but it consumes\nconsiderably less resources. Below is the comparison of runtime measurements for different variants of Porcupine on\nRaspberry Pi3:\n\n| Model Variant | CPU Usage | Model Size (KB)\n:---: | :---: | :---:\nStandard | 5.67% | 1388\nCompressed | 2.43% | 232\n\nFor an accuracy comparison of different variants, refer to\n[benchmark repository](https://github.com/Picovoice/wakeword-benchmark).\n\n## Structure of Repository\n\nPorcupine is shipped as an ANSI C precompiled library. The binary files for supported platforms are located under\n[lib/](/lib) and header files are at [include/](/include). Currently, BeagleBone, Raspberry Pi, Android, iOS, watchOS,\nLinux, macOS, Windows, and modern web browsers (with WebAssembly) are supported.\n\nBindings are available at [binding/](/binding) to facilitate usage from higher-level languages and platforms. Demo\napplications are located at [demo/](/demo). We recommend using one of the demo applications as a starting point for your own\nimplementation, when possible.\n\nFinally, [resources/](/resources) is a placeholder for data used by various applications within the repository.\n\nBelow is a quick walkthrough of the repository. For detailed instructions please visit the relevant pages. Throughout the\ndocumentation, it is assumed that the current working directory is the root of the repository.\n\n## Running Demo Applications\n\n### Python Demo Application\n\n### PIP\n\nInstall Porcupine [PIP package](https://pypi.org/project/pvporcupine/). Then with a working \xc2\xa0microphone connected to\nyour device run the following in the terminal\n\n```shell\npvporcupine_mic --keywords porcupine\n``` \n\nThe engine starts processing the audio input from the microphone in realtime and outputs to the terminal when it detects\nutterances of wake-word "porcupine".\n\nIn order to process audio files (e.g. WAV or FLAC) run\n\n```shell\npvporcupine_file --input_audio_file_path ${PATH_TO_AN_AUDIO_FILE} --keywords bumblebee\n``` \n\nThen the engine scans the given audio file for occurrences of keyword "bumblebee".\n\n### Repository\n\nThis [demo application](/demo/python) allows testing Porcupine using your computer\'s microphone. It opens an input audio\nstream, monitors it using Porcupine\'s library, and logs the detection events into the console. Below is an example of\nrunning the demo for hotword `picovoice` from the command line. Replace `${SYSTEM}` with the name of the operating system\non your machine (e.g. linux, mac, windows, or raspberrypi).\n\n```bash\npython demo/python/porcupine_demo.py --keyword_file_paths resources/keyword_files/${SYSTEM}/alexa_${SYSTEM}.ppn\n```\n\n### Android Demo Application\n\nUsing [Android Studio](https://developer.android.com/studio/index.html), open [demo/android](/demo/android) as an Android\nproject and then run the application. You will need an Android device (with developer options enabled) connected to\nyour machine.\n\n### iOS Demo Application\n\nUsing [Xcode](https://developer.apple.com/xcode/), open [demo/ios](/demo/ios) and run the application. You will need\nan iOS device connected to your machine and a valid Apple developer account.\n\n## Evaluating Keyword Files\n\nPorcupine enables developers to evaluate models for any wake word. This is done using\n[Picovoice\'s console](https://console.picovoice.ai/). You may use the console to train wake word models for execution on\nLinux (x86_64), Mac, or Windows and only for non-commercial and personal use.\n\n## Integration\n\nBelow are code snippets showcasing how Porcupine can be integrated into different applications.\n\n### C\n\nPorcupine is implemented in ANSI C and therefore can be directly linked to C applications.\n[include/pv_porcupine.h](/include/pv_porcupine.h) header file contains relevant information. An instance of Porcupine\nobject can be constructed as follows.\n\n```c\nconst char *model_file_path = ... // The file is available at lib/common/porcupine_params.pv\nconst char *keyword_file_path = ...\nconst float sensitivity = 0.5;\n\npv_porcupine_object_t *handle;\n\nconst pv_status_t status = pv_porcupine_init(model_file_path, keyword_file_path, sensitivity, &handle);\n\nif (status != PV_STATUS_SUCCESS) {\n    // error handling logic\n}\n```\n\nSensitivity is the parameter that enables developers to trade miss rate for false alarm. It is a floating number within\n[0, 1]. A higher sensitivity reduces miss rate (false reject rate) at cost of increased false alarm rate.\n\nNow the `handle` can be used to monitor incoming audio stream. Porcupine accepts single channel, 16-bit PCM audio.\nThe sample rate can be retrieved using `pv_sample_rate()`. Finally, Porcupine accepts input audio in consecutive chunks\n(aka frames) the length of each frame can be retrieved using `pv_porcupine_frame_length()`.\n\n```c\nextern const int16_t *get_next_audio_frame(void);\n\nwhile (true) {\n    const int16_t *pcm = get_next_audio_frame();\n    bool result;\n    const pv_status_t status = pv_porcupine_process(handle, pcm, &result);\n    if (status != PV_STATUS_SUCCESS) {\n        // error handling logic\n    }\n    if (result) {\n        // detection event logic/callback\n    }\n}\n```\n\nFinally, when done be sure to release the acquired resources.\n\n```c\npv_porcupine_delete(handle);\n```\n\n### Python\n\n#### PIP\n\nThe PIP package exposes a factory method to create instances of the engine as below\n\n```python\nimport pvporcupine\n\nhandle = pvporcupine.create(keywords=[\'picovoice\', \'bumblebee\'])\n```\n\n`keywords` argument is a shorthand for accessing default keyword files shipped with the library. The default keyword files\navailable can be retrieved via\n\n```python\nimport pvporcupine\n\nprint(pvporcupine.KEYWORDS)\n```\n\nIf you wish to use a non-default keyword file you need to identify its path as below\n\n```python\nimport pvporcupine\n\nhandle = pvporcupine.create(keyword_file_paths=[\'path/to/non/default/keyword/file\'])\n```\n\nIn order to learn how to use the created object continue reading the section below.\n\n#### Repository\n\n[/binding/python/porcupine.py](/binding/python/porcupine.py) provides a Python binding for Porcupine library. Below is a\nquick demonstration of how to construct an instance of it to detect multiple keywords concurrently.\n\n```python\nlibrary_path = ... # Path to Porcupine\'s C library available under lib/${SYSTEM}/${MACHINE}/\nmodel_file_path = ... # It is available at lib/common/porcupine_params.pv\nkeyword_file_paths = [\'path/to/keyword/1\', \'path/to/keyword/2\', ...]\nsensitivities = [0.5, 0.4, ...]\nhandle = Porcupine(library_path, model_file_path, keyword_file_paths=keyword_file_paths, sensitivities=sensitivities)\n```\n\nSensitivity is the parameter that enables developers to trade miss rate for false alarm. It is a floating number within\n[0, 1]. A higher sensitivity reduces miss rate at cost of increased false alarm rate.\n\nWhen initialized, valid sample rate can be obtained using ```handle.sample_rate```. Expected frame length\n(number of audio samples in an input array) is ```handle.frame_length```. The object can be used to monitor\nincoming audio as below.\n\n```python\ndef get_next_audio_frame():\n    pass\n\nwhile True:\n    pcm = get_next_audio_frame()\n    keyword_index = handle.process(pcm)\n    if keyword_index >= 0:\n        # detection event logic/callback\n        pass\n```\n\nFinally, when done be sure to explicitly release the resources as the binding class does not rely on the garbage\ncollector.\n\n```python\nhandle.delete()\n```\n\n### csharp\n\n[/binding/dotnet/PorcupineCS/Porcupine.cs](/binding/dotnet/PorcupineCS/Porcupine.cs) provides a c# binding for Porcupine\n. Below is a quick demonstration of how to construct an instance of it to detect multiple keywords concurrently.\n\n\n```csharp\nstring model_file_path = ... // The file is available at lib/common/porcupine_params.pv\nstring keyword_file_path = ...\nfloat sensitivity = 0.5;\nPorcupine instance;\n\ninstance = new Porcupine(model_file_path, keyword_file_path, sensitivity);\n\nif (instance.Status != PicoVoiceStatus.SUCCESS) {\n    // error handling logic\n}\n```\n\nSensitivity is the parameter that enables developers to trade miss rate for false alarm. It is a floating number within\n[0, 1]. A higher sensitivity reduces miss rate at cost of increased false alarm rate.\n\nNow the `instance` can be used to monitor incoming audio stream. Porcupine accepts single channel, 16-bit PCM audio.\nThe sample rate can be retrieved using `instance.SampleRate()`. Finally, Porcupine accepts input audio in consecutive chunks\n(aka frames) the length of each frame can be retrieved using `instance.FrameLength()`.\n\n```csharp\nInt16[] GetNextAudioFrame()\n{\n    ... // some functionality that gets the next frame\n}\n\n\nwhile (true) {\n    Int16[] frame = GetNextAudioFrame();\n    bool result;\n    PicoVoiceStatus status = instance.Process(pcm, out result);\n    if (status != PicoVoiceStatus.SUCCESS) {\n        // error handling logic\n    }\n    if (result) {\n        // detection event logic/callback\n    }\n}\n```\n\nFinally, when done we don\'t need to release the resources ourselves; the garbage collector will handle this.\nBut, if you want to do it yourself:\n\n```csharp\ninstance.Dispose();\n```\n\n### Android\n\nThere are two possibilities for integrating Porcupine into an Android application.\n\n#### Binding\n\n[Porcupine](/binding/android/Porcupine/app/src/main/java/ai/picovoice/porcupine/Porcupine.java) provides a binding for\nAndroid using [JNI](https://docs.oracle.com/javase/7/docs/technotes/guides/jni/). It can be initialized using.\n\n```java\n    final String modelFilePath = ... // It is available at lib/common/porcupine_params.pv\n    final String keywordFilePath = ...\n    final float sensitivity = 0.5f;\n\n    Porcupine porcupine = new Porcupine(modelFilePath, keywordFilePath, sensitivity);\n```\n\nSensitivity is the parameter that enables developers to trade miss rate for false alarm. It is a floating number within\n[0, 1]. A higher sensitivity reduces miss rate at cost of increased false alarm rate.\n\nOnce initialized, ```porcupine``` can be used to monitor incoming audio.\n\n```java\n    private short[] getNextAudioFrame();\n\n    while (true) {\n        final boolean result = porcupine.process(getNextAudioFrame());\n        if (result) {\n            // detection event logic/callback\n        }\n    }\n```\n\nFinally, be sure to explicitly release resources acquired by porcupine as the class does not rely on the garbage collector\nfor releasing native resources.\n\n```java\n    porcupine.delete();\n```\n\n#### High-Level API\n\n[PorcupineManager](binding/android/PorcupineManager/app/src/main/java/ai/picovoice/porcupinemanager/PorcupineManager.java)\n provides a high-level API for integrating Porcupine into Android applications. It manages all activities related to creating\n an input audio stream, feeding it into the Porcupine library, and invoking a user-provided detection callback. The class\n can be initialized as below.\n\n```java\n    final String modelFilePath = ... // It is available at lib/common/porcupine_params.pv\n    final String keywordFilePath = ...\n    final float sensitivity = 0.5f;\n\n    PorcupineManager manager = new PorcupineManager(\n            modelFilePath,\n            keywordFilePath,\n            sensitivity,\n            new KeywordCallback() {\n                @Override\n                public void run() {\n                    // detection event logic/callback\n                }\n            });\n```\n\nSensitivity is the parameter that enables developers to trade miss rate for false alarm. It is a floating number within\n[0, 1]. A higher sensitivity reduces miss rate at cost of increased false alarm rate.\n\nWhen initialized, input audio can be monitored using `manager.start()`. When done be sure to stop the manager using\n`manager.stop()`.\n\n### iOS\n\nThere are two approaches for integrating Porcupine into an iOS application.\n\n#### Direct\n\nPorcupine is shipped as a precompiled ANSI C library and can directly be used in Swift using module maps. It can be\ninitialized to detect multiple wake words concurrently using:\n\n```swift\nlet modelFilePath: String = ... // It is available at lib/common/porcupine_params.pv\nlet keywordFilePaths: [String] = ["path/to/keyword/1", "path/to/keyword/2", ...]\nlet sensitivities: [Float] = [0.3, 0.7, ...];\nvar handle: OpaquePointer?\n\nlet status = pv_porcupine_multiple_keywords_init(\n    modelFilePath,\n    Int32(keywordFilePaths.count), // Number of different keywords to monitor for\n    keywordFilePaths.map{ UnsafePointer(strdup($0)) },\n    sensitivities,\n    &handle)\nif status != PV_STATUS_SUCCESS {\n    // error handling logic\n}\n```\n\nThen `handle` can be used to monitor incoming audio stream.\n\n```swift\nfunc getNextAudioFrame() -> UnsafeMutablePointer<Int16> {\n    //\n}\n\nwhile true {\n    let pcm = getNextAudioFrame()\n    var keyword_index: Int32 = -1\n\n    let status = pv_porcupine_multiple_keywords_process(handle, pcm, &keyword_index)\n    if status != PV_STATUS_SUCCESS {\n        // error handling logic\n    }\n    if keyword_index >= 0 {\n        // detection event logic/callback\n    }\n}\n```\n\nWhen finished, release the resources via\n\n```swift\n    pv_porcupine_delete(handle)\n```\n\n#### Binding\n\nThe [PorcupineManager](/binding/ios/PorcupineManager.swift) class manages all activities related to creating an input audio\nstream, feeding it into Porcupine\'s library, and invoking a user-provided detection callback. The class can be\ninitialized as below:\n\n```swift\nlet modelFilePath: String = ... // It is available at lib/common/porcupine_params.pv\nlet keywordCallback: ((WakeWordConfiguration) -> Void) = {\n    // detection event callback\n}\n\nlet wakeWordConfiguration1 = WakeWordConfiguration(name: "1", filePath: "path/to/keyword/1", sensitivity: 0.5)\nlet wakewordConfiguration2 = WakeWordConfiguration(name: "2", filePath: "path/to/keyword/2", sensitivity: 0.7)\nlet configurations = [ wakeWordConfiguration1, wakewordConfiguration2 ]\n\nlet manager = try PorcupineManager(modelFilePath: modelFilePath, wakeKeywordConfigurations: configurations, onDetection: keywordCallback)\n```\n\nWhen initialized, input audio can be monitored using `manager.startListening()`. When done be sure to stop the manager using\n`manager.stopListening()`.\n\n### Javascript\n\nPorcupine is available on modern web browsers in [WebAssembly](https://webassembly.org/). The [Javascript binding](/binding/js/porcupine/porcupine.js)\nmakes it trivial use Porcupine within a Javascript environment. Instantiate a new instance of engine using the factory method\nas below\n\n```javascript\n    let keywordIDs = Array(UInt8Array(), ...);\n    let sensitivities = Float32Array(...);\n    let obj = Porcupine.create(keywordIDs, sensitivities);\n```\n\nwhen initialized incoming audio stream can be processed using the `process` method. Be sure to release the resources\nacquired by WebAssembly using `.release` when done\n\n```javascript\n    while (true) {\n        obj.process(audioFrameInt16Array);\n    }\n    \n    // release when done\n    obj.release();\n```\n\nFor more information, refer to [binding](/binding/js) and [demo](/demo/js).\n\n## Contributing\n\nIf you would like to contribute to Porcupine, please read through [CONTRIBUTING.md](CONTRIBUTING.md).\n\n### Acknowledgements\n\n* Thank you @charithe for Go binding/demo.\n* Thank you @HeadhunterXamd for C Sharp binding/demo.\n* Thank you @oziee for adding C++ ALSA demo.\n* Thank you @herlihalim for refactoring iOS binding and demo.\n* Thank you @veeableful for adding C++ and Rust demo.\n* Thank you @fquirin for adding non-blocking Python demo.\n* Thank you @dyah10 for adding watchOS binding and demo.\n\n## Releases\n\n### v1.6.0 - April 25th, 2019\n\n* Improved accuracy across all models.\n* Runtime optimization across all models\n* Added support for Beagle Bone\n* iOS build can run on simulator now.\n\n### v1.5.0 - November 13, 2018\n\n* Improved optimizer\'s accuracy.\n* Runtime optimization.\n* Added support for running within web browsers (WebAssembly).\n\n### v1.4.0 - July 20, 2018\n\n* Improved accuracy across all models (specifically compressed variant).\n* Runtime optimizations.\n* Updated documentation.\n\n### v1.3.0 - June 19, 2018\n\n* Added compressed model (200 KB) for deeply-embedded platforms.\n* Improved accuracy.\n* Runtime optimizations and bug fixes.\n\n### v1.2.0 - April 21, 2018\n\n* Runtime optimizations across platforms.\n* Added support for watchOS.\n\n### v1.1.0 - April 11, 2018\n\n* Added multiple command detection capability. Porcupine can now detect multiple commands with virtually no added\nCPU/memory footprint.\n\n### v1.0.0 - March 13, 2018\n\n* Initial release.\n\n## License\n\nThis repository is licensed under Apache 2.0. This allows running the Porcupine wake word detection library on all\nsupported platforms using the set of freely-available [keyword files](/resources/keyword_files).\n\nYou may create custom wake-word models (for execution on Linux, Mac, and Windows) using\n[Picovoice Console](https://console.picovoice.ai/) for non-commercial and personal use free of charge.\n\nCustom wake-words for other platforms must be generated by the Picovoice engineering team and are only provided with the\npurchase of the Picovoice development or commercial license. To enquire about the Picovoice development and commercial\nlicense terms and fees, [contact us](https://picovoice.ai/contact.html).\n\n## FAQ\n\n**[Q] Which Picovoice speech product should I use?**\n\n**[A]** If you need to recognize a single phrase or a number of predefined phrases (dozens or fewer), in an\nalways-listening fashion, then you should use Porcupine (wake word engine). If you need to recognize complex voice\ncommands within a confined and well-defined domain with limited number of vocabulary and variations of spoken forms\n(1000s or fewer), then you should use [Rhino](https://github.com/Picovoice/rhino) (speech-to-intent engine). If you need\nto transcribe free-form speech in an open domain, then you should use [Cheetah](https://github.com/Picovoice/cheetah)\n(speech-to-text engine).\n\n**[Q] What are the benefits of implementing voice interfaces on-device, instead of using cloud services?**\n\n**[A]** Privacy, minimal latency, improved reliability, runtime efficiency, and cost savings, to name a few. More detail is\navailable [here](https://picovoice.ai/blog/the_case_for_voice_ai_on_the_edge.html).\n\n**[Q] Does Picovoice technology work in far-field applications?**\n\n**[A]** It depends on many factors including the distance, ambient noise level, reverberation (echo), quality of\nmicrophone, and audio frontend used (if any). It is recommended to try out our technology using the freely-available\nsample models in your environment. Additionally, we often publish open-source benchmarks of our technology in noisy\nenvironments [1](https://github.com/Picovoice/wakeword-benchmark)\n[2](https://github.com/Picovoice/speech-to-intent-benchmark) [3](https://github.com/Picovoice/stt-benchmark). If the\ntarget environment is noisy and/or reverberant and the user is few meters away from the microphone, a multi-microphone audio\nfrontend can be beneficial.\n\n**[Q] Does Picovoice software work in my target environment and noise conditions?**\n\n**[A]** It depends on variety of factors. You should test it out yourself with the free samples made available on\nPicovoice GitHub pages. If it does not work, we can fine-tune it for your target environment.\n\n**[Q] Does Picovoice software work in presence of noise and reverberation?**\n\n**[A]** Picovoice software is designed to function robustly in presence of noise and reverberations. We have benchmarked\nand published the performance results under various noisy conditions [1](https://github.com/Picovoice/wakeword-benchmark)\n[2](https://github.com/Picovoice/speech-to-intent-benchmark) [3](https://github.com/Picovoice/stt-benchmark).\nThe end-to-end performance depends on the type and amount of noise and reverberation. We highly recommend testing out\nthe software using freely-available models in your target environment and application.\n\n**[Q] Can I use Picovoice software for telephony applications?**\n\n**[A]** We expect audio with 16000Hz sampling rate. PSTN networks usually sample at 8000Hz. It is possible to\nupsample, but then the frequency content above 4000Hz is missing and performance will be suboptimal. It is possible to train\nacoustic models for telephony applications, if the commercial opportunity is justified.\n\n**[Q] My audio source is 48kHz/44.1KHz. Does Picovoice software support that?**\n\n**[A]** Picovoice software expects a 16000Hz sampling rate. You will need to resample (downsample). Typically,\noperating systems or sound cards (Audio codecs) provide such functionality; otherwise, you will need to implement it.\n\n**[Q] Can Picovoice help with building my voice enabled product?**\n\n**[A]** Our core business is software licensing. That being said, we do have a wide variety of expertise internally\nin voice, software, and hardware. We consider such requests on a case-by-case basis and assist clients who can\nguarantee a certain minimum licensing volume.\n\n**[Q] If I am using GitHub to evaluate the software, do you provide technical support?**\n\n**[A]** Prior to commercial engagement, basic support solely pertaining to software issues or bugs is provided via\nGitHub issues by the open-source community or a member of our team. We do not offer any free support with integration\nor support with any platform (operating system or hardware) that is not officially supported via GitHub.\n\n**[Q] Why does Picovoice have GitHub repositories?**\n\n**[A]** To facilitate performance evaluation, for commercial prospects, and to enable the open source community to use the technology for personal and non-commercial applications.\n\n**[Q] What is the engagement process?**\n\n**[A]** You may use what is available on GitHub while respecting its governing license terms, without engaging with us.\nThis facilitates initial performance evaluation. Subsequently, you may acquire a development license to get access to custom speech models or use the software for development and internal evaluation within a company; the development license is for\nbuilding a proof-of-concept or prototype. When ready to commercialize your product, you need to acquire a commercial license.\n\n**[Q] Does Picovoice offer AEC, VAD, noise suppression, or microphone array beamforming?**\n\n**[A]** No. But we do have partners who provide such algorithms. Please add this to your inquiry when reaching out\nand we can help to connect you.\n\n**[Q] Can you build a voice-enabled app for me?**\n\n**[A]** We do not provide software development services, so most likely the answer is no. However, via a professional\nservices agreement we can help with proofs-of-concept (these will typically be rudimentary apps focused on voice user\ninterface or building the audio pipeline), evaluations on a specific domain/task, integration of SDKs in your app,\ntraining of custom acoustic and language models, and porting to custom hardware platforms.\n\n**[Q] How do I evaluate Porcupine software performance?**\n\n**[A]** We have benchmarked the performance of Porcupine software rigorously and published the results\n[here](https://github.com/Picovoice/wakeword-benchmark). We have also open-sourced the code and audio files used for\nbenchmarking on the same repository to make it possible to reproduce the results. You can also use the code with\nyour own audio files (noise sources collected from your target environment or utterances of your own wake word) to\nbenchmark the performance. Additionally, we have made a set of sample wake words freely available on this GitHub\nrepository on all platforms to facilitate evaluation, testing, and integration.\n\n**[Q] Can Porcupine wake word detection software detect non-English keywords?**\n\n**[A]** It depends. If English speakers can easily pronounce the non-English wake word,\nthen we can most likely generate it for you. We recommend sending us a few audio samples including the utterance of the\nrequested wake word so that our engineering team can review and provide feedback on feasibility.\n\n**[Q] What is Porcupine\xe2\x80\x99s wake word detection accuracy?**\n\n**[A]** We have extensive benchmarking on Porcupine performance compared accuracy against alternatives,\nand published the result [here](https://github.com/Picovoice/wakeword-benchmark). Porcupine can achieve 90%+ accuracy\n(detection rate) with less than 1 false alarm in 8 hours in the presence of ambient noise with 10dB SNR at microphone\nlevel.\n\n**[Q] Can Porcupine detect the wake word if the speaker is yelling/shouting in anger, excitement, or pain?**\n\n**[A]** Porcupine does not have a profile to recognize emotionally-coloured utterances such as yelling, dragging,\nmumbling, etc. We do require the speaker to somewhat clearly vocalize the phrase.\n\n**[Q] Does Porcupine\xe2\x80\x99s detection accuracy depend on the choice of wake word?**\n\n**[A]** Generally speaking yes, however it is difficult to quantify the cause-and-effect accurately. We have published\na guide [here](https://picovoice.ai/docs/choose-wake-word/index.html) to help you pick a wake word that would achieve\noptimal performance. You will need to avoid using short phrases, and make sure your wake word includes diverse sounds\nand at least six phonemes. Long phrases are also not recommended due to the poor user experience.\n\n**[Q] Is there a guideline for picking a wake word?**\n\n**[A]** We have published a guide [here](https://picovoice.ai/docs/choose-wake-word/index.html) to help you pick a\nwake word that would achieve optimal performance.\n\n**[Q] How much CPU and memory does Picovoice wake word detection software consume?**\n\n**[A]** We offer several trims for our wake word detection model. The standard model, which is recommended on most\nplatforms, uses roughly 1.5MB of readonly memory (ROM / FLASH) and 5% of a single core on a Raspberry Pi 3. \n\n**[Q] What should I set the sensitivity value to?**\n\n**[A]** You should pick a sensitivity parameter that suits your application requirements. A higher sensitivity value\ngives a lower miss rate at the expense of higher false alarm rate. If your application places tighter requirements on\nfalse alarms, but can tolerate misses, then you should lower the sensitivity value.  \n\n**[Q] What is an ROC curve?**\n\n**[A]** The accuracy of a binary classifier (any decision-making algorithm with a \xe2\x80\x9cyes\xe2\x80\x9d or \xe2\x80\x9cno\xe2\x80\x9d output) can be measured\nby two parameters: false rejection rate (FRR) and false acceptance rate (FAR). A wake word detector is a binary\nclassifier. Hence, we use these metrics to benchmark it.\n\nThe detection threshold of binary classifiers can be tuned to balance FRR and FAR. A lower detection threshold yields\nhigher sensitivity. A highly sensitive classifier has a high FAR and low FRR value (i.e. it accepts almost everything).\nA receiver operating characteristic (ROC) curve plots FRR values against corresponding FAR values for varying\nsensitivity values \n\nTo learn more about ROC curves and benchmarking a wake word detection, you may read the blog post\n[here](https://picovoice.ai/blog/benchmarking_wake_word_engines.html) and Porcupine benchmark published\n[here](https://github.com/Picovoice/wakeword-benchmark). \n\n**[Q] If I use Porcupine wake word detection in my mobile application, does it function when the app is running in the background?**\n\n**[A]** Developers have been able to successfully run Porcupine wake word detection software on iOS and Android in\nbackground mode. However, this feature is controlled by the operating system, and we cannot guarantee that this will\nbe possible in future releases of iOS or Android. Please check iOS and Android guidelines, technical documentation,\nand terms of service before choosing to run Porcupine wake word detection in the background. We recommend using the\nsample demo applications made available on this repository to test this capability in your end application before\nacquiring a development or commercial license.\n\n**[Q] Which platforms does Porcupine wake word detection support?**\n\n**[A]** Porcupine wake word detection software is supported on Raspberry Pi (all models), BeagleBone, Android, iOS,\nLinux (x86_64), macOS, Windows, and modern web browsers (excluding Internet Explorer). Additionally, we have support\nfor various ARM Cortex-A and ARM Cortex-M (M4/M7) MCUs by NXP and STMicro. \n\n**[Q] What is required to support additional languages?**\n\n**[A]** Porcupine is architected to work with any language, and there are no technical limitations on supporting most\nlanguages. However, supporting a new language requires significant effort and investment. The undertaking is a business\ndecision which depends on our current priorities, pipeline, and the scale of commercial opportunity for which the\nlanguage support is required.\n\n**[Q] Does Porcupine wake word detection software work with everyone\xe2\x80\x99s voice (universal) or does it only work with my voice (personal)?**\n\n**[A]** Porcupine wake word detection software is universal and trained to work with a variety of accents and people\xe2\x80\x99s\nvoices.\n\n**[Q] Does Porcupine wake word detection work with children\xe2\x80\x99s voices?**\n\n**[A]** Porcupine may not work well with very young children as their voices are different from adult voices. We have\nmade the software available for free evaluation with a set of sample wake words. We recommend that you test the\nengine with speech of children within your target age range before acquiring a development or commercial license.\n\n**[Q] Do users need to pause and remain silent before saying the wake word?**\n\n**[A]** By default, no. But if that is a requirement, we can customize the software\n(as part of our professional services for you) to require silence either before or after the wake word.\n\n**[Q] If my wake phrase is made of two words (e.g., \xe2\x80\x9cHey Siri\xe2\x80\x9d), does the software detect if the user inserts silence/pause in between each word?**\n\n**[A]** By default, the engine ignores silence in between the words. However, if that is a requirement, we can\ncustomize the software (as part of our standard professional services) to require silence between each word.\n\n**[Q] Our marketing team is having difficulty deciding on the choice for wake word, can you help?**\n\n**[A]** Yes, we can help you with the process of choosing the right wake word for your brand. We also offer the option\nfor revision if you change your mind after the purchase of a development license.\n\n**[Q] Does Porcupine wake word detection work with accents?**\n\n**[A]** Yes, it works generally well with accents. However, it\xe2\x80\x99s impossible to objectively quantify it. We recommend you\ntry the engine for yourself and perhaps evaluate with an accented dataset of your choice to see if it meets your\nrequirements.\n\n**[Q] How does Picovoice wake word detection software work when UK and US wake word pronunciations sometimes differ?**\n\n**[A]** For words that have different pronunciations in UK and US English, like \xe2\x80\x9ctomato\xe2\x80\x9d, we recommend listening\nfor both pronunciations simultaneously with two separate wake word model files, each targeting a distinct pronunciation.\n\n**[Q] How many wake words can Porcupine detect simultaneously?**\n\n**[A]** There is no technical limit on the number of wake words the software can listen to simultaneously. \n\n**[Q] How much additional memory and CPU is needed for detecting additional wake word or trigger phrases?**\n\n**[A]** Listening to additional wake words does not increase the CPU usage. However it will require 1 KB of\nmemory per additional wake word model.\n\n**[Q] Is the Picovoice \xe2\x80\x9cAlexa\xe2\x80\x9d wake word verified by Amazon?**\n\n**[A]** Amazon Alexa Certification requirements are different for near, mid, and far-field applications\n(AVS, AMA, etc.). Also, the certification is typically performed on the end hardware, and the outcome depends on many\ndesign choices such as microphone, enclosure acoustics, audio front end, and wake word. Picovoice can assist with new\nproduct introduction (NPI) and Alexa certification under our technical support package.\n\n**[Q] Does Picovoice wake word detection software work with Google Assistant?**\n\n**[A]** Yes. However, your product may have to go through a certification procedure with Google. Please check Google\xe2\x80\x99s\nguidelines and terms of service for related information.\n\n**[Q] Can you use Picovoice wake word detection software with Cortana, IBM Watson, or Samsung Bixby?**\n\n**[A]** Yes, Picovoice can generate any third-party wake words at your request. However, you are responsible for any\nnecessary integration with such platforms and potential areas of compliance.\n\n**[Q] What\xe2\x80\x99s the power consumption of Picovoice wake word detection engine?**\n\n**[A]** The absolute power consumption (in wattage) depends on numerous factors such as processor architecture,\nvendor, fabrication technology, and system level power management design. If your design requires low power\nconsumption in the (sub) milliwatt range for always-listening wake word detection, you will likely need to consider\nMCU (ARM Cortex-M) or DSP implementation.\n\n**[Q] Can Porcupine distinguish words with similar pronunciation?**\n\n**[A]** The rigidity of rejecting words with similar pronunciation would have several side effects such as rejecting\naccented pronunciations, as well as higher rejection rate in noisy conditions. By lowering the detection\nsensitivity you can achieve lower false acceptance of words with similar pronunciations at the cost of higher miss rate.\n\n**[Q] How can I run Picovoice software on my ARM-based MPU running a Yocto customized embedded Linux?**\n\n**[A]** As part of our standard professional services, we can port our software to custom platforms for a one-time\nengineering fee and prepaid license royalties. We review these on a case-by-case basis and provide a quotation based\non the complexity and type of the platform. Please note that the port must be performed in-house by our engineering\nteam, since it requires direct access to our IP, proprietary technology, and toolchains. We would also require at\nleast one development board running your target OS to perform this task.\n\n**[Q] What is your software licensing model?**\n\n**[A]** The software published on this repository is available under Apache 2.0. If you need custom wake word models on\na specific platform for commercial development (building PoC, prototyping, or product development) you need to acquire a\ndevelopment license. To install and use Picovoice software on commercial products with custom wake word models you need\nto acquire a commercial license. If you are developing a product within a company and working towards commercialization\nplease reach out to us to acquire the appropriate license by filling out the form\n[here](https://picovoice.ai/contact.html).\n\n**[Q] Can I use wake word models generated by the [Picovoice Console](https://console.picovoice.ai) in a commercial\nproduct?**\n\n**[A]** The Picovoice Console and keyword files it generates can only be used for non-commercial and evaluation purposes.\nIf you are developing a commercial product, you must acquire a development license. To acquire a development license \nfill out the form [here](https://picovoice.ai/contact.html).\n'