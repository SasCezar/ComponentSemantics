b'[![Build Status](https://travis-ci.org/sitewhere/sitewhere.svg?branch=master)](https://travis-ci.org/sitewhere/sitewhere) \n[![Docker Pulls](https://img.shields.io/docker/pulls/sitewhere/service-web-rest.svg?label=Docker%20Pulls&style=flat-square)](https://hub.docker.com/u/sitewhere) \n\n![SiteWhere](https://s3.amazonaws.com/sitewhere-branding/SiteWhereLogo.svg)\n\n---\n\nSiteWhere is an industrial strength, open source IoT Application Enablement Platform \nwhich facilitates the ingestion, storage, processing, and integration of IoT device data \nat massive scale. The platform leverages a microservices architecture which runs on top of \ncutting-edge technologies such as [Kubernetes](https://kubernetes.io/), [Istio](https://istio.io), \nand [Kafka](https://kafka.apache.org/) in order to scale efficiently \nto the loads expected in large IoT projects. \n\nSiteWhere embraces a distributed architecture which runs on Kubernetes and provides \nboth infrastructure such as highly-available databases and MQTT brokers as well as \nmicroservices to facilitate various aspects of IoT project development. The platform is \nbuilt with a framework approach using clearly defined APIs so that new technologies may easily \nbe integrated as the IoT ecosystem evolves.\n\n![SiteWhere Administration](https://sitewhere-web.s3.amazonaws.com/github-readme/admin-ui-2.1.0.png "SiteWhere Administration")\n\n## Deployment and Orchestration\n\nSiteWhere is composed of Java-based microservices which are built as\n[Docker](https://www.docker.com/) images and deployed to Kubernetes for\norchestration. To simplify installation and configuration, [Helm](https://helm.sh/) \nis used to provide standard templates for various deployment scenarios. Helm\n[charts](https://github.com/sitewhere/sitewhere-recipes/tree/master/charts)\nare provided to supply both the microservices and the dependencies needed to \nrun a complete SiteWhere deployment. Infrastructure components include \ntechnologies such as Apache Zookeeper and Kafka, highly available databases such\nas MongoDB, InfluxDB, and Cassandra, and other supporting technologies \nsuch as MQTT brokers.\n\n## Microservices\n\nRather than using a monolithic approach, SiteWhere is based on many microservices\nrunning as a distributed system. Each microservice is a completely self-contained \nentity that has its own configuration schema, internal components, data persistence, \nand interactions with the event processing pipeline. SiteWhere microservices\nare built on top of a custom microservice framework and run as separate\n[Spring Boot](https://projects.spring.io/spring-boot/) processes, each\ncontained in its own [Docker](https://www.docker.com/) image.\n\n![SiteWhere Architecture](https://sitewhere-web.s3.amazonaws.com/github-readme/sitewhere-microservices.png "SiteWhere 2.0 Architecture")\n\n### Separation of Concerns\n\nSeparating the system logic into microservices allows the interactions\nbetween various areas of the system to be more clearly defined. It also allows\nparts of the pipeline to be shutdown or fail gracefully without preventing other\nparts of the system from functioning. The event processing pipeline, which spans\nmany of the microservices, is buffered by Kafka so that data processing has\nstrong delivery guarantees while maintaining high throughput.\n\n### Scale What You Need. Leave Out What You Don\'t\n\nThe microservice architecture allows individual functional areas of the system to be scaled\nindependently or left out completely. In use cases where REST processing tends to\nbe a bottleneck, multiple REST microservices can be run concurrently to handle the load.\nConversely, services such as presence management that may not be required can be left\nout so that processing power can be dedicated to other aspects of the system.\n\n## Instance Management\n\nSiteWhere supports the concept of an _instance_, which allows the distributed system \nto act as a cohesive unit with some aspects addressed at the global level. All of the \nmicroservices for a single SiteWhere instance must be running on the same Kubernetes \ninfrastucture, though the system may be spread across tens or hundreds of machines \nto distribute the processing load.\n\n### Service Mesh with Istio\n\nSiteWhere leverages [Istio](https://istio.io/) to provide a service mesh for\nthe system microservices, allowing the platform to be scaled dynamically while \nalso providing a great deal of control over how data is routed. Istio allows\nmodern methods such as canary testing and fault injection to be used to \nprovide a more robust and fault-tolerant system. It also allows for detailed\nmonitoring and tracing of the data flowing through the components.\n\n### Centralized Configuration Management with Apache ZooKeeper\n\nSiteWhere configuration is stored in [Apache ZooKeeper](https://zookeeper.apache.org/) \nto allow for a scalable, externalized approach to configuration management. ZooKeeper \ncontains a hierarchical structure which represents the configuration for one or more \nSiteWhere instances and all of the microservices that are used to realize them. The \nconfiguration is replicated for high availabilty.\n\nEach microservice has a direct connection to ZooKeeper and uses the hierarchy to \ndetermine its configuration at runtime. Microservices listen for changes to the \nconfiguration data and react dynamically to updates. No configuration\nis stored locally within the microservice, which prevents problems with\nkeeping services in sync as system configuration is updated.\n\n### Distributed Storage with Rook.io\n\nSince many of the system components such as Zookeeper, Kafka, and various\ndatabases require access to persistent storage, SiteWhere uses\n[Rook.io](https://rook.io/) within Kubernetes to supply distributed,\nreplicated block storage that is resilient to hardware failures while\nstill offering good performance characteristics. As storage and throughput\nneeds increase over time, new storage devices can be made available\ndynamically. The underlying [Ceph](https://ceph.com/) architecture\nused by Rook.io can handle _exobytes_ of data while allowing data\nto be resilient to failures at the node, rack, or even datacenter level.\n\n## High Performance Data Processing Pipeline\n\nThe event processing pipeline in SiteWhere uses [Apache Kafka](https://kafka.apache.org/) \nto provide a resilient, high-performance mechanism for progressively processing device \nevent data. Microservices can plug in to key points in the event processing pipeline, \nreading data from well-known inbound topics, processing data, then sending data to well-known \noutbound topics. External entites that are interested in data at any point in the pipeline \ncan act as consumers of the SiteWhere topics to use the data as it moves through the system.\n\n### Fully Asynchronous Pipeline Processing\n\nThe SiteWhere event processing pipeline leverages Kafka\'s messaging constructs to allow\ndevice event data to be processed asynchronously. If a microservice shuts down and no other \nreplicas are available to process the load. The data will be queued until a replica starts\nand begins processing again. This acts as a guarantee against data loss as data is always\nbacked by Kafka\'s high-performance storage. SiteWhere microservices leverage Kafka\'s consumer \ngroups concept to distribute load across multiple consumers and scale processing accordingly.\n\nUsing Kafka also has other advantages that are leveraged by SiteWhere. Since all data in\nthe distributed log is stored on disk, it is possible to "replay" the event stream based\non previously gathered data. This is extremely valuable for aspects such as debugging\nprocessing logic or load testing the system.\n\n## API Connectivity Between Microservices\n\nWhile device event data generally flows in a pipeline from microservice to microservice on\nKafka topics, there are also API operations that need to occur in real time between the\nmicroservices. For instance, device management and event management functions are contained in\ntheir own microservices, but are required by many other components of the system. Many of the\nSiteWhere microservices offer APIs which may be accessed by other microservices to\nsupport aspects such as storing persistent data or initiating microservice-specific\nservices.\n\n### Using gRPC for a Performance Boost\n\nRather than solely using REST services based on HTTP 1.x, which tend to have significant\nconnection overhead, SiteWhere uses [gRPC](https://grpc.io/) to establish a long-lived\nconnection between microservices that need to communicate with each other. Since gRPC uses\npersistent HTTP2 connections, the overhead for interactions is greatly reduced, allowing\nfor decoupling without a significant performance penalty. Istio also allows the gRPC\nconnections to be multiplexed across multiple replicas of a microservice to scale \nprocessing and offer redundancy.\n\nThe entire SiteWhere data model has been captured in\n[Google Protocol Buffers](https://developers.google.com/protocol-buffers/) format so that\nit can be used within GRPC services. All of the SiteWhere APIs are exposed directly as\ngRPC services as well, allowing for high-performance, low-latency access to all API\nfunctions. The REST APIs are still made available via the Web/REST microservice (acting\nas an API gateway), but they use the gRPC APIs underneath to provide a consistent approach \nto accessing data.\n\n## Multitenancy\n\nSiteWhere is designed for large-scale IoT projects which may involve many system tenants\nsharing a single SiteWhere instance. A key differentiator for SiteWhere compared to most\nIoT platforms is that each tenant runs in isolation from other tenants. By default, tenants\ndo not share database resources or pipeline processing and have a completely separate \nconfiguration lifecycles. With this approach, each tenant may use its own database \ntechnologies, external integrations, and other configuration options. Parts of the tenant\'s\nprocessing pipeline may be reconfigured/restarted without causing an interruption to \nother tenants.\n\n### Data Privacy\n\nAn important consequence of the way SiteWhere handles multitenancy is that each tenant\'s \ndata is separated from the data of other tenants. Most platforms that offer multitenancy\nstore data for all tenants in shared tables, differentiated only by a tenant id. The shared\napproach opens up the possibility of one tenant\'s data corrupting another, which is not\nan acceptable risk in many IoT deployments. In addition, each tenant has its own processing\npipelines, so in-flight data is never co-mingled either.\n\nHaving dedicated resources for tenants can be expensive in terms of memory and processing\nresources, so SiteWhere also offers the concept of _customers_ within each tenant. Customers\nallow data to be differentiated within a tenant, but without having a separate dedicated\ndatabase and pipelines. In cases where colocated data is acceptable, the tenant can have\nany number of customers, which shared the same database and processing pipeline. This allows \nthe best of both worlds in terms of security and scalability.\n\n* * * *\n\nCopyright (c) 2009-2019 [SiteWhere LLC](http://www.sitewhere.com). All rights reserved.\n'