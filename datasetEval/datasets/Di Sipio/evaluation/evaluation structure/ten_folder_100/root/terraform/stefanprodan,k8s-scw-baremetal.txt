b'# k8s-scw-baremetal\n\nKubernetes Terraform installer for Scaleway bare-metal ARM and AMD64\n\n### Initial setup\n\nClone the repository and install the dependencies:\n\n```bash\n$ git clone https://github.com/stefanprodan/k8s-scw-baremetal.git\n$ cd k8s-scw-baremetal\n$ terraform init\n```\n\nNote that you\'ll need Terraform v0.10 or newer to run this project.\n\nBefore running the project you\'ll have to create an access token for Terraform to connect to the Scaleway API\n\nNow retrieve the `<ORGANIZATION_ID>` using your `<ACCESS-TOKEN>` from `/organizations` API endpoint:\n\n```bash\n$ curl https://account.scaleway.com/organizations -H "X-Auth-Token: <ACCESS-TOKEN>"\n```\n\nSample output (excerpt with organization ID):\n```bash\n"organizations": [{"id": "xxxxxxxxxxxxx", "name": "Organization Name"}],\n```\n\nUsing the token and your organization ID, create two environment variables:\n\n```bash\n$ export SCALEWAY_ORGANIZATION="<ORGANIZATION_ID>"\n$ export SCALEWAY_TOKEN="<ACCESS-TOKEN>"\n```\n\nTo configure your cluster, you\'ll need to have `jq` installed on your computer.\n\n### Usage\n\nCreate an AMD64 bare-metal Kubernetes cluster with one master and a node:\n\n```bash\n$ terraform workspace new amd64\n\n$ terraform apply \\\n -var region=par1 \\\n -var arch=x86_64 \\\n -var server_type=C2S \\\n -var nodes=1 \\\n -var server_type_node=C2S \\\n -var weave_passwd=ChangeMe \\\n -var docker_version=18.06 \\\n -var ubuntu_version="Ubuntu Bionic"\n```\n\nThis will do the following:\n\n* reserves public IPs for each server\n* provisions three bare-metal servers with Ubuntu 16.04.1 LTS (the size of the `master` and the `node` may be different but must remain in the same type of architecture)\n* connects to the master server via SSH and installs Docker CE and kubeadm apt packages\n* runs kubeadm init on the master server and configures kubectl\n* downloads the kubectl admin config file on your local machine and replaces the private IP with the public one\n* creates a Kubernetes secret with the Weave Net password\n* installs Weave Net with encrypted overlay\n* installs cluster add-ons (Kubernetes dashboard, metrics server and Heapster)\n* starts the nodes in parallel and installs Docker CE and kubeadm\n* joins the nodes in the cluster using the kubeadm token obtained from the master\n\nScale up by increasing the number of nodes:\n\n```bash\n$ terraform apply \\\n -var nodes=3\n```\n\nTear down the whole infrastructure with:\n\n```bash\nterraform destroy -force\n```\n\nCreate an ARMv7 bare-metal Kubernetes cluster with one master and two nodes:\n\n```bash\n$ terraform workspace new arm\n\n$ terraform apply \\\n -var region=par1 \\\n -var arch=arm \\\n -var server_type=C1 \\\n -var nodes=2 \\\n -var server_type_node=C1 \\\n -var weave_passwd=ChangeMe \\\n -var docker_version=18.06 \\\n -var ubuntu_version="Ubuntu Xenial"\n```\n\n### Remote control\n\nAfter applying the Terraform plan you\'ll see several output variables like the master public IP,\nthe kubeadmn join command and the current workspace admin config.\n\nIn order to run `kubectl` commands against the Scaleway cluster you can use the `kubectl_config` output variable:\n\nCheck if Heapster works:\n\n```bash\n$ kubectl --kubeconfig ./$(terraform output kubectl_config) \\\n  top nodes\n\nNAME           CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%\narm-master-1   655m         16%       873Mi           45%\narm-node-1     147m         3%        618Mi           32%\narm-node-2     101m         2%        584Mi           30%\n```\n\nThe `kubectl` config file format is `<WORKSPACE>.conf` as in `arm.conf` or `amd64.conf`.\n\nIn order to access the dashboard you can use port forward:\n\n```bash\n$ kubectl --kubeconfig ./$(terraform output kubectl_config) \\\n  -n kube-system port-forward deployment/kubernetes-dashboard 8888:9090\n```\n\nNow you can access the dashboard on your computer at `http://localhost:8888`.\n\n![Overview](https://github.com/stefanprodan/k8s-scw-baremetal/blob/master/screens/dash-overview.png)\n\n![Nodes](https://github.com/stefanprodan/k8s-scw-baremetal/blob/master/screens/dash-nodes.png)\n\n### Expose services outside the cluster\n\nSince we\'re running on bare-metal and Scaleway doesn\'t offer a load balancer, the easiest way to expose\napplications outside of Kubernetes is using a NodePort service.\n\nLet\'s deploy the [podinfo](https://github.com/stefanprodan/k8s-podinfo) app in the default namespace.\nPodinfo has a multi-arch Docker image and it will work on arm, arm64 or amd64.\n\nCreate the podinfo nodeport service:\n\n```bash\n$ kubectl --kubeconfig ./$(terraform output kubectl_config) \\\n  apply -f https://raw.githubusercontent.com/stefanprodan/k8s-podinfo/7a8506e60fca086572f16de57f87bf5430e2df48/deploy/podinfo-svc-nodeport.yaml\n \nservice "podinfo-nodeport" created\n```\n\nCreate the podinfo deployment:\n\n```bash\n$ kubectl --kubeconfig ./$(terraform output kubectl_config) \\\n  apply -f https://raw.githubusercontent.com/stefanprodan/k8s-podinfo/7a8506e60fca086572f16de57f87bf5430e2df48/deploy/podinfo-dep.yaml\n\ndeployment "podinfo" created\n```\n\nInspect the podinfo service to obtain the port number:\n\n```bash\n$ kubectl --kubeconfig ./$(terraform output kubectl_config) \\\n  get svc --selector=app=podinfo\n\nNAME               TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\npodinfo-nodeport   NodePort   10.104.132.14   <none>        9898:31190/TCP   3m\n```\n\nYou can access podinfo at `http://<MASTER_PUBLIC_IP>:31190` or using curl:\n\n```bash\n$ curl http://$(terraform output k8s_master_public_ip):31190\n\nruntime:\n  arch: arm\n  max_procs: "4"\n  num_cpu: "4"\n  num_goroutine: "12"\n  os: linux\n  version: go1.9.2\nlabels:\n  app: podinfo\n  pod-template-hash: "1847780700"\nannotations:\n  kubernetes.io/config.seen: 2018-01-08T00:39:45.580597397Z\n  kubernetes.io/config.source: api\nenvironment:\n  HOME: /root\n  HOSTNAME: podinfo-5d8ccd4c44-zrczc\n  KUBERNETES_PORT: tcp://10.96.0.1:443\n  KUBERNETES_PORT_443_TCP: tcp://10.96.0.1:443\n  KUBERNETES_PORT_443_TCP_ADDR: 10.96.0.1\n  KUBERNETES_PORT_443_TCP_PORT: "443"\n  KUBERNETES_PORT_443_TCP_PROTO: tcp\n  KUBERNETES_SERVICE_HOST: 10.96.0.1\n  KUBERNETES_SERVICE_PORT: "443"\n  KUBERNETES_SERVICE_PORT_HTTPS: "443"\n  PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nexternalIP:\n  IPv4: 163.172.139.112\n```\n\n### Horizontal Pod Autoscaling\n\nStarting from Kubernetes 1.9 `kube-controller-manager` is configured by default with\n`horizontal-pod-autoscaler-use-rest-clients`.\nIn order to use HPA we need to install the metrics server to enable the new metrics API used by HPA v2.\nBoth Heapster and the metrics server have been deployed from Terraform\nwhen the master node was provisioned.\n\nThe metric server collects resource usage data from each node using Kubelet Summary API.\nCheck if the metrics server is running:\n\n```bash\n$ kubectl --kubeconfig ./$(terraform output kubectl_config) \\\n get --raw "/apis/metrics.k8s.io/v1beta1/nodes" | jq\n```\n\n```json\n{\n  "kind": "NodeMetricsList",\n  "apiVersion": "metrics.k8s.io/v1beta1",\n  "metadata": {\n    "selfLink": "/apis/metrics.k8s.io/v1beta1/nodes"\n  },\n  "items": [\n    {\n      "metadata": {\n        "name": "arm-master-1",\n        "selfLink": "/apis/metrics.k8s.io/v1beta1/nodes/arm-master-1",\n        "creationTimestamp": "2018-01-08T15:17:09Z"\n      },\n      "timestamp": "2018-01-08T15:17:00Z",\n      "window": "1m0s",\n      "usage": {\n        "cpu": "384m",\n        "memory": "935792Ki"\n      }\n    },\n    {\n      "metadata": {\n        "name": "arm-node-1",\n        "selfLink": "/apis/metrics.k8s.io/v1beta1/nodes/arm-node-1",\n        "creationTimestamp": "2018-01-08T15:17:09Z"\n      },\n      "timestamp": "2018-01-08T15:17:00Z",\n      "window": "1m0s",\n      "usage": {\n        "cpu": "130m",\n        "memory": "649020Ki"\n      }\n    },\n    {\n      "metadata": {\n        "name": "arm-node-2",\n        "selfLink": "/apis/metrics.k8s.io/v1beta1/nodes/arm-node-2",\n        "creationTimestamp": "2018-01-08T15:17:09Z"\n      },\n      "timestamp": "2018-01-08T15:17:00Z",\n      "window": "1m0s",\n      "usage": {\n        "cpu": "120m",\n        "memory": "614180Ki"\n      }\n    }\n  ]\n}\n```\n\nLet\'s define a HPA that will maintain a minimum of two replicas and will scale up to ten\nif the CPU average is over 80% or if the memory goes over 200Mi.\n\n```yaml\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: podinfo\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1beta1\n    kind: Deployment\n    name: podinfo\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      targetAverageUtilization: 80\n  - type: Resource\n    resource:\n      name: memory\n      targetAverageValue: 200Mi\n```\n\nApply the podinfo HPA:\n\n```bash\n$ kubectl --kubeconfig ./$(terraform output kubectl_config) \\\n  apply -f https://raw.githubusercontent.com/stefanprodan/k8s-podinfo/7a8506e60fca086572f16de57f87bf5430e2df48/deploy/podinfo-hpa.yaml\n\nhorizontalpodautoscaler "podinfo" created\n```\n\nAfter a couple of seconds the HPA controller will contact the metrics server and will fetch the CPU\nand memory usage:\n\n```bash\n$ kubectl --kubeconfig ./$(terraform output kubectl_config) get hpa\n\nNAME      REFERENCE            TARGETS                      MINPODS   MAXPODS   REPLICAS   AGE\npodinfo   Deployment/podinfo   2826240 / 200Mi, 15% / 80%   2         10        2          5m\n```\n\nIn order to increase the CPU usage we could run a load test with hey:\n\n```bash\n#install hey\ngo get -u github.com/rakyll/hey\n\n#do 10K requests rate limited at 20 QPS\nhey -n 10000 -q 10 -c 5 http://$(terraform output k8s_master_public_ip):31190\n```\n\nYou can monitor the autoscaler events with:\n\n```bash\n$ watch -n 5 kubectl --kubeconfig ./$(terraform output kubectl_config) describe hpa\n\nEvents:\n  Type    Reason             Age   From                       Message\n  ----    ------             ----  ----                       -------\n  Normal  SuccessfulRescale  7m    horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target\n  Normal  SuccessfulRescale  3m    horizontal-pod-autoscaler  New size: 8; reason: cpu resource utilization (percentage of request) above target\n```\n\nAfter the load tests finishes the autoscaler will remove replicas until the deployment reaches the initial replica count:\n\n```\nEvents:\n  Type    Reason             Age   From                       Message\n  ----    ------             ----  ----                       -------\n  Normal  SuccessfulRescale  20m   horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target\n  Normal  SuccessfulRescale  16m   horizontal-pod-autoscaler  New size: 8; reason: cpu resource utilization (percentage of request) above target\n  Normal  SuccessfulRescale  12m   horizontal-pod-autoscaler  New size: 10; reason: cpu resource utilization (percentage of request) above target\n  Normal  SuccessfulRescale  6m    horizontal-pod-autoscaler  New size: 2; reason: All metrics below target\n```\n'