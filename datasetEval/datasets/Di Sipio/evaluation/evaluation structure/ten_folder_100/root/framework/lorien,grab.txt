b'Grab\n====\n\n.. image:: https://travis-ci.org/lorien/grab.png?branch=master\n    :target: https://travis-ci.org/lorien/grab?branch=master\n\n.. image:: https://ci.appveyor.com/api/projects/status/uxj24vjin7gptdlg\n    :target: https://ci.appveyor.com/project/lorien/grab\n\n.. image:: https://coveralls.io/repos/lorien/grab/badge.svg?branch=master\n    :target: https://coveralls.io/r/lorien/grab?branch=master\n\n.. image:: https://api.codacy.com/project/badge/Grade/18465ca1458b4c5e99026aafa5b58e98\n   :target: https://www.codacy.com/app/lorien/grab?utm_source=github.com&utm_medium=referral&utm_content=lorien/grab&utm_campaign=badger\n\n.. image:: https://readthedocs.org/projects/grab/badge/?version=latest\n    :target: http://docs.grablib.org/en/latest/\n\n\nInstallation\n------------\n\n.. code:: bash\n\n    $ pip install -U grab\n\nSee details about installing Grab on different platforms here http://docs.grablib.org/en/latest/usage/installation.html\n\n\nSupport\n-------\n\nDocumentation: https://grablib.org/en/latest/\n\nTelegram Chat: https://t.me/grablab\n\nTo report bug please use GitHub issue tracker: https://github.com/lorien/grab/issues\n\n\nWhat is Grab?\n-------------\n\nGrab is a python web scraping framework. Grab provides a number of helpful methods\nto perform network requests, scrape web sites and process the scraped content:\n\n* Automatic cookies (session) support\n* HTTP and SOCKS proxy with/without authorization\n* Keep-Alive support\n* IDN support\n* Tools to work with web forms\n* Easy multipart file uploading\n* Flexible customization of HTTP requests\n* Automatic charset detection\n* Powerful API to extract data from DOM tree of HTML documents with XPATH queries\n* Asynchronous API to make thousands of simultaneous queries. This part of\n  library called Spider. See list of spider fetures below.\n* Python 3 ready\n\nSpider is a framework for writing web-site scrapers. Features:\n\n* Rules and conventions to organize the request/parse logic in separate\n  blocks of codes\n* Multiple parallel network requests\n* Automatic processing of network errors (failed tasks go back to task queue)\n* You can create network requests and parse responses with Grab API (see above)\n* HTTP proxy support\n* Caching network results in permanent storage\n* Different backends for task queue (in-memory, redis, mongodb)\n* Tools to debug and collect statistics\n\n\nGrab Example\n------------\n\n.. code:: python\n\n    import logging\n\n    from grab import Grab\n\n    logging.basicConfig(level=logging.DEBUG)\n\n    g = Grab()\n\n    g.go(\'https://github.com/login\')\n    g.doc.set_input(\'login\', \'****\')\n    g.doc.set_input(\'password\', \'****\')\n    g.doc.submit()\n\n    g.doc.save(\'/tmp/x.html\')\n\n    g.doc(\'//ul[@id="user-links"]//button[contains(@class, "signout")]\').assert_exists()\n\n    home_url = g.doc(\'//a[contains(@class, "header-nav-link name")]/@href\').text()\n    repo_url = home_url + \'?tab=repositories\'\n\n    g.go(repo_url)\n\n    for elem in g.doc.select(\'//h3[@class="repo-list-name"]/a\'):\n        print(\'%s: %s\' % (elem.text(),\n                          g.make_url_absolute(elem.attr(\'href\'))))\n\n\nGrab::Spider Example\n--------------------\n\n.. code:: python\n\n    import logging\n\n    from grab.spider import Spider, Task\n\n    logging.basicConfig(level=logging.DEBUG)\n\n\n    class ExampleSpider(Spider):\n        def task_generator(self):\n            for lang in \'python\', \'ruby\', \'perl\':\n                url = \'https://www.google.com/search?q=%s\' % lang\n                yield Task(\'search\', url=url, lang=lang)\n\n        def task_search(self, grab, task):\n            print(\'%s: %s\' % (task.lang,\n                              grab.doc(\'//div[@class="s"]//cite\').text()))\n\n\n    bot = ExampleSpider(thread_number=2)\n    bot.run()\n\n\n\n'