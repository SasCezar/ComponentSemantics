b'# BadgerDB [![GoDoc](https://godoc.org/github.com/dgraph-io/badger?status.svg)](https://godoc.org/github.com/dgraph-io/badger) [![Go Report Card](https://goreportcard.com/badge/github.com/dgraph-io/badger)](https://goreportcard.com/report/github.com/dgraph-io/badger) [![Sourcegraph](https://sourcegraph.com/github.com/dgraph-io/badger/-/badge.svg)](https://sourcegraph.com/github.com/dgraph-io/badger?badge) [![Build Status](https://teamcity.dgraph.io/guestAuth/app/rest/builds/buildType:(id:Badger_UnitTests)/statusIcon.svg)](https://teamcity.dgraph.io/viewLog.html?buildTypeId=Badger_UnitTests&buildId=lastFinished&guest=1) ![Appveyor](https://ci.appveyor.com/api/projects/status/github/dgraph-io/badger?branch=master&svg=true) [![Coverage Status](https://coveralls.io/repos/github/dgraph-io/badger/badge.svg?branch=master)](https://coveralls.io/github/dgraph-io/badger?branch=master)\n\n![Badger mascot](images/diggy-shadow.png)\n\nBadgerDB is an embeddable, persistent and fast key-value (KV) database written\nin pure Go. It is the underlying database for [Dgraph](https://dgraph.io), a\nfast, distributed graph database. It\'s meant to be a performant alternative to\nnon-Go-based key-value stores like RocksDB.\n\n## Project Status [Jun 26, 2019]\n\nBadger is stable and is being used to serve data sets worth hundreds of\nterabytes. Badger supports concurrent ACID transactions with serializable\nsnapshot isolation (SSI) guarantees. A Jepsen-style bank test runs nightly for\n8h, with `--race` flag and ensures the maintenance of transactional guarantees.\nBadger has also been tested to work with filesystem level anomalies, to ensure\npersistence and consistency.\n\nBadger v1.0 was released in Nov 2017, and the latest version that is data-compatible\nwith v1.0 is v1.6.0.\n\nBadger v2.0, a new release coming up very soon will use a new storage format which won\'t\nbe compatible with all of the v1.x. The [Changelog] is kept fairly up-to-date.\n\nFor more details on our version naming schema please read [Choosing a version](#choosing-a-version).\n\n[Changelog]:https://github.com/dgraph-io/badger/blob/master/CHANGELOG.md\n\n## Table of Contents\n * [Getting Started](#getting-started)\n    + [Installing](#installing)\n      - [Choosing a version](#choosing-a-version)\n    + [Opening a database](#opening-a-database)\n    + [Transactions](#transactions)\n      - [Read-only transactions](#read-only-transactions)\n      - [Read-write transactions](#read-write-transactions)\n      - [Managing transactions manually](#managing-transactions-manually)\n    + [Using key/value pairs](#using-keyvalue-pairs)\n    + [Monotonically increasing integers](#monotonically-increasing-integers)\n    * [Merge Operations](#merge-operations)\n    + [Setting Time To Live(TTL) and User Metadata on Keys](#setting-time-to-livettl-and-user-metadata-on-keys)\n    + [Iterating over keys](#iterating-over-keys)\n      - [Prefix scans](#prefix-scans)\n      - [Key-only iteration](#key-only-iteration)\n    + [Stream](#stream)\n    + [Garbage Collection](#garbage-collection)\n    + [Database backup](#database-backup)\n    + [Memory usage](#memory-usage)\n    + [Statistics](#statistics)\n  * [Resources](#resources)\n    + [Blog Posts](#blog-posts)\n  * [Contact](#contact)\n  * [Design](#design)\n    + [Comparisons](#comparisons)\n    + [Benchmarks](#benchmarks)\n  * [Other Projects Using Badger](#other-projects-using-badger)\n  * [Frequently Asked Questions](#frequently-asked-questions)\n\n## Getting Started\n\n### Installing\nTo start using Badger, install Go 1.11 or above and run `go get`:\n\n```sh\n$ go get github.com/dgraph-io/badger/...\n```\n\nThis will retrieve the library and install the `badger` command line\nutility into your `$GOBIN` path.\n\n##### Note: Badger does not directly use CGO but it relies on https://github.com/DataDog/zstd for compression and it requires gcc/cgo. If you wish to use badger without gcc/cgo, you can run `CGO_ENABLED=0 go get github.com/dgraph-io/badger/...` which will download badger without the support for ZSTD compression algorithm.\n\n#### Choosing a version\n\nBadgerDB is a pretty special package from the point of view that the most important change we can\nmake to it is not on its API but rather on how data is stored on disk.\n\nThis is why we follow a version naming schema that differs from Semantic Versioning.\n\n- New major versions are released when the data format on disk changes in an incompatible way.\n- New minor versions are released whenever the API changes but data compatibility is maintained.\n Note that the changes on the API could be backward-incompatible - unlike Semantic Versioning.\n- New patch versions are released when there\'s no changes to the data format nor the API.\n\nFollowing these rules:\n\n- v1.5.0 and v1.6.0 can be used on top of the same files without any concerns, as their major\n version is the same, therefore the data format on disk is compatible.\n- v1.6.0 and v2.0.0 are data incompatible as their major version implies, so files created with\n v1.6.0 will need to be converted into the new format before they can be used by v2.0.0.\n\nFor a longer explanation on the reasons behind using a new versioning naming schema, you can read\n[VERSIONING.md](VERSIONING.md).\n\n### Opening a database\nThe top-level object in Badger is a `DB`. It represents multiple files on disk\nin specific directories, which contain the data for a single database.\n\nTo open your database, use the `badger.Open()` function, with the appropriate\noptions. The `Dir` and `ValueDir` options are mandatory and must be\nspecified by the client. They can be set to the same value to simplify things.\n\n```go\npackage main\n\nimport (\n\t"log"\n\n\tbadger "github.com/dgraph-io/badger"\n)\n\nfunc main() {\n  // Open the Badger database located in the /tmp/badger directory.\n  // It will be created if it doesn\'t exist.\n  db, err := badger.Open(badger.DefaultOptions("/tmp/badger"))\n  if err != nil {\n\t  log.Fatal(err)\n  }\n  defer db.Close()\n \xc2\xa0// Your code here\xe2\x80\xa6\n}\n```\n\nPlease note that Badger obtains a lock on the directories so multiple processes\ncannot open the same database at the same time.\n\n### Transactions\n\n#### Read-only transactions\nTo start a read-only transaction, you can use the `DB.View()` method:\n\n```go\nerr := db.View(func(txn *badger.Txn) error {\n \xc2\xa0// Your code here\xe2\x80\xa6\n \xc2\xa0return nil\n})\n```\n\nYou cannot perform any writes or deletes within this transaction. Badger\nensures that you get a consistent view of the database within this closure. Any\nwrites that happen elsewhere after the transaction has started, will not be\nseen by calls made within the closure.\n\n#### Read-write transactions\nTo start a read-write transaction, you can use the `DB.Update()` method:\n\n```go\nerr := db.Update(func(txn *badger.Txn) error {\n \xc2\xa0// Your code here\xe2\x80\xa6\n \xc2\xa0return nil\n})\n```\n\nAll database operations are allowed inside a read-write transaction.\n\nAlways check the returned error value. If you return an error\nwithin your closure it will be passed through.\n\nAn `ErrConflict` error will be reported in case of a conflict. Depending on the state\nof your application, you have the option to retry the operation if you receive\nthis error.\n\nAn `ErrTxnTooBig` will be reported in case the number of pending writes/deletes in\nthe transaction exceeds a certain limit. In that case, it is best to commit the\ntransaction and start a new transaction immediately. Here is an example (we are\nnot checking for errors in some places for simplicity):\n\n```go\nupdates := make(map[string]string)\ntxn := db.NewTransaction(true)\nfor k,v := range updates {\n  if err := txn.Set([]byte(k),[]byte(v)); err == badger.ErrTxnTooBig {\n    _ = txn.Commit()\n    txn = db.NewTransaction(true)\n    _ = txn.Set([]byte(k),[]byte(v))\n  }\n}\n_ = txn.Commit()\n```\n\n#### Managing transactions manually\nThe `DB.View()` and `DB.Update()` methods are wrappers around the\n`DB.NewTransaction()` and `Txn.Commit()` methods (or `Txn.Discard()` in case of\nread-only transactions). These helper methods will start the transaction,\nexecute a function, and then safely discard your transaction if an error is\nreturned. This is the recommended way to use Badger transactions.\n\nHowever, sometimes you may want to manually create and commit your\ntransactions. You can use the `DB.NewTransaction()` function directly, which\ntakes in a boolean argument to specify whether a read-write transaction is\nrequired. For read-write transactions, it is necessary to call `Txn.Commit()`\nto ensure the transaction is committed. For read-only transactions, calling\n`Txn.Discard()` is sufficient. `Txn.Commit()` also calls `Txn.Discard()`\ninternally to cleanup the transaction, so just calling `Txn.Commit()` is\nsufficient for read-write transaction. However, if your code doesn\xe2\x80\x99t call\n`Txn.Commit()` for some reason (for e.g it returns prematurely with an error),\nthen please make sure you call `Txn.Discard()` in a `defer` block. Refer to the\ncode below.\n\n```go\n// Start a writable transaction.\ntxn := db.NewTransaction(true)\ndefer txn.Discard()\n\n// Use the transaction...\nerr := txn.Set([]byte("answer"), []byte("42"))\nif err != nil {\n    return err\n}\n\n// Commit the transaction and check for error.\nif err := txn.Commit(); err != nil {\n    return err\n}\n```\n\nThe first argument to `DB.NewTransaction()` is a boolean stating if the transaction\nshould be writable.\n\nBadger allows an optional callback to the `Txn.Commit()` method. Normally, the\ncallback can be set to `nil`, and the method will return after all the writes\nhave succeeded. However, if this callback is provided, the `Txn.Commit()`\nmethod returns as soon as it has checked for any conflicts. The actual writing\nto the disk happens asynchronously, and the callback is invoked once the\nwriting has finished, or an error has occurred. This can improve the throughput\nof the application in some cases. But it also means that a transaction is not\ndurable until the callback has been invoked with a `nil` error value.\n\n### Using key/value pairs\nTo save a key/value pair, use the `Txn.Set()` method:\n\n```go\nerr := db.Update(func(txn *badger.Txn) error {\n  err := txn.Set([]byte("answer"), []byte("42"))\n  return err\n})\n```\n\nKey/Value pair can also be saved by first creating `Entry`, then setting this\n`Entry` using `Txn.SetEntry()`. `Entry` also exposes methods to set properties\non it.\n\n```go\nerr := db.Update(func(txn *badger.Txn) error {\n  e := NewEntry([]byte("answer"), []byte("42"))\n  err := txn.SetEntry(e)\n  return err\n})\n```\n\nThis will set the value of the `"answer"` key to `"42"`. To retrieve this\nvalue, we can use the `Txn.Get()` method:\n\n```go\nerr := db.View(func(txn *badger.Txn) error {\n  item, err := txn.Get([]byte("answer"))\n  handle(err)\n\n  var valNot, valCopy []byte\n  err := item.Value(func(val []byte) error {\n    // This func with val would only be called if item.Value encounters no error.\n\n    // Accessing val here is valid.\n    fmt.Printf("The answer is: %s\\n", val)\n\n    // Copying or parsing val is valid.\n    valCopy = append([]byte{}, val...)\n\n    // Assigning val slice to another variable is NOT OK.\n    valNot = val // Do not do this.\n    return nil\n  })\n  handle(err)\n\n  // DO NOT access val here. It is the most common cause of bugs.\n  fmt.Printf("NEVER do this. %s\\n", valNot)\n\n  // You must copy it to use it outside item.Value(...).\n  fmt.Printf("The answer is: %s\\n", valCopy)\n\n  // Alternatively, you could also use item.ValueCopy().\n  valCopy, err = item.ValueCopy(nil)\n  handle(err)\n  fmt.Printf("The answer is: %s\\n", valCopy)\n\n  return nil\n})\n```\n\n`Txn.Get()` returns `ErrKeyNotFound` if the value is not found.\n\nPlease note that values returned from `Get()` are only valid while the\ntransaction is open. If you need to use a value outside of the transaction\nthen you must use `copy()` to copy it to another byte slice.\n\nUse the `Txn.Delete()` method to delete a key.\n\n### Monotonically increasing integers\n\nTo get unique monotonically increasing integers with strong durability, you can\nuse the `DB.GetSequence` method. This method returns a `Sequence` object, which\nis thread-safe and can be used concurrently via various goroutines.\n\nBadger would lease a range of integers to hand out from memory, with the\nbandwidth provided to `DB.GetSequence`. The frequency at which disk writes are\ndone is determined by this lease bandwidth and the frequency of `Next`\ninvocations. Setting a bandwidth too low would do more disk writes, setting it\ntoo high would result in wasted integers if Badger is closed or crashes.\nTo avoid wasted integers, call `Release` before closing Badger.\n\n```go\nseq, err := db.GetSequence(key, 1000)\ndefer seq.Release()\nfor {\n  num, err := seq.Next()\n}\n```\n\n### Merge Operations\nBadger provides support for ordered merge operations. You can define a func\nof type `MergeFunc` which takes in an existing value, and a value to be\n_merged_ with it. It returns a new value which is the result of the _merge_\noperation. All values are specified in byte arrays. For e.g., here is a merge\nfunction (`add`) which appends a  `[]byte` value to an existing `[]byte` value.\n\n```Go\n// Merge function to append one byte slice to another\nfunc add(originalValue, newValue []byte) []byte {\n  return append(originalValue, newValue...)\n}\n```\n\nThis function can then be passed to the `DB.GetMergeOperator()` method, along\nwith a key, and a duration value. The duration specifies how often the merge\nfunction is run on values that have been added using the `MergeOperator.Add()`\nmethod.\n\n`MergeOperator.Get()` method can be used to retrieve the cumulative value of the key\nassociated with the merge operation.\n\n```Go\nkey := []byte("merge")\n\nm := db.GetMergeOperator(key, add, 200*time.Millisecond)\ndefer m.Stop()\n\nm.Add([]byte("A"))\nm.Add([]byte("B"))\nm.Add([]byte("C"))\n\nres, _ := m.Get() // res should have value ABC encoded\n```\n\nExample: Merge operator which increments a counter\n\n```Go\nfunc uint64ToBytes(i uint64) []byte {\n  var buf [8]byte\n  binary.BigEndian.PutUint64(buf[:], i)\n  return buf[:]\n}\n\nfunc bytesToUint64(b []byte) uint64 {\n  return binary.BigEndian.Uint64(b)\n}\n\n// Merge function to add two uint64 numbers\nfunc add(existing, new []byte) []byte {\n  return uint64ToBytes(bytesToUint64(existing) + bytesToUint64(new))\n}\n```\nIt can be used as\n```Go\nkey := []byte("merge")\n\nm := db.GetMergeOperator(key, add, 200*time.Millisecond)\ndefer m.Stop()\n\nm.Add(uint64ToBytes(1))\nm.Add(uint64ToBytes(2))\nm.Add(uint64ToBytes(3))\n\nres, _ := m.Get() // res should have value 6 encoded\n```\n\n### Setting Time To Live(TTL) and User Metadata on Keys\nBadger allows setting an optional Time to Live (TTL) value on keys. Once the TTL has\nelapsed, the key will no longer be retrievable and will be eligible for garbage\ncollection. A TTL can be set as a `time.Duration` value using the `Entry.WithTTL()`\nand `Txn.SetEntry()` API methods.\n\n```go\nerr := db.Update(func(txn *badger.Txn) error {\n  e := NewEntry([]byte("answer"), []byte("42")).WithTTL(time.Hour)\n  err := txn.SetEntry(e)\n  return err\n})\n```\n\nAn optional user metadata value can be set on each key. A user metadata value\nis represented by a single byte. It can be used to set certain bits along\nwith the key to aid in interpreting or decoding the key-value pair. User\nmetadata can be set using `Entry.WithMeta()` and `Txn.SetEntry()` API methods.\n\n```go\nerr := db.Update(func(txn *badger.Txn) error {\n  e := NewEntry([]byte("answer"), []byte("42")).WithMeta(byte(1))\n  err := txn.SetEntry(e)\n  return err\n})\n```\n\n`Entry` APIs can be used to add the user metadata and TTL for same key. This `Entry`\nthen can be set using `Txn.SetEntry()`.\n\n```go\nerr := db.Update(func(txn *badger.Txn) error {\n  e := NewEntry([]byte("answer"), []byte("42")).WithMeta(byte(1)).WithTTL(time.Hour)\n  err := txn.SetEntry(e)\n  return err\n})\n```\n\n### Iterating over keys\nTo iterate over keys, we can use an `Iterator`, which can be obtained using the\n`Txn.NewIterator()` method. Iteration happens in byte-wise lexicographical sorting\norder.\n\n\n```go\nerr := db.View(func(txn *badger.Txn) error {\n  opts := badger.DefaultIteratorOptions\n  opts.PrefetchSize = 10\n  it := txn.NewIterator(opts)\n  defer it.Close()\n  for it.Rewind(); it.Valid(); it.Next() {\n    item := it.Item()\n    k := item.Key()\n    err := item.Value(func(v []byte) error {\n      fmt.Printf("key=%s, value=%s\\n", k, v)\n      return nil\n    })\n    if err != nil {\n      return err\n    }\n  }\n  return nil\n})\n```\n\nThe iterator allows you to move to a specific point in the list of keys and move\nforward or backward through the keys one at a time.\n\nBy default, Badger prefetches the values of the next 100 items. You can adjust\nthat with the `IteratorOptions.PrefetchSize` field. However, setting it to\na value higher than `GOMAXPROCS` (which we recommend to be 128 or higher)\nshouldn\xe2\x80\x99t give any additional benefits. You can also turn off the fetching of\nvalues altogether. See section below on key-only iteration.\n\n#### Prefix scans\nTo iterate over a key prefix, you can combine `Seek()` and `ValidForPrefix()`:\n\n```go\ndb.View(func(txn *badger.Txn) error {\n  it := txn.NewIterator(badger.DefaultIteratorOptions)\n  defer it.Close()\n  prefix := []byte("1234")\n  for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() {\n    item := it.Item()\n    k := item.Key()\n    err := item.Value(func(v []byte) error {\n      fmt.Printf("key=%s, value=%s\\n", k, v)\n      return nil\n    })\n    if err != nil {\n      return err\n    }\n  }\n  return nil\n})\n```\n\n#### Key-only iteration\nBadger supports a unique mode of iteration called _key-only_ iteration. It is\nseveral order of magnitudes faster than regular iteration, because it involves\naccess to the LSM-tree only, which is usually resident entirely in RAM. To\nenable key-only iteration, you need to set the `IteratorOptions.PrefetchValues`\nfield to `false`. This can also be used to do sparse reads for selected keys\nduring an iteration, by calling `item.Value()` only when required.\n\n```go\nerr := db.View(func(txn *badger.Txn) error {\n  opts := badger.DefaultIteratorOptions\n  opts.PrefetchValues = false\n  it := txn.NewIterator(opts)\n  defer it.Close()\n  for it.Rewind(); it.Valid(); it.Next() {\n    item := it.Item()\n    k := item.Key()\n    fmt.Printf("key=%s\\n", k)\n  }\n  return nil\n})\n```\n\n### Stream\nBadger provides a Stream framework, which concurrently iterates over all or a\nportion of the DB, converting data into custom key-values, and streams it out\nserially to be sent over network, written to disk, or even written back to\nBadger. This is a lot faster way to iterate over Badger than using a single\nIterator. Stream supports Badger in both managed and normal mode.\n\nStream uses the natural boundaries created by SSTables within the LSM tree, to\nquickly generate key ranges. Each goroutine then picks a range and runs an\niterator to iterate over it. Each iterator iterates over all versions of values\nand is created from the same transaction, thus working over a snapshot of the\nDB. Every time a new key is encountered, it calls `ChooseKey(item)`, followed\nby `KeyToList(key, itr)`. This allows a user to select or reject that key, and\nif selected, convert the value versions into custom key-values. The goroutine\nbatches up 4MB worth of key-values, before sending it over to a channel.\nAnother goroutine further batches up data from this channel using *smart\nbatching* algorithm and calls `Send` serially.\n\nThis framework is designed for high throughput key-value iteration, spreading\nthe work of iteration across many goroutines. `DB.Backup` uses this framework to\nprovide full and incremental backups quickly.  Dgraph is a heavy user of this\nframework.  In fact, this framework was developed and used within Dgraph, before\ngetting ported over to Badger.\n\n```go\nstream := db.NewStream()\n// db.NewStreamAt(readTs) for managed mode.\n\n// -- Optional settings\nstream.NumGo = 16                     // Set number of goroutines to use for iteration.\nstream.Prefix = []byte("some-prefix") // Leave nil for iteration over the whole DB.\nstream.LogPrefix = "Badger.Streaming" // For identifying stream logs. Outputs to Logger.\n\n// ChooseKey is called concurrently for every key. If left nil, assumes true by default.\nstream.ChooseKey = func(item *badger.Item) bool {\n  return bytes.HasSuffix(item.Key(), []byte("er"))\n}\n\n// KeyToList is called concurrently for chosen keys. This can be used to convert\n// Badger data into custom key-values. If nil, uses stream.ToList, a default\n// implementation, which picks all valid key-values.\nstream.KeyToList = nil\n\n// -- End of optional settings.\n\n// Send is called serially, while Stream.Orchestrate is running.\nstream.Send = func(list *pb.KVList) error {\n  return proto.MarshalText(w, list) // Write to w.\n}\n\n// Run the stream\nif err := stream.Orchestrate(context.Background()); err != nil {\n  return err\n}\n// Done.\n```\n\n### Garbage Collection\nBadger values need to be garbage collected, because of two reasons:\n\n* Badger keeps values separately from the LSM tree. This means that the compaction operations\nthat clean up the LSM tree do not touch the values at all. Values need to be cleaned up\nseparately.\n\n* Concurrent read/write transactions could leave behind multiple values for a single key, because they\nare stored with different versions. These could accumulate, and take up unneeded space beyond the\ntime these older versions are needed.\n\nBadger relies on the client to perform garbage collection at a time of their choosing. It provides\nthe following method, which can be invoked at an appropriate time:\n\n* `DB.RunValueLogGC()`: This method is designed to do garbage collection while\n  Badger is online. Along with randomly picking a file, it uses statistics generated by the\n  LSM-tree compactions to pick files that are likely to lead to maximum space\n  reclamation. It is recommended to be called during periods of low activity in\n  your system, or periodically. One call would only result in removal of at max\n  one log file. As an optimization, you could also immediately re-run it whenever\n  it returns nil error (indicating a successful value log GC), as shown below.\n\n\t```go\n\tticker := time.NewTicker(5 * time.Minute)\n\tdefer ticker.Stop()\n\tfor range ticker.C {\n\tagain:\n\t\terr := db.RunValueLogGC(0.7)\n\t\tif err == nil {\n\t\t\tgoto again\n\t\t}\n\t}\n\t```\n\n* `DB.PurgeOlderVersions()`: This method is **DEPRECATED** since v1.5.0. Now, Badger\'s LSM tree automatically discards older/invalid versions of keys.\n\n**Note: The RunValueLogGC method would not garbage collect the latest value log.**\n\n### Database backup\nThere are two public API methods `DB.Backup()` and `DB.Load()` which can be\nused to do online backups and restores. Badger v0.9 provides a CLI tool\n`badger`, which can do offline backup/restore. Make sure you have `$GOPATH/bin`\nin your PATH to use this tool.\n\nThe command below will create a version-agnostic backup of the database, to a\nfile `badger.bak` in the current working directory\n\n```\nbadger backup --dir <path/to/badgerdb>\n```\n\nTo restore `badger.bak` in the current working directory to a new database:\n\n```\nbadger restore --dir <path/to/badgerdb>\n```\n\nSee `badger --help` for more details.\n\nIf you have a Badger database that was created using v0.8 (or below), you can\nuse the `badger_backup` tool provided in v0.8.1, and then restore it using the\ncommand above to upgrade your database to work with the latest version.\n\n```\nbadger_backup --dir <path/to/badgerdb> --backup-file badger.bak\n```\n\nWe recommend all users to use the `Backup` and `Restore` APIs and tools. However,\nBadger is also rsync-friendly because all files are immutable, barring the\nlatest value log which is append-only. So, rsync can be used as rudimentary way\nto perform a backup. In the following script, we repeat rsync to ensure that the\nLSM tree remains consistent with the MANIFEST file while doing a full backup.\n\n```\n#!/bin/bash\nset -o history\nset -o histexpand\n# Makes a complete copy of a Badger database directory.\n# Repeat rsync if the MANIFEST and SSTables are updated.\nrsync -avz --delete db/ dst\nwhile !! | grep -q "(MANIFEST\\|\\.sst)$"; do :; done\n```\n\n### Memory usage\nBadger\'s memory usage can be managed by tweaking several options available in\nthe `Options` struct that is passed in when opening the database using\n`DB.Open`.\n\n- `Options.ValueLogLoadingMode` can be set to `options.FileIO` (instead of the\n  default `options.MemoryMap`) to avoid memory-mapping log files. This can be\n  useful in environments with low RAM.\n- Number of memtables (`Options.NumMemtables`)\n  - If you modify `Options.NumMemtables`, also adjust `Options.NumLevelZeroTables` and\n   `Options.NumLevelZeroTablesStall` accordingly.\n- Number of concurrent compactions (`Options.NumCompactors`)\n- Mode in which LSM tree is loaded (`Options.TableLoadingMode`)\n- Size of table (`Options.MaxTableSize`)\n- Size of value log file (`Options.ValueLogFileSize`)\n\nIf you want to decrease the memory usage of Badger instance, tweak these\noptions (ideally one at a time) until you achieve the desired\nmemory usage.\n\n### Statistics\nBadger records metrics using the [expvar] package, which is included in the Go\nstandard library. All the metrics are documented in [y/metrics.go][metrics]\nfile.\n\n`expvar` package adds a handler in to the default HTTP server (which has to be\nstarted explicitly), and serves up the metrics at the `/debug/vars` endpoint.\nThese metrics can then be collected by a system like [Prometheus], to get\nbetter visibility into what Badger is doing.\n\n[expvar]: https://golang.org/pkg/expvar/\n[metrics]: https://github.com/dgraph-io/badger/blob/master/y/metrics.go\n[Prometheus]: https://prometheus.io/\n\n## Resources\n\n### Blog Posts\n1. [Introducing Badger: A fast key-value store written natively in\nGo](https://open.dgraph.io/post/badger/)\n2. [Make Badger crash resilient with ALICE](https://blog.dgraph.io/post/alice/)\n3. [Badger vs LMDB vs BoltDB: Benchmarking key-value databases in Go](https://blog.dgraph.io/post/badger-lmdb-boltdb/)\n4. [Concurrent ACID Transactions in Badger](https://blog.dgraph.io/post/badger-txn/)\n\n## Design\nBadger was written with these design goals in mind:\n\n- Write a key-value database in pure Go.\n- Use latest research to build the fastest KV database for data sets spanning terabytes.\n- Optimize for SSDs.\n\nBadger\xe2\x80\x99s design is based on a paper titled _[WiscKey: Separating Keys from\nValues in SSD-conscious Storage][wisckey]_.\n\n[wisckey]: https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf\n\n### Comparisons\n| Feature                        | Badger                                     | RocksDB                       | BoltDB    |\n| -------                        | ------                                     | -------                       | ------    |\n| Design                         | LSM tree with value log                    | LSM tree only                 | B+ tree   |\n| High Read throughput           | Yes                                        | No                            | Yes       |\n| High Write throughput          | Yes                                        | Yes                           | No        |\n| Designed for SSDs              | Yes (with latest research <sup>1</sup>)    | Not specifically <sup>2</sup> | No        |\n| Embeddable                     | Yes                                        | Yes                           | Yes       |\n| Sorted KV access               | Yes                                        | Yes                           | Yes       |\n| Pure Go (no Cgo)               | Yes                                        | No                            | Yes       |\n| Transactions                   | Yes, ACID, concurrent with SSI<sup>3</sup> | Yes (but non-ACID)            | Yes, ACID |\n| Snapshots                      | Yes                                        | Yes                           | Yes       |\n| TTL support                    | Yes                                        | Yes                           | No        |\n| 3D access (key-value-version)  | Yes<sup>4</sup>                            | No                            | No        |\n\n<sup>1</sup> The [WISCKEY paper][wisckey] (on which Badger is based) saw big\nwins with separating values from keys, significantly reducing the write\namplification compared to a typical LSM tree.\n\n<sup>2</sup> RocksDB is an SSD optimized version of LevelDB, which was designed specifically for rotating disks.\nAs such RocksDB\'s design isn\'t aimed at SSDs.\n\n<sup>3</sup> SSI: Serializable Snapshot Isolation. For more details, see the blog post [Concurrent ACID Transactions in Badger](https://blog.dgraph.io/post/badger-txn/)\n\n<sup>4</sup> Badger provides direct access to value versions via its Iterator API.\nUsers can also specify how many versions to keep per key via Options.\n\n### Benchmarks\nWe have run comprehensive benchmarks against RocksDB, Bolt and LMDB. The\nbenchmarking code, and the detailed logs for the benchmarks can be found in the\n[badger-bench] repo. More explanation, including graphs can be found the blog posts (linked\nabove).\n\n[badger-bench]: https://github.com/dgraph-io/badger-bench\n\n## Other Projects Using Badger\nBelow is a list of known projects that use Badger:\n\n* [0-stor](https://github.com/zero-os/0-stor) - Single device object store.\n* [Dgraph](https://github.com/dgraph-io/dgraph) - Distributed graph database.\n* [TalariaDB](https://github.com/grab/talaria) - Distributed, low latency time-series database.\n* [Dispatch Protocol](https://github.com/dispatchlabs/disgo) - Blockchain protocol for distributed application data analytics.\n* [Sandglass](https://github.com/celrenheit/sandglass) - distributed, horizontally scalable, persistent, time sorted message queue.\n* [Usenet Express](https://usenetexpress.com/) - Serving over 300TB of data with Badger.\n* [go-ipfs](https://github.com/ipfs/go-ipfs) - Go client for the InterPlanetary File System (IPFS), a new hypermedia distribution protocol.\n* [gorush](https://github.com/appleboy/gorush) - A push notification server written in Go.\n* [emitter](https://github.com/emitter-io/emitter) - Scalable, low latency, distributed pub/sub broker with message storage, uses MQTT, gossip and badger.\n* [GarageMQ](https://github.com/valinurovam/garagemq) - AMQP server written in Go.\n* [RedixDB](https://alash3al.github.io/redix/) - A real-time persistent key-value store with the same redis protocol.\n* [BBVA](https://github.com/BBVA/raft-badger) - Raft backend implementation using BadgerDB for Hashicorp raft.\n* [Riot](https://github.com/go-ego/riot) - An open-source, distributed search engine.\n* [Fantom](https://github.com/Fantom-foundation/go-lachesis) - aBFT Consensus platform for distributed applications.\n* [decred](https://github.com/decred/dcrdata) - An open, progressive, and self-funding cryptocurrency with a system of community-based governance integrated into its blockchain.\n* [OpenNetSys](https://github.com/opennetsys/c3-go) - Create useful dApps in any software language.\n* [HoneyTrap](https://github.com/honeytrap/honeytrap) - An extensible and opensource system for running, monitoring and managing honeypots.\n* [Insolar](https://github.com/insolar/insolar) - Enterprise-ready blockchain platform.\n* [IoTeX](https://github.com/iotexproject/iotex-core) - The next generation of the decentralized network for IoT powered by scalability- and privacy-centric blockchains.\n* [go-sessions](https://github.com/kataras/go-sessions) - The sessions manager for Go net/http and fasthttp.\n* [Babble](https://github.com/mosaicnetworks/babble) - BFT Consensus platform for distributed applications.\n* [Tormenta](https://github.com/jpincas/tormenta) - Embedded object-persistence layer / simple JSON database for Go projects.\n* [BadgerHold](https://github.com/timshannon/badgerhold) - An embeddable NoSQL store for querying Go types built on Badger\n* [Goblero](https://github.com/didil/goblero) - Pure Go embedded persistent job queue backed by BadgerDB\n* [Surfline](https://www.surfline.com) - Serving global wave and weather forecast data with Badger.\n* [Cete](https://github.com/mosuka/cete) - Simple and highly available distributed key-value store built on Badger. Makes it easy bringing up a cluster of Badger with Raft consensus algorithm by hashicorp/raft. \n* [Volument](https://volument.com/) - A new take on website analytics backed by Badger.\n* [Sloop](https://github.com/salesforce/sloop) - Kubernetes History Visualization.\n* [KVdb](https://kvdb.io/) - Hosted key-value store and serverless platform built on top of Badger.\n\nIf you are using Badger in a project please send a pull request to add it to the list.\n\n## Frequently Asked Questions\n### My writes are getting stuck. Why?\n\n**Update: With the new `Value(func(v []byte))` API, this deadlock can no longer\nhappen.**\n\nThe following is true for users on Badger v1.x.\n\nThis can happen if a long running iteration with `Prefetch` is set to false, but\na `Item::Value` call is made internally in the loop. That causes Badger to\nacquire read locks over the value log files to avoid value log GC removing the\nfile from underneath. As a side effect, this also blocks a new value log GC\nfile from being created, when the value log file boundary is hit.\n\nPlease see Github issues [#293](https://github.com/dgraph-io/badger/issues/293)\nand [#315](https://github.com/dgraph-io/badger/issues/315).\n\nThere are multiple workarounds during iteration:\n\n1. Use `Item::ValueCopy` instead of `Item::Value` when retrieving value.\n1. Set `Prefetch` to true. Badger would then copy over the value and release the\n   file lock immediately.\n1. When `Prefetch` is false, don\'t call `Item::Value` and do a pure key-only\n   iteration. This might be useful if you just want to delete a lot of keys.\n1. Do the writes in a separate transaction after the reads.\n\n### My writes are really slow. Why?\n\nAre you creating a new transaction for every single key update, and waiting for\nit to `Commit` fully before creating a new one? This will lead to very low\nthroughput.\n\nWe have created `WriteBatch` API which provides a way to batch up\nmany updates into a single transaction and `Commit` that transaction using\ncallbacks to avoid blocking. This amortizes the cost of a transaction really\nwell, and provides the most efficient way to do bulk writes.\n\n```go\nwb := db.NewWriteBatch()\ndefer wb.Cancel()\n\nfor i := 0; i < N; i++ {\n  err := wb.Set(key(i), value(i), 0) // Will create txns as needed.\n  handle(err)\n}\nhandle(wb.Flush()) // Wait for all txns to finish.\n```\n\nNote that `WriteBatch` API does not allow any reads. For read-modify-write\nworkloads, you should be using the `Transaction` API.\n\n### I don\'t see any disk writes. Why?\n\nIf you\'re using Badger with `SyncWrites=false`, then your writes might not be written to value log\nand won\'t get synced to disk immediately. Writes to LSM tree are done inmemory first, before they\nget compacted to disk. The compaction would only happen once `MaxTableSize` has been reached. So, if\nyou\'re doing a few writes and then checking, you might not see anything on disk. Once you `Close`\nthe database, you\'ll see these writes on disk.\n\n### Reverse iteration doesn\'t give me the right results.\n\nJust like forward iteration goes to the first key which is equal or greater than the SEEK key, reverse iteration goes to the first key which is equal or lesser than the SEEK key. Therefore, SEEK key would not be part of the results. You can typically add a `0xff` byte as a suffix to the SEEK key to include it in the results. See the following issues: [#436](https://github.com/dgraph-io/badger/issues/436) and [#347](https://github.com/dgraph-io/badger/issues/347).\n\n### Which instances should I use for Badger?\n\nWe recommend using instances which provide local SSD storage, without any limit\non the maximum IOPS. In AWS, these are storage optimized instances like i3. They\nprovide local SSDs which clock 100K IOPS over 4KB blocks easily.\n\n### I\'m getting a closed channel error. Why?\n\n```\npanic: close of closed channel\npanic: send on closed channel\n```\n\nIf you\'re seeing panics like above, this would be because you\'re operating on a closed DB. This can happen, if you call `Close()` before sending a write, or multiple times. You should ensure that you only call `Close()` once, and all your read/write operations finish before closing.\n\n### Are there any Go specific settings that I should use?\n\nWe *highly* recommend setting a high number for `GOMAXPROCS`, which allows Go to\nobserve the full IOPS throughput provided by modern SSDs. In Dgraph, we have set\nit to 128. For more details, [see this\nthread](https://groups.google.com/d/topic/golang-nuts/jPb_h3TvlKE/discussion).\n\n### Are there any Linux specific settings that I should use?\n\nWe recommend setting `max file descriptors` to a high number depending upon the expected size of\nyour data. On Linux and Mac, you can check the file descriptor limit with `ulimit -n -H` for the\nhard limit and `ulimit -n -S` for the soft limit. A soft limit of `65535` is a good lower bound.\nYou can adjust the limit as needed.\n\n### I see "manifest has unsupported version: X (we support Y)" error.\n\nThis error means you have a badger directory which was created by an older version of badger and\nyou\'re trying to open in a newer version of badger. The underlying data format can change across\nbadger versions and users will have to migrate their data directory.\nBadger data can be migrated from version X of badger to version Y of badger by following the steps\nlisted below.\nAssume you were on badger v1.6.0 and you wish to migrate to v2.0.0 version.\n1. Install badger version v1.6.0\n    - `cd $GOPATH/src/github.com/dgraph-io/badger`\n    - `git checkout v1.6.0`\n    - `cd badger && go install`\n\n      This should install the old badger binary in your $GOBIN.\n2. Create Backup\n    - `badger backup --dir path/to/badger/directory -f badger.backup`\n3. Install badger version v2.0.0\n    - `cd $GOPATH/src/github.com/dgraph-io/badger`\n    - `git checkout v2.0.0`\n    - `cd badger && go install`\n\n      This should install new badger binary in your $GOBIN\n4. Install badger version v2.0.0\n    - `badger restore --dir path/to/new/badger/directory -f badger.backup`\n\n      This will create a new directory on `path/to/new/badger/directory` and add badger data in\n      newer format to it.\n\nNOTE - The above steps shouldn\'t cause any data loss but please ensure the new data is valid before\ndeleting the old badger directory.\n\n### Why do I need gcc to build badger? Does badger need CGO?\n\nBadger does not directly use CGO but it relies on https://github.com/DataDog/zstd library for\nzstd compression and the library requires `gcc/cgo`. You can build badger without cgo by running\n`CGO_ENABLED=0 go build`. This will build badger without the support for ZSTD compression algorithm.\n\n## Contact\n- Please use [discuss.dgraph.io](https://discuss.dgraph.io) for questions, feature requests and discussions.\n- Please use [Github issue tracker](https://github.com/dgraph-io/badger/issues) for filing bugs or feature requests.\n- Join [![Slack Status](http://slack.dgraph.io/badge.svg)](http://slack.dgraph.io).\n- Follow us on Twitter [@dgraphlabs](https://twitter.com/dgraphlabs).\n\n'