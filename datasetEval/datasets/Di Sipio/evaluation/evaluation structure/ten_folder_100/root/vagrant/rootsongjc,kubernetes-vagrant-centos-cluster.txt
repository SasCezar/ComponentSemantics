b'# Setting up a distributed Kubernetes cluster along with Istio service mesh locally with Vagrant and VirtualBox\n\n[\xe4\xbd\xbf\xe7\x94\xa8Vagrant\xe5\x92\x8cVirtualBox\xe5\x9c\xa8\xe6\x9c\xac\xe5\x9c\xb0\xe6\x90\xad\xe5\xbb\xba\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8fKubernetes\xe9\x9b\x86\xe7\xbe\xa4\xe5\x92\x8cIstio Service Mesh - \xe4\xb8\xad\xe6\x96\x87](README-cn.md)\n\nSetting up a Kubernetes cluster and Istio service mesh with `vagrantfile` which consists of 1 master(also as node) and 3 nodes. You don\'t have to create complicated CA files or configuration.\n\n**Note**: Because of using virtual machines to setup distributed Kubernetes cluster will bring high load on your computer, so I created the lightweight [Cloud Native Sandbox](https://github.com/rootsongjc/cloud-native-sandbox) using Docker to setup a standalone Kubernetes.\n\n## Demo\n\nClick the following image to watch the video.\n\n[![Watch the video](https://img.youtube.com/vi/26kbaZxcB4A/maxresdefault.jpg)](https://youtu.be/26kbaZxcB4A)\n\n### Why not use kubeadm?\n\nBecause I want to setup the etcd, apiserver, controller and scheduler without docker container.\n\n### Architecture\n\nWe will create a Kubernetes 1.15.0 cluster with 3 nodes which contains the components below:\n\n| IP           | Hostname | Componets                                |\n| ------------ | -------- | ---------------------------------------- |\n| 172.17.8.101 | node1    | kube-apiserver, kube-controller-manager, kube-scheduler, etcd, kubelet, docker, flannel, dashboard |\n| 172.17.8.102 | node2    | kubelet, docker, flannel\xe3\x80\x81traefik         |\n| 172.17.8.103 | node3    | kubelet, docker, flannel                 |\n\nThe default setting will create the private network from 172.17.8.101 to 172.17.8.103 for nodes, and it will use the host\'s DHCP for the public IP.\n\nThe kubernetes service\'s VIP range is `10.254.0.0/16`.\n\nThe container network range is `170.33.0.0/16` owned by flanneld with `host-gw` backend.\n\n`kube-proxy` will run as `ipvs` mode.\n\n## Usage\n\n### Prerequisite\n\n* Host server with 8G+ mem(More is better), 60G disk, 8 core cpu at lease\n* Vagrant 2.0+\n* VirtualBox 5.2 (5.2+ is not supported)\n* Kubernetes 1.9+ (support the latest version 1.15.0)\n* Across GFW to download the kubernetes files (For China users only)\n* MacOS/Linux (**Windows is not supported completely**)\n* NFS Server Package \n\n### Support Add-ons\n\n**Required**\n\n- CoreDNS\n- Dashboard\n- Traefik\n\n**Optional**\n\n- Heapster + InfluxDB + Grafana\n- ElasticSearch + Fluentd + Kibana\n- Istio service mesh\n- Helm\n- Vistio\n- Kiali\n\n#### Setup\n\nClone this repo into your local machine and download kubernetes binary release first and move them into the root directory of this repo (GitBash for the Windows must be run as Administrator to install ```vagrant-winnfsd``` plugin).\n\n```bash\nvagrant plugin install vagrant-winnfsd\ngit clone https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster.git\ncd kubernetes-vagrant-centos-cluster\n```\n\n**Note**: If this your first time to setup Kubernetes cluster with vagrant, just skip the above step and run the following command, it will download Kubernetes release automatically for you and no need to download the release next time. You can find the download address the Kubernetes releases [here](https://kubernetes.io/docs/imported/release/notes/). Download the release of version you wanted, move it to the root of this repo, rename it to `kubernetes-server-linux-amd64.tar.gz` then the `install.sh` script will skip the download step.\n\nAs this repo folder is mounted to `/vagrant` with NFS in virtual machines, you may be required to enter a password to for administrator privileges during the installation.\n\nSet up Kubernetes cluster with vagrant.\n\n```bash\nvagrant up\n```\n\nWait about 10 minutes the kubernetes cluster will be setup automatically.\n\nIf you have difficult to vagrant up the cluster because of have no way to downlaod the `centos/7` box, you can download the box and add it first.\n\n**Add centos/7 box manually**\n\n```bash\nwget -c http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1801_02.VirtualBox.box\nvagrant box add CentOS-7-x86_64-Vagrant-1801_02.VirtualBox.box --name centos/7\n```\n\nThe next time you run `vagrant up`, vagrant will import the local box automatically.\n\n#### Note for Windows\n- The project will run some bash script under the VirtualMachines. These scripts line ending need to be in LF. Git for windows set ```core.autocrlf``` true by default at the installation time. When you clone this project repository, this parameter (set to true) ask git to change all line ending to CRLF. This behavior need to be changed before cloning the repository (or after for each files by hand). We recommand to turn this to off by running ```git config --global core.autocrlf false``` and ```git config --global core.eol lf``` before cloning. Then, after cloning, do not forget to turn the behavior back if you want to run other windows projects: ```git config --global core.autocrlf true``` and ```git config --global core.eol crlf```.\n\n\nIf you have executed the previous git global configuration then, you will not see these output while node3 is going to be complete:\n\n```bash\n    node3: Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.\n    node3: Created symlink from /etc/systemd/system/multi-user.target.wants/kube-proxy.service to /usr/lib/systemd/system/kube-proxy.service.\n    node3: deploy coredns\n    node3: /tmp/vagrant-shell: ./dns-deploy.sh: /bin/bash^M: bad interpreter: No such file or directory\n    node3: error: no objects passed to apply\n    node3: /home/vagrant\n```\n\nSolution:\n\n```bash\nvagrant ssh node3\nsudo -i\ncd /vagrant/addon/dns\nyum -y install dos2unix\ndos2unix dns-deploy.sh\n./dns-deploy.sh -r 10.254.0.0/16 -i 10.254.0.2 |kubectl apply -f -\n```\n\n#### Connect to kubernetes cluster\n\nThere are 3 ways to access the kubernetes cluster.\n\n- on local\n- login to VM\n- Kubernetes dashboard\n\n**local**\n\nIn order to manage the cluster on local you should Install `kubectl` command line tool first(But, you don\'t need to do it manual because of ```install.sh``` script itself do this)\n\nGo to [Kubernetes release notes](https://kubernetes.io/docs/imported/release/notes/), download the client binaries, unzip it and then move `kubectl`  to your `$PATH` folder, for MacOS:\n\n```bash\nwget https://storage.googleapis.com/kubernetes-release/release/v1.15.0/kubernetes-client-darwin-amd64.tar.gz\ntar xvf kubernetes-client-darwin-amd64.tar.gz && cp kubernetes/client/bin/kubectl /usr/local/bin\n```\n\nCopy `conf/admin.kubeconfig` to `~/.kube/config`, using `kubectl` CLI to access the cluster.\n\n```bash\nmkdir -p ~/.kube\ncp conf/admin.kubeconfig ~/.kube/config\n```\n\nWe recommend you fellow this way.\n\n**VM**\n\nLogin to the virtual machine for debuging. In most situations, you have no need to login the VMs.\n\n```bash\nvagrant ssh node1\nsudo -i\nkubectl get nodes\nkubectl get pods --namespace=kube-system\n```\n\n**Kubernetes dashboard**\n\nKubernetes dashboard URL: <https://172.17.8.101:8443>\n\nGet the admin token:\n\n```bash\nkubectl -n kube-system describe secret `kubectl -n kube-system get secret|grep admin-token|cut -d " " -f1`|grep "token:"|tr -s " "|cut -d " " -f2\n```\n\n**Note**: You can see the token message on console when  `vagrant up` done.\n\n![Kubernetes dashboard animation](images/dashboard-animation.gif)\n\nOnly if you install the heapter addon bellow that you can see the metrics.\n\n**Visit from Chrome/Firefox on Windows**\n\nIf you see the hint `NET::ERR_CERT_INVALID`, follow these steps:\n\n```bash\nvagrant ssh node1\nsudo -i\ncd /vagrant/addon/dashboard/\nmkdir certs\nopenssl req -nodes -newkey rsa:2048 -keyout certs/dashboard.key -out certs/dashboard.csr -subj "/C=/ST=/L=/O=/OU=/CN=kubernetes-dashboard"\nopenssl x509 -req -sha256 -days 365 -in certs/dashboard.csr -signkey certs/dashboard.key -out certs/dashboard.crt\nkubectl delete secret kubernetes-dashboard-certs -n kube-system\nkubectl create secret generic kubernetes-dashboard-certs --from-file=certs -n kube-system\nkubectl delete pods $(kubectl get pods -n kube-system|grep kubernetes-dashboard|awk \'{print $1}\') -n kube-system #re-install dashboard\n```\n\nRefresh the browser and click `Advance`, skip it. You will see the dashboard page there.\n\n## Components\n\n**Heapster monitoring**\n\nRun this command on your local machine.\n\n```bash\nkubectl apply -f /vagrant/addon/heapster/\n```\n\nAppend the following item to your local `/etc/hosts` file.\n\n```ini\n172.17.8.102 grafana.jimmysong.io\n```\n\nOpen the URL in browser: <http://grafana.jimmysong.io>\n\n![Grafana animation](images/grafana-animation.gif)\n\n**Traefik**\n\nRun this command on your local machine.\n\n```bash\nkubectl apply -f /vagrant/addon/traefik-ingress\n```\n\nAppend the following item to your  local file  `/etc/hosts`.\n\n```ini\n172.17.8.102 traefik.jimmysong.io\n```\n\nTraefik UI URL: <http://traefik.jimmysong.io>\n\n![Traefik Ingress controller](images/traefik-ingress.gif)\n\n**EFK**\n\nRun this command on your local machine.\n\n```bash\nkubectl apply -f /vagrant/addon/efk/\n```\n\n**Note**: Powerful CPU and memory allocation required. At least 4G per virtual machine.\n\n**Helm**\n\nRun this command on your local machine.\n\n```bash\n/vagrant/hack/deploy-helm.sh\n```\n\n### Service Mesh\n\nWe use [istio](https://istio.io) as the default service mesh.\n\n**Installation**\n\nGo to [Istio release](https://github.com/istio/istio/releases) to download the binary package, install istio command line tool on local and move `istioctl` to your `$PATH` folder, for Mac:\n\n```bash\nwget https://github.com/istio/istio/releases/download/1.0.0/istio-1.0.0-osx.tar.gz\ntar xvf istio-1.0.0-osx.tar.gz\nmv istio-1.0.0/bin/istioctl /usr/local/bin/\n```\n\nDeploy istio into Kubernetes:\n\n```bash\nkubectl apply -f /vagrant/addon/istio/istio-demo.yaml\nkubectl apply -f /vagrant/addon/istio/istio-ingress.yaml\n```\n\n**Run sample**\n\nWe will let the sidecars be auto injected.\n\n```bash\nkubectl label namespace default istio-injection=enabled\nkubectl apply -n default -f /vagrant/yaml/istio-bookinfo/bookinfo.yaml\nkubectl apply -n default -f /vagrant/yaml/istio-bookinfo/bookinfo-gateway.yaml\nkubectl apply -n default -f /vagrant/yaml/istio-bookinfo/destination-rule-all.yaml\n```\n\nAdd the following items into the file  `/etc/hosts` of your local machine.\n\n```\n172.17.8.102 grafana.istio.jimmysong.io\n172.17.8.102 prometheus.istio.jimmysong.io\n172.17.8.102 servicegraph.istio.jimmysong.io\n172.17.8.102 jaeger-query.istio.jimmysong.io\n```\n\nWe can see the services from the following URLs.\n\n| Service      | URL                                                          |\n| ------------ | ------------------------------------------------------------ |\n| grafana      | http://grafana.istio.jimmysong.io                            |\n| servicegraph | <http://servicegraph.istio.jimmysong.io/dotviz>, <http://servicegraph.istio.jimmysong.io/graph>,<http://servicegraph.istio.jimmysong.io/force/forcegraph.html> |\n| tracing      | http://jaeger-query.istio.jimmysong.io                       |\n| productpage  | http://172.17.8.101:31380/productpage                        |\n\nMore detail see https://istio.io/docs/examples/bookinfo/\n\n![Bookinfo Demo](images/bookinfo-demo.gif)\n\n### Vistio\n\n[Vizceral](https://github.com/Netflix/vizceral)\xc2\xa0is an open source project released by Netflix to monitor network traffic between applications and clusters in near real time. Vistio is an adaptation of Vizceral for Istio and mesh monitoring. It utilizes metrics generated by Istio Mixer which are then fed into Prometheus. Vistio queries Prometheus and stores that data locally to allow for the replaying of traffic.\n\nRun the following commands in your local machine.\n\n```bash\n# Deploy vistio via kubectl\nkubectl -n default apply -f /vagrant/addon/vistio/\n\n# Expose vistio-api\nkubectl -n default port-forward $(kubectl -n default get pod -l app=vistio-api -o jsonpath=\'{.items[0].metadata.name}\') 9091:9091 &\n\n# Expose vistio in another terminal window\nkubectl -n default port-forward $(kubectl -n default get pod -l app=vistio-web -o jsonpath=\'{.items[0].metadata.name}\') 8080:8080 &\n```\n\nIf everything up until now is working you should be able to load the Vistio UI  in your browser http://localhost:8080\n\n![vistio animation](images/vistio-animation.gif)\n\nMore details see [Vistio\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8aVisualize your Istio Mesh Using Netflix\xe2\x80\x99s Vizceral](https://itnext.io/vistio-visualize-your-istio-mesh-using-netflixs-vizceral-b075c402e18e).\n\n### Kiali\n\nKiali is a project to help observability for the Istio service mesh, see\xc2\xa0[https://kiali.io](https://kiali.io/).\n\nRun the following commands in your local machine.\n\n```bash\nkubectl apply -n istio-system -f /vagrant/addon/kiali\n```\n\nKiali web: http://172.17.8.101:32439\n\nUser/password: admin/admin\n\n![kiali](images/kiali.gif)\n\n**Note**: Kiali use jaeger for tracing. Do not block the pop-up windows for kiali.\n\n### Weave scope\n\n[Weave scope](https://github.com/weaveworks/scope) is a project for monitoring, visualisation & management for Docker & Kubernetes, see <https://www.weave.works/oss/scope/> \n\nRun the following commands in your local machine.\n\n```bash\nkubectl apply -f /vagrant/addon/weave-scope\n```\n\nAdd a record on your local  `/etc/hosts`.\n\n```\n172.17.8.102 scope.weave.jimmysong.io\n```\n\nNow open your browser on http://scope.weave.jimmysong.io/\n\n![Weave scope animation](images/weave-scope-animation.gif)\n\n## Operation\n\nExcept for special claim, execute the following commands under the current git repo\'s root directory.\n\n### Suspend\n\nSuspend the current state of VMs.\n\n```bash\nvagrant suspend\n```\n\n### Resume\n\nResume the last state of VMs.\n\n```bash\nvagrant resume\n```\n\nNote: every time you resume the VMs you will find that the machine time is still at you last time you suspended it. So consider to halt the VMs and restart them.\n\n### Restart\n\nHalt the VMs and up them again.\n\n```bash\nvagrant halt\nvagrant up\n# login to node1\nvagrant ssh node1\n# run the prosivision scripts\n/vagrant/hack/k8s-init.sh\nexit\n# login to node2\nvagrant ssh node2\n# run the prosivision scripts\n/vagrant/hack/k8s-init.sh\nexit\n# login to node3\nvagrant ssh node3\n# run the prosivision scripts\n/vagrant/hack/k8s-init.sh\nsudo -i\ncd /vagrant/hack\n./deploy-base-services.sh\nexit\n```\n\nNow you have provisioned the base kubernetes environments and you can login to kubernetes dashboard, run the following command at the root of this repo to get the admin token.\n\n```bash\nhack/get-dashboard-token.sh\n```\n\nFollowing the hint to login.\n\n### Clean\n\nClean up the VMs.\n\n```bash\nvagrant destroy\nrm -rf .vagrant\n```\n\n### Note\n\nOnly use for development and test, don\'t use it in production environment.\n\n## Reference\n\n* [Kubernetes Handbook - jimmysong.io](https://jimmysong.io/kubernetes-handbook/)\n* [duffqiu/centos-vagrant](https://github.com/duffqiu/centos-vagrant)\n* [coredns/deployment](https://github.com/coredns/deployment)\n* [kubernetes ipvs](https://github.com/kubernetes/kubernetes/tree/master/pkg/proxy/ipvs)\n* [Vistio\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8aVisualize your Istio Mesh Using Netflix\xe2\x80\x99s Vizceral](https://itnext.io/vistio-visualize-your-istio-mesh-using-netflixs-vizceral-b075c402e18e)\n\n**Follow the [ServiceMesher community](http://www.servicemesher.com) on [twitter](https://twitter.com/servicemesher) to get more information about [Istio](https://istio.io) and service mesh.**\n'