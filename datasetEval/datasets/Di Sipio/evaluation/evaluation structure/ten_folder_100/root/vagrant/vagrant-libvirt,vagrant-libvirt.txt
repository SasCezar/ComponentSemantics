b'# Vagrant Libvirt Provider\n\n[![Join the chat at https://gitter.im/vagrant-libvirt/vagrant-libvirt](https://badges.gitter.im/vagrant-libvirt/vagrant-libvirt.svg)](https://gitter.im/vagrant-libvirt/vagrant-libvirt?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Build Status](https://travis-ci.org/vagrant-libvirt/vagrant-libvirt.svg)](https://travis-ci.org/vagrant-libvirt/vagrant-libvirt)\n[![Coverage Status](https://coveralls.io/repos/github/vagrant-libvirt/vagrant-libvirt/badge.svg?branch=master)](https://coveralls.io/github/vagrant-libvirt/vagrant-libvirt?branch=master)\n\nThis is a [Vagrant](http://www.vagrantup.com) plugin that adds a\n[Libvirt](http://libvirt.org) provider to Vagrant, allowing Vagrant to\ncontrol and provision machines via Libvirt toolkit.\n\n**Note:** Actual version is still a development one. Feedback is welcome and\ncan help a lot :-)\n\n## Index\n\n\n- [Features](#features)\n- [Future work](#future-work)\n- [Installation](#installation)\n  - [Possible problems with plugin installation on Linux](#possible-problems-with-plugin-installation-on-linux)\n- [Vagrant Project Preparation](#vagrant-project-preparation)\n  - [Add Box](#add-box)\n  - [Create Vagrantfile](#create-vagrantfile)\n  - [Start VM](#start-vm)\n  - [How Project Is Created](#how-project-is-created)\n  - [Libvirt Configuration](#libvirt-configuration)\n  - [Provider Options](#provider-options)\n  - [Domain Specific Options](#domain-specific-options)\n    - [Reload behavior](#reload-behavior)\n- [Networks](#networks)\n  - [Private Network Options](#private-network-options)\n  - [Public Network Options](#public-network-options)\n  - [Management Network](#management-network)\n- [Additional Disks](#additional-disks)\n    - [Reload behavior](#reload-behavior-1)\n- [CDROMs](#cdroms)\n- [Input](#input)\n- [PCI device passthrough](#pci-device-passthrough)\n- [Using USB Devices](#using-usb-devices)\n  - [USB Controller Configuration](#usb-controller-configuration)\n  - [USB Device Passthrough](#usb-device-passthrough)\n  - [USB Redirector Devices](#usb-redirector-devices)\n- [Random number generator passthrough](#random-number-generator-passthrough)\n- [Watchdog\xc2\xb7Device](#watchdog-device)\n- [Smartcard device](#smartcard-device)\n- [Hypervisor Features](#hypervisor-features)\n- [CPU Features](#cpu-features)\n- [No box and PXE boot](#no-box-and-pxe-boot)\n- [SSH Access To VM](#ssh-access-to-vm)\n- [Forwarded Ports](#forwarded-ports)\n- [Synced Folders](#synced-folders)\n- [QEMU Session Support](#qemu-session-support)\n- [Customized Graphics](#customized-graphics)\n- [Box Format](#box-format)\n- [Create Box](#create-box)\n- [Package Box from VM](#package-box-from-vm)\n- [Troubleshooting VMs](#troubleshooting-vms)\n- [Development](#development)\n- [Contributing](#contributing)\n\n## Features\n\n* Control local Libvirt hypervisors.\n* Vagrant `up`, `destroy`, `suspend`, `resume`, `halt`, `ssh`, `reload`,\n  `package` and `provision` commands.\n* Upload box image (qcow2 format) to Libvirt storage pool.\n* Create volume as COW diff image for domains.\n* Create private networks.\n* Create and boot Libvirt domains.\n* SSH into domains.\n* Setup hostname and network interfaces.\n* Provision domains with any built-in Vagrant provisioner.\n* Synced folder support via `rsync`, `nfs` or `9p`.\n* Snapshots via [sahara](https://github.com/jedi4ever/sahara).\n* Package caching via\n  [vagrant-cachier](http://fgrehm.viewdocs.io/vagrant-cachier/).\n* Use boxes from other Vagrant providers via\n  [vagrant-mutate](https://github.com/sciurus/vagrant-mutate).\n* Support VMs with no box for PXE boot purposes (Vagrant 1.6 and up)\n\n## Future work\n\n* Take a look at [open\n  issues](https://github.com/vagrant-libvirt/vagrant-libvirt/issues?state=open).\n\n## Installation\n\nFirst, you should have both QEMU and Libvirt installed if you plan to run VMs\non your local system. For instructions, refer to your Linux distribution\'s\ndocumentation.\n\n**NOTE:** Before you start using vagrant-libvirt, please make sure your Libvirt\nand QEMU installation is working correctly and you are able to create QEMU or\nKVM type virtual machines with `virsh` or `virt-manager`.\n\nNext, you must have [Vagrant\ninstalled](http://docs.vagrantup.com/v2/installation/index.html).\nVagrant-libvirt supports Vagrant 2.0, 2.1 & 2.2. It should also work with earlier\nreleases from 1.5 onwards but they are not actively tested.\n\nCheck the [.travis.yml](https://github.com/vagrant-libvirt/vagrant-libvirt/blob/master/.travis.yml)\nfor the current list of tested versions.\n\n*We only test with the upstream version!* If you decide to install your distro\'s\nversion and you run into problems, as a first step you should switch to upstream.\n\nNow you need to make sure your have all the build dependencies installed for\nvagrant-libvirt. This depends on your distro. An overview:\n\n* Ubuntu 18.10, Debian 9 and up:\n```shell\napt-get build-dep vagrant ruby-libvirt\napt-get install qemu libvirt-daemon-system libvirt-clients ebtables dnsmasq-base\napt-get install libxslt-dev libxml2-dev libvirt-dev zlib1g-dev ruby-dev\n```\n\n* Ubuntu 18.04, Debian 8 and older:\n```shell\napt-get build-dep vagrant ruby-libvirt\napt-get install qemu libvirt-bin ebtables dnsmasq-base\napt-get install libxslt-dev libxml2-dev libvirt-dev zlib1g-dev ruby-dev\n```\n\n(It is possible some users will already have libraries from the third line installed, but this is the way to make it work OOTB.)\n\n* CentOS 6, 7, Fedora 21:\n```shell\nyum install qemu libvirt libvirt-devel ruby-devel gcc qemu-kvm\n```\n\n* Fedora 22 and up:\n```shell\ndnf -y install qemu libvirt libvirt-devel ruby-devel gcc\n```\n\n* OpenSUSE leap 15.1:\n```shell\nzypper install qemu libvirt libvirt-devel ruby-devel gcc qemu-kvm\n```\n\n* Arch Linux: please read the related [ArchWiki](https://wiki.archlinux.org/index.php/Vagrant#vagrant-libvirt) page.\n```shell\npacman -S vagrant\n```\n\nNow you\'re ready to install vagrant-libvirt using standard [Vagrant\nplugin](http://docs.vagrantup.com/v2/plugins/usage.html) installation methods.\n\n```shell\n$ vagrant plugin install vagrant-libvirt\n```\n\n### Possible problems with plugin installation on Linux\n\nIn case of problems with building nokogiri and ruby-libvirt gem, install\nmissing development libraries for libxslt, libxml2 and libvirt.\n\n\nOn Ubuntu, Debian, make sure you are running all three of the `apt` commands above with `sudo`.\n\n\nOn RedHat, Centos, Fedora, ...\n\n```shell\n$ sudo dnf install libxslt-devel libxml2-devel libvirt-devel \\\n  libguestfs-tools-c ruby-devel gcc\n```\n\nOn Arch Linux it is recommended to follow [steps from ArchWiki](https://wiki.archlinux.org/index.php/Vagrant#vagrant-libvirt).\n\nIf have problem with installation - check your linker. It should be `ld.gold`:\n\n```shell\nsudo alternatives --set ld /usr/bin/ld.gold\n# OR\nsudo ln -fs /usr/bin/ld.gold /usr/bin/ld\n```\n\nIf you have issues building ruby-libvirt, try the following:\n```shell\nCONFIGURE_ARGS=\'with-ldflags=-L/opt/vagrant/embedded/lib with-libvirt-include=/usr/include/libvirt with-libvirt-lib=/usr/lib\' GEM_HOME=~/.vagrant.d/gems GEM_PATH=$GEM_HOME:/opt/vagrant/embedded/gems PATH=/opt/vagrant/embedded/bin:$PATH vagrant plugin install vagrant-libvirt\n```\n\n## Vagrant Project Preparation\n\n### Add Box\n\nAfter installing the plugin (instructions above), the quickest way to get\nstarted is to add Libvirt box and specify all the details manually within a\n`config.vm.provider` block. So first, add Libvirt box using any name you want.\nYou can find more Libvirt-ready boxes at\n[Vagrant Cloud](https://app.vagrantup.com/boxes/search?provider=libvirt). For\nexample:\n\n```shell\nvagrant init fedora/24-cloud-base\n```\n\n### Create Vagrantfile\n\nAnd then make a Vagrantfile that looks like the following, filling in your\ninformation where necessary. For example:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.define :test_vm do |test_vm|\n    test_vm.vm.box = "fedora/24-cloud-base"\n  end\nend\n```\n\n### Start VM\n\nIn prepared project directory, run following command:\n\n```shell\n$ vagrant up --provider=libvirt\n```\n\nVagrant needs to know that we want to use Libvirt and not default VirtualBox.\nThat\'s why there is `--provider=libvirt` option specified. Other way to tell\nVagrant to use Libvirt provider is to setup environment variable\n\n```shell\nexport VAGRANT_DEFAULT_PROVIDER=libvirt\n```\n\n### How Project Is Created\n\nVagrant goes through steps below when creating new project:\n\n1. Connect to Libvirt locally or remotely via SSH.\n2. Check if box image is available in Libvirt storage pool. If not, upload it\n   to remote Libvirt storage pool as new volume.\n3. Create COW diff image of base box image for new Libvirt domain.\n4. Create and start new domain on Libvirt host.\n5. Check for DHCP lease from dnsmasq server.\n6. Wait till SSH is available.\n7. Sync folders and run Vagrant provisioner on new domain if setup in\n   Vagrantfile.\n\n### Libvirt Configuration\n\n### Provider Options\n\nAlthough it should work without any configuration for most people, this\nprovider exposes quite a few provider-specific configuration options. The\nfollowing options allow you to configure how vagrant-libvirt connects to\nLibvirt, and are used to generate the [Libvirt connection\nURI](http://libvirt.org/uri.html):\n\n* `driver` - A hypervisor name to access. For now only KVM and QEMU are\n  supported\n* `host` - The name of the server, where Libvirtd is running\n* `connect_via_ssh` - If use ssh tunnel to connect to Libvirt. Absolutely\n  needed to access Libvirt on remote host. It will not be able to get the IP\n  address of a started VM otherwise.\n* `username` - Username and password to access Libvirt\n* `password` - Password to access Libvirt\n* `id_ssh_key_file` - If not nil, uses this ssh private key to access Libvirt.\n  Default is `$HOME/.ssh/id_rsa`. Prepends `$HOME/.ssh/` if no directory\n* `socket` - Path to the Libvirt unix socket (e.g.\n  `/var/run/libvirt/libvirt-sock`)\n* `uri` - For advanced usage. Directly specifies what Libvirt connection URI\n  vagrant-libvirt should use. Overrides all other connection configuration\n  options\n\nConnection-independent options:\n\n* `storage_pool_name` - Libvirt storage pool name, where box image and instance\n  snapshots will be stored.\n\nFor example:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.host = "example.com"\n  end\nend\n```\n\n### Domain Specific Options\n\n* `disk_bus` - The type of disk device to emulate. Defaults to virtio if not\n  set. Possible values are documented in Libvirt\'s [description for\n  _target_](http://libvirt.org/formatdomain.html#elementsDisks). NOTE: this\n  option applies only to disks associated with a box image. To set the bus type\n  on additional disks, see the [Additional Disks](#additional-disks) section.\n* `disk_device` - The disk device to emulate. Defaults to vda if not\n  set, which should be fine for paravirtualized guests, but some fully\n  virtualized guests may require hda. NOTE: this option also applies only to\n  disks associated with a box image.\n* `nic_model_type` - parameter specifies the model of the network adapter when\n  you create a domain value by default virtio KVM believe possible values, see\n  the [documentation for\n  Libvirt](https://libvirt.org/formatdomain.html#elementsNICSModel).\n* `memory` - Amount of memory in MBytes. Defaults to 512 if not set.\n* `cpus` - Number of virtual cpus. Defaults to 1 if not set.\n* `cputopology` - Number of CPU sockets, cores and threads running per core. All fields of `:sockets`, `:cores` and `:threads` are mandatory, `cpus` domain option must be present and must be equal to total count of **sockets * cores * threads**. For more details see [documentation](https://libvirt.org/formatdomain.html#elementsCPU).\n\n  ```ruby\n  Vagrant.configure("2") do |config|\n    config.vm.provider :libvirt do |libvirt|\n      libvirt.cpus = 4\n      libvirt.cputopology :sockets => \'2\', :cores => \'2\', :threads => \'1\'\n    end\n  end\n  ```\n\n* `nested` - [Enable nested\n  virtualization](https://github.com/torvalds/linux/blob/master/Documentation/virtual/kvm/nested-vmx.txt).\n  Default is false.\n* `cpu_mode` - [CPU emulation\n  mode](https://libvirt.org/formatdomain.html#elementsCPU). Defaults to\n  \'host-model\' if not set. Allowed values: host-model, host-passthrough,\n  custom.\n* `cpu_model` - CPU Model. Defaults to \'qemu64\' if not set and `cpu_mode` is\n  `custom` and to \'\' otherwise. This can really only be used when setting\n  `cpu_mode` to `custom`.\n* `cpu_fallback` - Whether to allow Libvirt to fall back to a CPU model close\n  to the specified model if features in the guest CPU are not supported on the\n  host. Defaults to \'allow\' if not set. Allowed values: `allow`, `forbid`.\n* `numa_nodes` - Specify an array of NUMA nodes for the guest. The syntax is similar to what would be set in the domain XML. `memory` must be in MB. Symmetrical and asymmetrical topologies are supported but make sure your total count of defined CPUs adds up to `v.cpus`.\n\n  The sum of all the memory defined here will act as your total memory for your guest VM. **This sum will override what is set in `v.memory`**\n  ```\n  v.cpus = 4\n  v.numa_nodes = [\n    {:cpus => "0-1", :memory => "1024"},\n    {:cpus => "2-3", :memory => "4096"}\n  ]\n  ```\n* `loader` - Sets path to custom UEFI loader.\n* `volume_cache` - Controls the cache mechanism. Possible values are "default",\n  "none", "writethrough", "writeback", "directsync" and "unsafe". [See\n  driver->cache in Libvirt\n  documentation](http://libvirt.org/formatdomain.html#elementsDisks).\n* `kernel` - To launch the guest with a kernel residing on host filesystems.\n  Equivalent to qemu `-kernel`.\n* `initrd` - To specify the initramfs/initrd to use for the guest. Equivalent\n  to qemu `-initrd`.\n* `random_hostname` - To create a domain name with extra information on the end\n  to prevent hostname conflicts.\n* `default_prefix` - The default Libvirt guest name becomes a concatenation of the\n   `<current_directory>_<guest_name>`. The current working directory is the default prefix \n   to the guest name. The `default_prefix` options allow you to set the guest name prefix.\n* `cmd_line` - Arguments passed on to the guest kernel initramfs or initrd to\n  use. Equivalent to qemu `-append`, only possible to use in combination with `initrd` and `kernel`.\n* `graphics_type` - Sets the protocol used to expose the guest display.\n  Defaults to `vnc`.  Possible values are "sdl", "curses", "none", "gtk", "vnc"\n  or "spice".\n* `graphics_port` - Sets the port for the display protocol to bind to.\n  Defaults to 5900.\n* `graphics_ip` - Sets the IP for the display protocol to bind to.  Defaults to\n  "127.0.0.1".\n* `graphics_passwd` - Sets the password for the display protocol. Working for\n  vnc and Spice. by default working without passsword.\n* `graphics_autoport` - Sets autoport for graphics, Libvirt in this case\n  ignores graphics_port value, Defaults to \'yes\'. Possible value are "yes" and\n  "no"\n* `keymap` - Set keymap for vm. default: en-us\n* `kvm_hidden` - [Hide the hypervisor from the\n  guest](https://libvirt.org/formatdomain.html#elementsFeatures). Useful for\n  [GPU passthrough](#pci-device-passthrough) on stubborn drivers. Default is false.\n* `video_type` - Sets the graphics card type exposed to the guest.  Defaults to\n  "cirrus".  [Possible\n  values](http://libvirt.org/formatdomain.html#elementsVideo) are "vga",\n  "cirrus", "vmvga", "xen", "vbox", or "qxl".\n* `video_vram` - Used by some graphics card types to vary the amount of RAM\n  dedicated to video.  Defaults to 9216.\n* `sound_type` - [Set the virtual sound card](https://libvirt.org/formatdomain.html#elementsSound)\n  Defaults to "ich6".\n* `machine_type` - Sets machine type. Equivalent to qemu `-machine`. Use\n  `qemu-system-x86_64 -machine help` to get a list of supported machines.\n* `machine_arch` - Sets machine architecture. This helps Libvirt to determine\n  the correct emulator type. Possible values depend on your version of QEMU.\n  For possible values, see which emulator executable `qemu-system-*` your\n  system provides. Common examples are `aarch64`, `alpha`, `arm`, `cris`,\n  `i386`, `lm32`, `m68k`, `microblaze`, `microblazeel`, `mips`, `mips64`,\n  `mips64el`, `mipsel`, `moxie`, `or32`, `ppc`, `ppc64`, `ppcemb`, `s390x`,\n  `sh4`, `sh4eb`, `sparc`, `sparc64`, `tricore`, `unicore32`, `x86_64`,\n  `xtensa`, `xtensaeb`.\n* `machine_virtual_size` - Sets the disk size in GB for the machine overriding\n  the default specified in the box. Allows boxes to defined with a minimal size\n  disk by default and to be grown to a larger size at creation time. Will\n  ignore sizes smaller than the size specified by the box metadata. Note that\n  currently there is no support for automatically resizing the filesystem to\n  take advantage of the larger disk.\n* `emulator_path` - Explicitly select which device model emulator to use by\n  providing the path, e.g. `/usr/bin/qemu-system-x86_64`. This is especially\n  useful on systems that fail to select it automatically based on\n  `machine_arch` which then results in a capability error.\n* `boot` - Change the boot order and enables the boot menu. Possible options\n  are "hd", "network", "cdrom". Defaults to "hd" with boot menu disabled. When\n  "network" is set without "hd", only all NICs will be tried; see below for\n  more detail.\n* `nic_adapter_count` - Defaults to \'8\'. Only use case for increasing this\n  count is for VMs that virtualize switches such as Cumulus Linux. Max value\n  for Cumulus Linux VMs is 33.\n* `uuid` - Force a domain UUID. Defaults to autogenerated value by Libvirt if\n  not set.\n* `suspend_mode` - What is done on vagrant suspend. Possible values: \'pause\',\n  \'managedsave\'. Pause mode executes a la `virsh suspend`, which just pauses\n  execution of a VM, not freeing resources. Managed save mode does a la `virsh\n  managedsave` which frees resources suspending a domain.\n* `tpm_model` - The model of the TPM to which you wish to connect.\n* `tpm_type` - The type of TPM device to which you are connecting.\n* `tpm_path` - The path to the TPM device on the host system.\n* `dtb` - The device tree blob file, mostly used for non-x86 platforms. In case\n  the device tree isn\'t added in-line to the kernel, it can be manually\n  specified here.\n* `autostart` - Automatically start the domain when the host boots. Defaults to\n  \'false\'.\n* `channel` - [Libvirt\n  channels](https://libvirt.org/formatdomain.html#elementCharChannel).\n  Configure a private communication channel between the host and guest, e.g.\n  for use by the [QEMU guest\n  agent](http://wiki.libvirt.org/page/Qemu_guest_agent) and the Spice/QXL\n  graphics type.\n* `mgmt_attach` - Decide if VM has interface in mgmt network. If set to \'false\'\n  it is not possible to communicate with VM through `vagrant ssh` or run\n  provisioning. Setting to \'false\' is only possible when VM doesn\'t use box.\n  Defaults set to \'true\'.\n\nSpecific domain settings can be set for each domain separately in multi-VM\nenvironment. Example below shows a part of Vagrantfile, where specific options\nare set for dbserver domain.\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.define :dbserver do |dbserver|\n    dbserver.vm.box = "centos64"\n    dbserver.vm.provider :libvirt do |domain|\n      domain.memory = 2048\n      domain.cpus = 2\n      domain.nested = true\n      domain.volume_cache = \'none\'\n    end\n  end\n\n  # ...\n```\n\nThe following example shows part of a Vagrantfile that enables the VM to boot\nfrom a network interface first and a hard disk second. This could be used to\nrun VMs that are meant to be a PXE booted machines. Be aware that if `hd` is\nnot specified as a boot option, it will never be tried.\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.define :pxeclient do |pxeclient|\n    pxeclient.vm.box = "centos64"\n    pxeclient.vm.provider :libvirt do |domain|\n      domain.boot \'network\'\n      domain.boot \'hd\'\n    end\n  end\n\n  # ...\n```\n\n#### Reload behavior\n\nOn `vagrant reload` the following domain specific attributes are updated in\ndefined domain:\n\n* `disk_bus` - Is updated only on disks. It skips CDROMs\n* `nic_model_type` - Updated\n* `memory` - Updated\n* `cpus` - Updated\n* `nested` - Updated\n* `cpu_mode` - Updated. Pay attention that custom mode is not supported\n* `graphics_type` - Updated\n* `graphics_port` - Updated\n* `graphics_ip` - Updated\n* `graphics_passwd` - Updated\n* `graphics_autoport` - Updated\n* `keymap` - Updated\n* `video_type` - Updated\n* `video_vram` - Updated\n* `tpm_model` - Updated\n* `tpm_type` - Updated\n* `tpm_path` - Updated\n\n## Networks\n\nNetworking features in the form of `config.vm.network` support private networks\nconcept. It supports both the virtual network switch routing types and the\npoint to point Guest OS to Guest OS setting using UDP/Mcast/TCP tunnel\ninterfaces.\n\nhttp://wiki.libvirt.org/page/VirtualNetworking\n\nhttps://libvirt.org/formatdomain.html#elementsNICSTCP\n\nhttp://libvirt.org/formatdomain.html#elementsNICSMulticast\n\nhttp://libvirt.org/formatdomain.html#elementsNICSUDP _(in Libvirt v1.2.20 and higher)_\n\nPublic Network interfaces are currently implemented using the macvtap driver.\nThe macvtap driver is only available with the Linux Kernel version >= 2.6.24.\nSee the following Libvirt documentation for the details of the macvtap usage.\n\nhttp://www.libvirt.org/formatdomain.html#elementsNICSDirect\n\nAn examples of network interface definitions:\n\n```ruby\n  # Private network using virtual network switching\n  config.vm.define :test_vm1 do |test_vm1|\n    test_vm1.vm.network :private_network, :ip => "10.20.30.40"\n  end\n\n  # Private network using DHCP and a custom network\n  config.vm.define :test_vm1 do |test_vm1|\n    test_vm1.vm.network :private_network,\n      :type => "dhcp",\n      :libvirt__network_address => \'10.20.30.0\'\n  end\n\n  # Private network (as above) using a domain name\n  config.vm.define :test_vm1 do |test_vm1|\n    test_vm1.vm.network :private_network,\n      :ip => "10.20.30.40",\n      :libvirt__domain_name => "test.local"\n  end\n\n  # Private network. Point to Point between 2 Guest OS using a TCP tunnel\n  # Guest 1\n  config.vm.define :test_vm1 do |test_vm1|\n    test_vm1.vm.network :private_network,\n      :libvirt__tunnel_type => \'server\',\n      # default is 127.0.0.1 if omitted\n      # :libvirt__tunnel_ip => \'127.0.0.1\',\n      :libvirt__tunnel_port => \'11111\'\n    # network with ipv6 support\n    test_vm1.vm.network :private_network,\n      :ip => "10.20.5.42",\n      :libvirt__guest_ipv6 => "yes",\n      :libvirt__ipv6_address => "2001:db8:ca2:6::1",\n      :libvirt__ipv6_prefix => "64"\n\n  # Guest 2\n  config.vm.define :test_vm2 do |test_vm2|\n    test_vm2.vm.network :private_network,\n      :libvirt__tunnel_type => \'client\',\n      # default is 127.0.0.1 if omitted\n      # :libvirt__tunnel_ip => \'127.0.0.1\',\n      :libvirt__tunnel_port => \'11111\'\n    # network with ipv6 support\n    test_vm2.vm.network :private_network,\n      :ip => "10.20.5.45",\n      :libvirt__guest_ipv6 => "yes",\n      :libvirt__ipv6_address => "2001:db8:ca2:6::1",\n      :libvirt__ipv6_prefix => "64"\n\n\n  # Public Network\n  config.vm.define :test_vm1 do |test_vm1|\n    test_vm1.vm.network :public_network,\n      :dev => "virbr0",\n      :mode => "bridge",\n      :type => "bridge"\n  end\n```\n\nIn example below, one network interface is configured for VM `test_vm1`. After\nyou run `vagrant up`, VM will be accessible on IP address `10.20.30.40`. So if\nyou install a web server via provisioner, you will be able to access your\ntesting server on `http://10.20.30.40` URL. But beware that this address is\nprivate to Libvirt host only. It\'s not visible outside of the hypervisor box.\n\nIf network `10.20.30.0/24` doesn\'t exist, provider will create it. By default\ncreated networks are NATed to outside world, so your VM will be able to connect\nto the internet (if hypervisor can). And by default, DHCP is offering addresses\non newly created networks.\n\nThe second interface is created and bridged into the physical device `eth0`.\nThis mechanism uses the macvtap Kernel driver and therefore does not require an\nexisting bridge device. This configuration assumes that DHCP and DNS services\nare being provided by the public network. This public interface should be\nreachable by anyone with access to the public network.\n\n### Private Network Options\n\n*Note: These options are not applicable to public network interfaces.*\n\nThere is a way to pass specific options for Libvirt provider when using\n`config.vm.network` to configure new network interface. Each parameter name\nstarts with `libvirt__` string. Here is a list of those options:\n\n* `:libvirt__network_name` - Name of Libvirt network to connect to. By default,\n  network \'default\' is used.\n* `:libvirt__netmask` - Used only together with `:ip` option. Default is\n  \'255.255.255.0\'.\n* `:libvirt__network_address` - Used only when `:type` is set to `dhcp`. Only `/24` subnet is supported. Default is `172.28.128.0`.\n* `:libvirt__host_ip` - Address to use for the host (not guest).  Default is\n  first possible address (after network address).\n* `:libvirt__domain_name` - DNS domain of the DHCP server. Used only\n  when creating new network.\n* `:libvirt__dhcp_enabled` - If DHCP will offer addresses, or not. Used only\n  when creating new network. Default is true.\n* `:libvirt__dhcp_start` - First address given out via DHCP.  Default is third\n  address in range (after network name and gateway).\n* `:libvirt__dhcp_stop` - Last address given out via DHCP.  Default is last\n  possible address in range (before broadcast address).\n* `:libvirt__dhcp_bootp_file` - The file to be used for the boot image.  Used\n  only when dhcp is enabled.\n* `:libvirt__dhcp_bootp_server` - The server that runs the DHCP server.  Used\n  only when dhcp is enabled.By default is the same host that runs the DHCP\n  server.\n* `:libvirt__adapter` - Number specifiyng sequence number of interface.\n* `:libvirt__forward_mode` - Specify one of `veryisolated`, `none`, `nat` or\n  `route` options.  This option is used only when creating new network. Mode\n  `none` will create isolated network without NATing or routing outside. You\n  will want to use NATed forwarding typically to reach networks outside of\n  hypervisor. Routed forwarding is typically useful to reach other networks\n  within hypervisor.  `veryisolated` described\n  [here](https://libvirt.org/formatnetwork.html#examplesNoGateway).  By\n  default, option `nat` is used.\n* `:libvirt__forward_device` - Name of interface/device, where network should\n  be forwarded (NATed or routed). Used only when creating new network. By\n  default, all physical interfaces are used.\n* `:libvirt__tunnel_type` - Set to \'udp\' if using UDP unicast tunnel mode\n  (libvirt v1.2.20 or higher).  Set this to either "server" or "client" for tcp\n  tunneling. Set this to \'mcast\' if using multicast tunneling. This\n  configuration type uses tunnels to generate point to point connections\n  between Guests. Useful for Switch VMs like Cumulus Linux. No virtual switch\n  setting like `libvirt__network_name` applies with tunnel interfaces and will\n  be ignored if configured.\n* `:libvirt__tunnel_ip` - Sets the source IP of the Libvirt tunnel interface.\n  By default this is `127.0.0.1` for TCP and UDP tunnels and `239.255.1.1` for\n  Multicast tunnels. It populates the address field in the `<source\n  address="XXX">` of the interface xml configuration.\n* `:libvirt__tunnel_port` - Sets the source port the tcp/udp/mcast tunnel with\n  use. This port information is placed in the `<source port=XXX/>` section of\n  interface xml configuration.\n* `:libvirt__tunnel_local_port` - Sets the local port used by the udp tunnel\n  interface type. It populates the port field in the `<local port=XXX">`\n  section of the interface xml configuration. _(This feature only works in\n  Libvirt 1.2.20 and higher)_\n* `:libvirt__tunnel_local_ip` - Sets the local IP used by the udp tunnel\n  interface type. It populates the ip entry of the `<local address=XXX">`\n  section of the interface xml configuration. _(This feature only works in\n  Libvirt 1.2.20 and higher)_\n* `:libvirt__guest_ipv6` - Enable or disable guest-to-guest IPv6 communication.\n  See [here](https://libvirt.org/formatnetwork.html#examplesPrivate6), and\n  [here](http://libvirt.org/git/?p=libvirt.git;a=commitdiff;h=705e67d40b09a905cd6a4b8b418d5cb94eaa95a8)\n  for for more information. *Note: takes either \'yes\' or \'no\' for value*\n* `:libvirt__ipv6_address` - Define ipv6 address, require also prefix.\n* `:libvirt__ipv6_prefix` - Define ipv6 prefix. generate string `<ip family="ipv6" address="address" prefix="prefix" >`\n* `:libvirt__iface_name` - Define a name for the private network interface.\n  With this feature one can [simulate physical link\n  failures](https://github.com/vagrant-libvirt/vagrant-libvirt/pull/498)\n* `:mac` - MAC address for the interface. *Note: specify this in lowercase\n  since Vagrant network scripts assume it will be!*\n* `:libvirt__mtu` - MTU size for the Libvirt network, if not defined, the\n  created network will use the Libvirt default (1500). VMs still need to set the\n  MTU accordingly.\n* `:model_type` - parameter specifies the model of the network adapter when you\n  create a domain value by default virtio KVM believe possible values, see the\n  documentation for Libvirt\n* `:libvirt__driver_name` - Define which network driver to use. [More\n  info](https://libvirt.org/formatdomain.html#elementsDriverBackendOptions)\n* `:libvirt__driver_queues` - Define a number of queues to be used for network\n  interface. Set equal to numer of vCPUs for best performance. [More\n  info](http://www.linux-kvm.org/page/Multiqueue)\n* `:autostart` - Automatic startup of network by the Libvirt daemon.\n  If not specified the default is \'false\'.\n* `:bus` - The bus of the PCI device. Both :bus and :slot have to be defined.\n* `:slot` - The slot of the PCI device. Both :bus and :slot have to be defined.\n\nWhen the option `:libvirt__dhcp_enabled` is to to \'false\' it shouldn\'t matter\nwhether the virtual network contains a DHCP server or not and vagrant-libvirt\nshould not fail on it. The only situation where vagrant-libvirt should fail is\nwhen DHCP is requested but isn\'t configured on a matching already existing\nvirtual network.\n\n### Public Network Options\n\n* `:dev` - Physical device that the public interface should use. Default is\n  \'eth0\'.\n* `:mode` - The mode in which the public interface should operate in. Supported\n  modes are available from the [libvirt\n  documentation](http://www.libvirt.org/formatdomain.html#elementsNICSDirect).\n  Default mode is \'bridge\'.\n* `:type` - is type of interface.(`<interface type="#{@type}">`)\n* `:mac` - MAC address for the interface.\n* `:network_name` - Name of Libvirt network to connect to.\n* `:portgroup` - Name of Libvirt portgroup to connect to.\n* `:ovs` - Support to connect to an Open vSwitch bridge device. Default is\n  \'false\'.\n* `:trust_guest_rx_filters` - Support trustGuestRxFilters attribute. Details\n  are listed [here](http://www.libvirt.org/formatdomain.html#elementsNICSDirect).\n  Default is \'false\'.\n\n### Management Network\n\nvagrant-libvirt uses a private network to perform some management operations on\nVMs. All VMs will have an interface connected to this network and an IP address\ndynamically assigned by Libvirt unless you set `:mgmt_attach` to \'false\'.\nThis is in addition to any networks you configure. The name and address\nused by this network are configurable at the provider level.\n\n* `management_network_name` - Name of Libvirt network to which all VMs will be\n  connected. If not specified the default is \'vagrant-libvirt\'.\n* `management_network_address` - Address of network to which all VMs will be\n  connected. Must include the address and subnet mask. If not specified the\n  default is \'192.168.121.0/24\'.\n* `management_network_mode` - Network mode for the Libvirt management network.\n  Specify one of veryisolated, none, nat or route options. Further documented\n  under [Private Networks](#private-network-options)\n* `management_network_guest_ipv6` - Enable or disable guest-to-guest IPv6\n  communication. See\n  [here](https://libvirt.org/formatnetwork.html#examplesPrivate6), and\n  [here](http://libvirt.org/git/?p=libvirt.git;a=commitdiff;h=705e67d40b09a905cd6a4b8b418d5cb94eaa95a8)\n  for for more information.\n* `management_network_autostart` - Automatic startup of mgmt network, if not\n  specified the default is \'false\'.\n* `management_network_pci_bus` -  The bus of the PCI device.\n* `management_network_pci_slot` -  The slot of the PCI device.\n* `management_network_mac` - MAC address of management network interface.\n* `management_network_domain` - Domain name assigned to the management network.\n\nYou may wonder how vagrant-libvirt knows the IP address a VM received.  Libvirt\ndoesn\'t provide a standard way to find out the IP address of a running domain.\nBut we do know the MAC address of the virtual machine\'s interface on the\nmanagement network. Libvirt is closely connected with dnsmasq, which acts as a\nDHCP server. dnsmasq writes lease information in the `/var/lib/libvirt/dnsmasq`\ndirectory. Vagrant-libvirt looks for the MAC address in this file and extracts\nthe corresponding IP address.\n\n## Additional Disks\n\nYou can create and attach additional disks to a VM via `libvirt.storage :file`.\nIt has a number of options:\n\n* `path` - Location of the disk image. If unspecified, a path is automtically\n  chosen in the same storage pool as the VMs primary disk.\n* `device` - Name of the device node the disk image will have in the VM, e.g.\n  *vdb*. If unspecified, the next available device is chosen.\n* `size` - Size of the disk image. If unspecified, defaults to 10G.\n* `type` - Type of disk image to create. Defaults to *qcow2*.\n* `bus` - Type of bus to connect device to. Defaults to *virtio*.\n* `cache` - Cache mode to use, e.g. `none`, `writeback`, `writethrough` (see\n  the [libvirt documentation for possible\n  values](http://libvirt.org/formatdomain.html#elementsDisks) or\n  [here](https://www.suse.com/documentation/sles11/book_kvm/data/sect1_chapter_book_kvm.html)\n  for a fuller explanation). Defaults to *default*.\n* `allow_existing` - Set to true if you want to allow the VM to use a\n  pre-existing disk. If the disk doesn\'t exist it will be created.\n  Disks with this option set to true need to be removed manually.\n* `shareable` - Set to true if you want to simulate shared SAN storage.\n* `serial` - Serial number of the disk device.\n\nThe following example creates two additional disks.\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.storage :file, :size => \'20G\'\n    libvirt.storage :file, :size => \'40G\', :type => \'raw\'\n  end\nend\n```\n\nFor shared SAN storage to work the following example can be used:\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.storage :file, :size => \'20G\', :path => \'my_shared_disk.img\', :allow_existing => true, :shareable => true, :type => \'raw\'\n  end\nend\n```\n\n### Reload behavior\n\nOn `vagrant reload` the following additional disk attributes are updated in\ndefined domain:\n\n* `bus` - Updated. Uses `device` as a search marker. It is not required to\n  define `device`, but it\'s recommended. If `device` is defined then the order\n  of addtitional disk definition becomes irrelevant.\n\n## CDROMs\n\nYou can attach up to four CDROMs to a VM via `libvirt.storage :file,\n:device => :cdrom`. Available options are:\n\n* `path` - The path to the iso to be used for the CDROM drive.\n* `dev` - The device to use (`hda`, `hdb`, `hdc`, or `hdd`). This will be\n  automatically determined if unspecified.\n* `bus` - The bus to use for the CDROM drive. Defaults to `ide`\n\nThe following example creates three CDROM drives in the VM:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.storage :file, :device => :cdrom, :path => \'/path/to/iso1.iso\'\n    libvirt.storage :file, :device => :cdrom, :path => \'/path/to/iso2.iso\'\n    libvirt.storage :file, :device => :cdrom, :path => \'/path/to/iso3.iso\'\n  end\nend\n```\n\n## Input\n\nYou can specify multiple inputs to the VM via `libvirt.input`. Available\noptions are listed below. Note that both options are required:\n\n* `type` - The type of the input\n* `bus` - The bus of the input\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # this is the default\n    # libvirt.input :type => "mouse", :bus => "ps2"\n\n    # very useful when having mouse issues when viewing VM via VNC\n    libvirt.input :type => "tablet", :bus => "usb"\n  end\nend\n```\n\n## PCI device passthrough\n\nYou can specify multiple PCI devices to passthrough to the VM via\n`libvirt.pci`. Available options are listed below. Note that all options are\nrequired:\n\n* `bus` - The bus of the PCI device\n* `slot` - The slot of the PCI device\n* `function` - The function of the PCI device\n\nYou can extract that information from output of `lspci` command. First\ncharacters of each line are in format `[<bus>]:[<slot>].[<func>]`. For example:\n\n```shell\n$ lspci| grep NVIDIA\n03:00.0 VGA compatible controller: NVIDIA Corporation GK110B [GeForce GTX TITAN Black] (rev a1)\n```\n\nIn that case `bus` is `0x03`, `slot` is `0x00` and `function` is `0x0`.\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.pci :bus => \'0x06\', :slot => \'0x12\', :function => \'0x5\'\n\n    # Add another one if it is neccessary\n    libvirt.pci :bus => \'0x03\', :slot => \'0x00\', :function => \'0x0\'\n  end\nend\n```\n\nNote! Above options affect configuration only at domain creation. It won\'t change VM behaviour on `vagrant reload` after domain was created.\n\nDon\'t forget to [set](#domain-specific-options) `kvm_hidden` option to `true` especially if you are passthroughing NVIDIA GPUs. Otherwise GPU is visible from VM but cannot be operated.\n\n\n## Using USB Devices\n\nThere are several ways to pass a USB device through to a running instance:\n* Use `libvirt.usb` to [attach a USB device at boot](#usb-device-passthrough), with the device ID specified in the Vagrantfile\n* Use a client (such as `virt-viewer` or `virt-manager`) to attach the device at runtime [via USB redirectors](#usb-redirector-devices)\n* Use `virsh attach-device` once the VM is running (however, this is outside the scope of this readme)\n\nIn all cases, if you wish to use a high-speed USB device,\nyou will need to use `libvirt.usb_controller` to specify a USB2 or USB3 controller,\nas the default configuration only exposes a USB1.1 controller.\n\n### USB Controller Configuration\n\nThe USB controller can be configured using `libvirt.usb_controller`, with the following options:\n\n* `model` - The USB controller device model to emulate. (mandatory)\n* `ports` - The number of devices that can be connected to the controller.\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # Set up a USB3 controller\n    libvirt.usb_controller :model => "nec-xhci"\n  end\nend\n```\n\nSee the [libvirt documentation](https://libvirt.org/formatdomain.html#elementsControllers) for a list of valid models.\n\n\n### USB Device Passthrough\n\nYou can specify multiple USB devices to passthrough to the VM via\n`libvirt.usb`. The device can be specified by the following options:\n\n* `bus` - The USB bus ID, e.g. "1"\n* `device` - The USB device ID, e.g. "2"\n* `vendor` - The USB devices vendor ID (VID), e.g. "0x1234"\n* `product` - The USB devices product ID (PID), e.g. "0xabcd"\n\nAt least one of these has to be specified, and `bus` and `device` may only be\nused together.\n\nThe example values above match the device from the following output of `lsusb`:\n\n```\nBus 001 Device 002: ID 1234:abcd Example device\n```\n\nAdditionally, the following options can be used:\n\n* `startupPolicy` - Is passed through to Libvirt and controls if the device has\n  to exist.  Libvirt currently allows the following values: "mandatory",\n  "requisite", "optional".\n\n\n### USB Redirector Devices\nYou can specify multiple redirect devices via `libvirt.redirdev`. There are two types, `tcp` and `spicevmc` supported, for forwarding USB-devices to the guest. Available options are listed below.\n\n* `type` - The type of the USB redirector device. (`tcp` or `spicevmc`)\n* `host` - The host where the device is attached to. (mandatory for type `tcp`)\n* `port` - The port where the device is listening. (mandatory for type `tcp`)\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # add two devices using spicevmc channel\n    (1..2).each do\n      libvirt.redirdev :type => "spicevmc"\n    end\n    # add device, provided by localhost:4000\n    libvirt.redirdev :type => "tcp", :host => "localhost", :port => "4000"\n  end\nend\n```\n\nNote that in order to enable USB redirection with Spice clients,\nyou may need to also set `libvirt.graphics_type = "spice"`\n\n#### Filter for USB Redirector Devices\nYou can define filter for redirected devices. These filters can be positiv or negative, by setting the mandatory option `allow=yes` or `allow=no`. All available options are listed below. Note the option `allow` is mandatory.\n\n* `class` - The device class of the USB device. A list of device classes is available on [Wikipedia](https://en.wikipedia.org/wiki/USB#Device_classes).\n* `vendor` - The vendor of the USB device.\n* `product` - The product id of the USB device.\n* `version` - The version of the USB device. Note that this is the version of `bcdDevice`\n* `allow` - allow or disallow redirecting this device. (mandatory)\n\nYou can extract that information from output of `lsusb` command. Every line contains the information in format `Bus [<bus>] Device [<device>]: ID [<vendor>:[<product>]`. The `version` can be extracted from the detailed output of the device using `lsusb -D /dev/usb/[<bus>]/[<device>]`. For example:\n\n```shell\n# get bcdDevice from\n$: lsusb\nBus 001 Device 009: ID 08e6:3437 Gemalto (was Gemplus) GemPC Twin SmartCard Reader\n\n$: lsusb -D /dev/bus/usb/001/009 | grep bcdDevice\n  bcdDevice            2.00\n```\n\nIn this case, the USB device with `class 0x0b`, `vendor 0x08e6`, `product 0x3437` and `bcdDevice version 2.00` is allowed to be redirected to the guest. All other devices will be refused.\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.redirdev :type => "spicevmc"\n    libvirt.redirfilter :class => "0x0b" :vendor => "0x08e6" :product => "0x3437" :version => "2.00" :allow => "yes"\n    libvirt.redirfilter :allow => "no"\n  end\nend\n```\n\n## Random number generator passthrough\n\nYou can pass through `/dev/random` to your VM by configuring the domain like this:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # Pass through /dev/random from the host to the VM\n    libvirt.random :model => \'random\'\n  end\nend\n```\n\nAt the moment only the `random` backend is supported.\n\n## Watchdog device\nA virtual hardware watchdog device can be added to the guest via the `libvirt.watchdog` element. The option `model` is mandatory and could have on of the following values.\n\n* `i6300esb` - the recommended device, emulating a PCI Intel 6300ESB\n* \'ib700` - emulating an ISA iBase IB700\n* `diag288` - emulating an S390 DIAG288 device\n\nThe optional action attribute describes what `action` to take when the watchdog expires. Valid values are specific to the underlying hypervisor. The default behavior is `reset`.\n\n* `reset` - default, forcefully reset the guest\n* `shutdown` - gracefully shutdown the guest (not recommended)\n* `poweroff` - forcefully power off the guest\n* `pause` - pause the guest\n* `none` - do nothing\n* `dump` - automatically dump the guest\n* `inject-nmi` - inject a non-maskable interrupt into the guest\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # Add Libvirt watchdog device model i6300esb\n    libvirt.watchdog :model => \'i6300esb\', :action => \'reset\'\n  end\nend\n```\n\n## Smartcard device\nA virtual smartcard device can be supplied to the guest via the `libvirt.smartcard` element. The option `mode` is mandatory and currently only value `passthrough` is supported. The value `spicevmc` for option `type` is default value and can be supressed. On using `type = tcp`, the options `source_mode`, `source_host` and `source_service` are mandatory.\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # Add smartcard device with type \'spicevmc\'\n    libvirt.smartcard :mode => \'passthrough\', :type => \'spicevmc\'\n  end\nend\n```\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # Add smartcard device with type \'tcp\'\n    domain.smartcard :mode => \'passthrough\', :type => \'tcp\', :source_mode => \'bind\', :source_host => \'127.0.0.1\', :source_service => \'2001\'\n  end\nend\n```\n## Hypervisor Features\n\nHypervisor features can be specified via `libvirt.features` as a list. The default\noptions that are enabled are `acpi`, `apic` and `pae`. If you define `libvirt.features`\nyou overwrite the defaults, so keep that in mind.\n\nAn example:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # Specify the default hypervisor features\n    libvirt.features = [\'acpi\', \'apic\', \'pae\' ]\n  end\nend\n```\n\nA different example for ARM boards:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # Specify the default hypervisor features\n    libvirt.features = ["apic", "gic version=\'2\'" ]\n  end\nend\n```\n\nYou can also specify a special set of features that help improve the behavior of guests\nrunning Microsoft Windows.\n\nYou can specify HyperV features via `libvirt.hyperv_feature`. Available\noptions are listed below. Note that both options are required:\n\n* `name` - The name of the feature Hypervisor feature (see Libvirt doc)\n* `state` - The state for this feature which can be either `on` or `off`.\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # Relax constraints on timers\n    libvirt.hyperv_feature :name => \'relaxed\', :state => \'on\'\n    # Enable virtual APIC\n    libvirt.hyperv_feature :name => \'vapic\', :state => \'on\'\n  end\nend\n```\n\n## CPU features\n\nYou can specify CPU feature policies via `libvirt.cpu_feature`. Available\noptions are listed below. Note that both options are required:\n\n* `name` - The name of the feature for the chosen CPU (see Libvirt\'s\n  `cpu_map.xml`)\n* `policy` - The policy for this feature (one of `force`, `require`,\n  `optional`, `disable` and `forbid` - see Libvirt documentation)\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # The feature will not be supported by virtual CPU.\n    libvirt.cpu_feature :name => \'hypervisor\', :policy => \'disable\'\n    # Guest creation will fail unless the feature is supported by host CPU.\n    libvirt.cpu_feature :name => \'vmx\', :policy => \'require\'\n    # The virtual CPU will claim the feature is supported regardless of it being supported by host CPU.\n    libvirt.cpu_feature :name => \'pdpe1gb\', :policy => \'force\'\n  end\nend\n```\n\n## Memory Backing\n\nYou can specify memoryBacking options via `libvirt.memorybacking`. Available options are shown below. Full documentation is available at the [libvirt _memoryBacking_ section](https://libvirt.org/formatdomain.html#elementsMemoryBacking).\n\nNOTE: The hugepages `<page>` element is not yet supported\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.memorybacking :hugepages\n    libvirt.memorybacking :nosharepages\n    libvirt.memorybacking :locked\n    libvirt.memorybacking :source, :type => \'file\'\n    libvirt.memorybacking :access, :mode => \'shared\'\n    libvirt.memorybacking :allocation, :mode => \'immediate\'\n  end\nend\n```\n\n## No box and PXE boot\n\nThere is support for PXE booting VMs with no disks as well as PXE booting VMs\nwith blank disks. There are some limitations:\n\n* Requires Vagrant 1.6.0 or newer\n* No provisioning scripts are ran\n* No network configuration is being applied to the VM\n* No SSH connection can be made\n* `vagrant halt` will only work cleanly if the VM handles ACPI shutdown signals\n\nIn short, VMs without a box can be created, halted and destroyed but all other\nfunctionality cannot be used.\n\nAn example for a PXE booted VM with no disks whatsoever:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.define :pxeclient do |pxeclient|\n    pxeclient.vm.provider :libvirt do |domain|\n      domain.boot \'network\'\n    end\n  end\nend\n```\n\nAnd an example for a PXE booted VM with no box but a blank disk which will boot from this HD if the NICs fail to PXE boot:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.define :pxeclient do |pxeclient|\n    pxeclient.vm.provider :libvirt do |domain|\n      domain.storage :file, :size => \'100G\', :type => \'qcow2\'\n      domain.boot \'network\'\n      domain.boot \'hd\'\n    end\n  end\nend\n```\n\nExample for vm with 2 networks and only 1 is bootable and has dhcp server in this subnet, for example foreman with dhcp server\nName of network "foreman_managed" is key for define boot order\n```ruby\n    config.vm.define :pxeclient do |pxeclient|\n      pxeclient.vm.network :private_network,ip: \'10.0.0.5\',\n            libvirt__network_name: "foreman_managed",\n            libvirt__dhcp_enabled: false,\n            libvirt__host_ip: \'10.0.0.1\'\n\n       pxeclient.vm.provider :libvirt do |domain|\n          domain.memory = 1000\n          boot_network = {\'network\' => \'foreman_managed\'}\n          domain.storage :file, :size => \'100G\', :type => \'qcow2\'\n          domain.boot boot_network\n          domain.boot \'hd\'\n        end\n      end\n```\n\n## SSH Access To VM\n\nvagrant-libvirt supports vagrant\'s [standard ssh\nsettings](https://docs.vagrantup.com/v2/vagrantfile/ssh_settings.html).\n\n## Forwarded Ports\n\nvagrant-libvirt supports Forwarded Ports via ssh port forwarding. Please note\nthat due to a well known limitation only the TCP protocol is supported. For\neach `forwarded_port` directive you specify in your Vagrantfile,\nvagrant-libvirt will maintain an active ssh process for the lifetime of the VM.\nIf your VM should happen to be rebooted, the SSH session will need to be\nrestablished by halting the VM and bringing it back up.\n\nvagrant-libvirt supports an additional `forwarded_port` option `gateway_ports`\nwhich defaults to `false`, but can be set to `true` if you want the forwarded\nport to be accessible from outside the Vagrant host.  In this case you should\nalso set the `host_ip` option to `\'*\'` since it defaults to `\'localhost\'`.\n\nYou can also provide a custom adapter to forward from by \'adapter\' option.\nDefault is `eth0`.\n\n**Internally Accessible Port Forward**\n\n`config.vm.network :forwarded_port, guest: 80, host: 2000`\n\n**Externally Accessible Port Forward**\n\n`config.vm.network :forwarded_port, guest: 80, host: 2000, host_ip: "0.0.0.0"`\n\n## Synced Folders\n\nVagrant automatically syncs the project folder on the host to `/vagrant` in the guest. You can also configure\nadditional synced folders.\n\n`vagrant-libvirt` supports bidirectional synced folders via [NFS](https://en.wikipedia.org/wiki/Network_File_System) or [VirtFS](http://www.linux-kvm.org/page/VirtFS) ([9p or Plan 9](https://en.wikipedia.org/wiki/9P_(protocol))) and\nunidirectional via rsync. The default is NFS. Difference between NFS and 9p is explained [here](https://unix.stackexchange.com/questions/240281/virtfs-plan-9-vs-nfs-as-tool-for-share-folder-for-virtual-machine).\n\nYou can change the synced folder type for `/vagrant` by explicity configuring\nit an setting the type, e.g.\n\n```shell\nconfig.vm.synced_folder \'./\', \'/vagrant\', type: \'rsync\'\n```\n\nor\n\n```shell\nconfig.vm.synced_folder \'./\', \'/vagrant\', type: \'9p\', disabled: false, accessmode: "squash", owner: "1000"\n```\n\nor\n\n```shell\nconfig.vm.synced_folder \'./\', \'/vagrant\', type: \'9p\', disabled: false, accessmode: "mapped", mount: false\n```\n\nFor 9p shares, a `mount: false` option allows to define synced folders without\nmounting them at boot.\n\nFurther documentation on using 9p can be found in [kernel docs](https://www.kernel.org/doc/Documentation/filesystems/9p.txt) and in [QEMU wiki](https://wiki.qemu.org/Documentation/9psetup#Starting_the_Guest_directly). Please do note that 9p depends on support in the guest and not all distros come with the 9p module by default.\n\n**SECURITY NOTE:** for remote Libvirt, nfs synced folders requires a bridged\npublic network interface and you must connect to Libvirt via ssh.\n\n## QEMU Session Support\n\nvagrant-libvirt supports using the QEMU session connection to maintain Vagrant VMs. As the session connection does not have root access to the system features which require root will not work. Access to networks created by the system QEMU connection can be granted by using the [QEMU bridge helper](https://wiki.qemu.org/Features/HelperNetworking). The bridge helper is enabled by default on some distros but may need to be enabled/installed on others.\n\nAn example configuration of a machine using the QEMU session connection:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    # Use QEMU session instead of system connection\n    libvirt.qemu_use_session = true\n    # URI of QEMU session connection, default is as below\n    libvirt.uri = \'qemu:///session\'\n    # URI of QEMU system connection, use to obtain IP address for management\n    libvirt.system_uri = \'qemu:///system\'\n    # Path to store Libvirt images for the virtual machine, default is as ~/.local/share/libvirt/images\n    libvirt.storage_pool_path = \'/home/user/.local/share/libvirt/images\'\n    # Management network device\n    libvirt.management_network_device = \'virbr0\'\n  end\n\n  # Public network configuration using existing network device\n  # Note: Private networks do not work with QEMU session enabled as root access is required to create new network devices\n  config.vm.network :public_network, :dev => "virbr1",\n      :mode => "bridge",\n      :type => "bridge"\nend\n```\n\n## Customized Graphics\n\nvagrant-libvirt supports customizing the display and video settings of the\nmanaged guest.  This is probably most useful for VNC-type displays with\nmultiple guests.  It lets you specify the exact port for each guest to use\ndeterministically.\n\nHere is an example of using custom display options:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.graphics_port = 5901\n    libvirt.graphics_ip = \'0.0.0.0\'\n    libvirt.video_type = \'qxl\'\n  end\nend\n```\n\n## TPM Devices\n\nModern versions of Libvirt support connecting to TPM devices on the host\nsystem. This allows you to enable Trusted Boot Extensions, among other\nfeatures, on your guest VMs.\n\nIn general, you will only need to modify the `tpm_path` variable in your guest\nconfiguration. However, advanced usage, such as the application of a Software\nTPM, may require modifying the `tpm_model` and `tpm_type` variables.\n\nThe TPM options will only be used if you specify a TPM path. Declarations of\nany TPM options without specifying a path will result in those options being\nignored.\n\nHere is an example of using the TPM options:\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.tpm_model = \'tpm-tis\'\n    libvirt.tpm_type = \'passthrough\'\n    libvirt.tpm_path = \'/dev/tpm0\'\n  end\nend\n```\n\n## Libvirt communication channels\n\nFor certain functionality to be available within a guest, a private\ncommunication channel must be established with the host. Two notable examples\nof this are the QEMU guest agent, and the Spice/QXL graphics type.\n\nBelow is a simple example which exposes a virtio serial channel to the guest.\nNote: in a multi-VM environment, the channel would be created for all VMs.\n\n```ruby\nvagrant.configure(2) do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.channel :type => \'unix\', :target_name => \'org.qemu.guest_agent.0\', :target_type => \'virtio\'\n  end\nend\n```\n\nBelow is the syntax for creating a spicevmc channel for use by a qxl graphics\ncard.\n\n```ruby\nvagrant.configure(2) do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.channel :type => \'spicevmc\', :target_name => \'com.redhat.spice.0\', :target_type => \'virtio\'\n  end\nend\n```\n\nThese settings can be specified on a per-VM basis, however the per-guest\nsettings will OVERRIDE any global \'config\' setting. In the following example,\nwe create 3 VMs with the following configuration:\n\n* **master**: No channel settings specified, so we default to the provider\n  setting of a single virtio guest agent channel.\n* **node1**: Override the channel setting, setting both the guest agent\n  channel, and a spicevmc channel\n* **node2**: Override the channel setting, setting both the guest agent\n  channel, and a \'guestfwd\' channel. TCP traffic sent by the guest to the given\n  IP address and port is forwarded to the host socket `/tmp/foo`. Note: this\n  device must be unique for each VM.\n\nFor example:\n\n```ruby\nVagrant.configure(2) do |config|\n  config.vm.box = "fedora/24-cloud-base"\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.channel :type => \'unix\', :target_name => \'org.qemu.guest_agent.0\', :target_type => \'virtio\'\n  end\n\n  config.vm.define "master" do |master|\n    master.vm.provider :libvirt do |domain|\n        domain.memory = 1024\n    end\n  end\n  config.vm.define "node1" do |node1|\n    node1.vm.provider :libvirt do |domain|\n      domain.channel :type => \'unix\', :target_name => \'org.qemu.guest_agent.0\', :target_type => \'virtio\'\n      domain.channel :type => \'spicevmc\', :target_name => \'com.redhat.spice.0\', :target_type => \'virtio\'\n    end\n  end\n  config.vm.define "node2" do |node2|\n    node2.vm.provider :libvirt do |domain|\n      domain.channel :type => \'unix\', :target_name => \'org.qemu.guest_agent.0\', :target_type => \'virtio\'\n      domain.channel :type => \'unix\', :target_type => \'guestfwd\', :target_address => \'192.0.2.42\', :target_port => \'4242\',\n                     :source_path => \'/tmp/foo\'\n    end\n  end\nend\n```\n\n## Custom command line arguments\nYou can also specify multiple qemuargs arguments for qemu-system\n\n* `value` - Value\n\n```ruby\nVagrant.configure("2") do |config|\n  config.vm.provider :libvirt do |libvirt|\n    libvirt.qemuargs :value => "-device"\n    libvirt.qemuargs :value => "intel-iommu"\n  end\nend\n```\n\n## Box Format\n\nYou can view an example box in the\n[`example_box/directory`](https://github.com/vagrant-libvirt/vagrant-libvirt/tree/master/example_box).\nThat directory also contains instructions on how to build a box.\n\nThe box is a tarball containing:\n\n* qcow2 image file named `box.img`\n* `metadata.json` file describing box image (`provider`, `virtual_size`,\n  `format`)\n* `Vagrantfile` that does default settings for the provider-specific\n  configuration for this provider\n\n## Create Box\n\nTo create a vagrant-libvirt box from a qcow2 image, run `create_box.sh`\n(located in the tools directory):\n\n```shell\n$ create_box.sh ubuntu14.qcow2\n```\n\nYou can also create a box by using [Packer](https://packer.io). Packer\ntemplates for use with vagrant-libvirt are available at\nhttps://github.com/jakobadam/packer-qemu-templates. After cloning that project\nyou can build a vagrant-libvirt box by running:\n\n```shell\n$ cd packer-qemu-templates\n$ packer build ubuntu-14.04-server-amd64-vagrant.json\n```\n\n## Package Box from VM\n\nvagrant-libvirt has native support for [`vagrant\npackage`](https://www.vagrantup.com/docs/cli/package.html) via\nlibguestfs [virt-sysprep](http://libguestfs.org/virt-sysprep.1.html).\nvirt-sysprep operations can be customized via the\n`VAGRANT_LIBVIRT_VIRT_SYSPREP_OPERATIONS` environment variable; see the\n[upstream\ndocumentation](http://libguestfs.org/virt-sysprep.1.html#operations) for\nfurther details especially on default sysprep operations enabled for\nyour system.\n\nFor example, on Chef [bento](https://github.com/chef/bento) VMs that\nrequire SSH hostkeys already set (e.g. bento/debian-7) as well as leave\nexisting LVM UUIDs untouched (e.g. bento/ubuntu-18.04), these can be\npackaged into vagrant-libvirt boxes like so:\n\n```shell\n$ export VAGRANT_LIBVIRT_VIRT_SYSPREP_OPERATIONS="defaults,-ssh-userdir,-ssh-hostkeys,-lvm-uuids"\n$ vagrant package\n```\n\n## Troubleshooting VMs\n\nThe first step for troubleshooting a VM image that appears to not boot correctly,\nor hangs waiting to get an IP, is to check it with a VNC viewer. A key thing\nto remember is that if the VM doesn\'t get an IP, then vagrant can\'t communicate\nwith it to configure anything, so a problem at this stage is likely to come from \nthe VM, but we\'ll outline the tools and common problems to help you troubleshoot\nthat.\n\nBy default, when you create a new VM, a vnc server will listen on `127.0.0.1` on\nport `TCP5900`. If you connect with a vnc viewer you can see the boot process. If\nyour VM isn\'t listening on `5900` by default, you can use `virsh dumpxml` to find\nout which port it\'s listening on, or can configure it with `graphics_port` and\n`graphics_ip` (see \'Domain Specific Options\' above).\n\nNote: Connecting with the console (`virsh console`) requires additional config,\nso some VMs may not show anything on the console at all, instead displaying it in\nthe VNC console. The issue with the text console is that you also need to build the \nimage used to tell the kernel to output to the console during boot, and typically \nmost do not have this built in.\n\nProblems we\'ve seen in the past include:\n- Forgetting to remove `/etc/udev/rules.d/70-persistent-net.rules` before packaging\nthe VM\n- VMs expecting a specific disk device to be connected\n\nIf you\'re still confused, check the Github Issues for this repo for anything that\nlooks similar to your problem.\n\n[Github Issue #1032](https://github.com/vagrant-libvirt/vagrant-libvirt/issues/1032) \ncontains some historical troubleshooting for VMs that appeared\nto hang. \n\nDid you hit a problem that you\'d like to note here to save time in the future?\nPlease do!\n\n\n## Development\n\nTo work on the `vagrant-libvirt` plugin, clone this repository out, and use\n[Bundler](http://gembundler.com) to get the dependencies:\n\n```shell\n$ git clone https://github.com/vagrant-libvirt/vagrant-libvirt.git\n$ cd vagrant-libvirt\n$ bundle install\n```\n\nOnce you have the dependencies, verify the unit tests pass with `rspec`:\n\n```shell\n$ bundle exec rspec spec/\n```\n\nIf those pass, you\'re ready to start developing the plugin. You can test the\nplugin without installing it into your Vagrant environment by just creating a\n`Vagrantfile` in the top level of this directory (it is gitignored) that uses\nit. Don\'t forget to add following line at the beginning of your `Vagrantfile`\nwhile in development mode:\n\n```ruby\nVagrant.require_plugin "vagrant-libvirt"\n```\n\nNow you can use bundler to execute Vagrant:\n\n```shell\n$ bundle exec vagrant up --provider=libvirt\n```\n\n**IMPORTANT NOTE:** bundle is crucial. You need to use bundled Vagrant.\n\n## Contributing\n\n1. Fork it\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am \'Add some feature\'`)\n4. Push to the branch (`git push origin my-new-feature`)\n5. Create new Pull Request\n'