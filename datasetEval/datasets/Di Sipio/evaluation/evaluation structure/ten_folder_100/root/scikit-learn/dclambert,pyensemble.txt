b'pyensemble v0.41\n================\n\n###### An implementation of [Caruana et al\'s Ensemble Selection algorithm] (http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf) [1][2] in Python, based on [scikit-learn](http://scikit-learn.org).\n\n###### From the abstract:\n\n> We present a method for constructing ensembles from libraries of thousands of models.\nModel libraries are generated using different learning algorithms and parameter settings.\nForward stepwise selection is used to add to the ensemble the models that maximize its\nperformance.  Ensemble selection allows ensembles to be optimized to performance metrics\nsuch as accuracy, cross entropy, mean precision or ROC Area.  Experiments with seven test\nproblems and ten metrics demonstrate the benefit of ensemble selection.\n\nIt\'s a work in progress, so things can/might/will change.\n\n__David C. Lambert__  \n__dcl [at] panix [dot] com__  \n\n__Copyright \xc2\xa9 2013__  \n__License: Simple BSD__\n\nFiles\n-----\n\n#### __ensemble.py__\n\nContaining the EnsembleSelectionClassifier object\n\nThe EnsembleSelectionClassifier object tries to implement all of the methods in the combined\npaper, including internal cross validation, bagged ensembling, initialization with the best\nmodels, pruning of the worst models prior to selection, and sampling with replacement of the\nmodel candidates.\n\nIt uses sqlite as the backing store containing pickled unfitted models, fitted model \'siblings\'\nfor each internal cross validation fold, scores and predictions for each model, and the list of\nmodel ids and weightings for the final ensemble.\n\nHillclimbing can be performed using auc, accuracy, rmse, cross entropy or F1 score.\n\nIf the object is initialized with the _model_ parameter equal to None, the object tries to load\na fitted ensemble from the database specified.\n\n__*(NOTE: Expects class labels to be sequential integers starting at zero [for now].)*__\n    \n#### __model_library.py__\n\nExample model library building code.\n\n#### __ensemble_train.py__\n\nTraining utility to run ensemble selection on svm data files.\n\nThe user can choose from the following candidate models:\n\n*    sgd     : Stochastic Gradient Descent\n*    svc     : Support Vector Machines\n*    gbc     : Gradient Boosting Classifiers\n*    dtree   : Decision Trees\n*    forest  : Random Forests\n*    extra   : Extra Trees\n*    kmp     : KMeans->LogisticRegression Pipelines\n*    kernp   : Nystroem Approx->Logistic Regression Pipelines\n\nSome model choices are __very slow__.  The default is to use decision trees, which are reasonably fast.\n\nThe simplest command line is:\n\n    unix> ./ensemble_train.py some_dbfile.db some_data.svm\n\n__*(NOTE: Expects \'some_dbfile.db\' not to exist, and will quit if it does [so you don\'t accidentally blow away your model].)*__\n    \nFull usage is:\n\n```\nusage: ensemble_train.py [-h]\n                         [-M {svc,sgd,gbc,dtree,forest,extra,kmp,kernp}\n                            [{svc,sgd,gbc,dtree,forest,extra,kmp,kernp} ...]]\n                         [-S {f1,auc,rmse,accuracy,xentropy}] [-b N_BAGS]\n                         [-f BAG_FRACTION] [-B N_BEST] [-m MAX_MODELS]\n                         [-F N_FOLDS] [-p PRUNE_FRACTION] [-u] [-U]\n                         [-e EPSILON] [-t TEST_SIZE] [-s SEED] [-v]\n                         db_file data_file\n\nEnsembleSelectionClassifier training harness\n\npositional arguments:\n  db_file               sqlite db file for backing store\n  data_file             training data in svm format\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -M {svc,sgd,gbc,dtree,forest,extra,kmp,kernp}\n    [{svc,sgd,gbc,dtree,forest,extra,kmp,kernp} ...]\n                        model types to include as ensemble candidates\n                        (default: [\'dtree\'])\n  -S {f1,auc,rmse,accuracy,xentropy}\n                        scoring metric used for hillclimbing (default:\n                        accuracy)\n  -b N_BAGS             bags to create (default: 20)\n  -f BAG_FRACTION       fraction of models in each bag (after pruning)\n                        (default: 0.25)\n  -B N_BEST             number of best models in initial ensemble (default: 5)\n  -m MAX_MODELS         maximum number of models per bagged ensemble (default:\n                        25)\n  -F N_FOLDS            internal cross-validation folds (default: 3)\n  -p PRUNE_FRACTION     fraction of worst models pruned pre-selection\n                        (default: 0.75)\n  -u                    use epsilon to stop adding models (default: False)\n  -U                    use bootstrap sample to generate training/hillclimbing\n                        folds (default: False)\n  -e EPSILON            score improvement threshold to include new model\n                        (default: 0.0001)\n  -t TEST_SIZE          fraction of data to use for testing (default: 0.75)\n  -s SEED               random seed\n  -v                    show progress messages\n```\n\n\n\n#### __ensemble_predict.py__\n\nGet predictions from trained EnsembleSelectionClassifier given\nsvm format data file.\n\nCan output predicted classes or probabilities from the full\nensemble or just the best model.\n\nExpects to find a trained ensemble in the sqlite db specified.\n\n```\nusage: ensemble_predict.py [-h] [-s {best,ens}] [-p] db_file data_file\n\nGet EnsembleSelectionClassifier predictions\n\npositional arguments:\n  db_file        sqlite db file containing model\n  data_file      testing data in svm format\n\noptional arguments:\n  -h, --help     show this help message and exit\n  -s {best,ens}  choose source of prediction ["best", "ens"]\n  -p             predict probabilities\n```\n\nRequirements\n------------\n\nWritten using Python 2.7.3, numpy 1.6.1, scipy 0.10.1, scikit-learn 0.14.1 and sqlite 3.7.14\n\n\nReferences\n----------\n[1] Caruana, et al, "Ensemble Selection from Libraries of Rich Models", Proceedings of the 21st International Conference on Machine Learning (ICML `04).\n    \n[2] Caruana, et al, "Getting the Most Out of Ensemble Selection", Proceedings of the 6th International Conference on Data Mining (ICDM `06).\n    \n\n'