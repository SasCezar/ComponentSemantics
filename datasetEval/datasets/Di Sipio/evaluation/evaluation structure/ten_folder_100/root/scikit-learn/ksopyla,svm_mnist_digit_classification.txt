b"# SVM MNIST digit classification in python using scikit-learn\n\nThe project presents the well-known problem of [MNIST handwritten digit classification](https://en.wikipedia.org/wiki/MNIST_database).\nFor the purpose of this tutorial, I will use [Support Vector Machine (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) \nthe algorithm with raw pixel features. \nThe solution is written in python with use of [scikit-learn](http://scikit-learn.org/stable/) easy to use machine learning library.\n\n![Sample MNIST digits visualization](/images/mnist_digits.png)\n\n\n\nThe goal of this project is not to achieve the state of the art performance, rather teach you \n**how to train SVM classifier on image data** with use of SVM from sklearn. \nAlthough the solution isn't optimized for high accuracy, the results are quite good (see table below). \n\nIf you want to hit the top performance, this two resources will show you current state of the art solutions:\n\n* [Who is the best in MNIST ?](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)\n* [Kaggle digit recognizer comptetition](https://www.kaggle.com/c/digit-recognizer)\n\nThe table below shows some results in comparison with other models:\n\n\n| Method                                     | Accuracy | Comments     |\n|--------------------------------------------|----------|--------------|\n| Random forest                              | 0.937    |              |\n| Simple one-layer neural network            | 0.926    |              |\n| Simple 2 layer convolutional network       | 0.981    |              |\n| SVM RBF                                    | 0.9852   | C=5, gamma=0.05 |\n| Linear SVM + Nystroem kernel approximation |          |              |\n| Linear SVM + Fourier kernel approximation  |          |              |\n\n\n## Project Setup\n\nThis tutorial was written and tested on Ubuntu 18.10.\nProject contains the Pipfile with all necessary libraries\n\n* Python - version >= 3.6 \n* pipenv - package and virtual environment management \n* numpy\n* matplotlib\n* scikit-learn\n\n\n1. Install Python.\n1. [Install pipenv](https://pipenv.readthedocs.io/en/latest/install/#pragmatic-installation-of-pipenv)\n1. Git clone the repository\n1. Install all necessary python packages executing this command in terminal\n\n```\ngit clone https://github.com/ksopyla/svm_mnist_digit_classification.git\ncd svm_mnist_digit_classification\npipenv install\n```\n\n\n\n\n## Solution\n\nIn this tutorial, I use two approaches to SVM learning. \nFirst, uses classical SVM with RBF kernel. The drawback of this solution is rather long training on big datasets, although the accuracy with good parameters is high. \nThe second, use Linear SVM, which allows for training in O(n) time. In order to achieve high accuracy, we use some trick. We approximate RBF kernel in a high dimensional space by embeddings. The theory behind is quite complicated, \nhowever [sklearn has ready to use classes for kernel approximation](http://scikit-learn.org/stable/modules/kernel_approximation.html#kernel-approximation). \nWe will use:\n\n* Nystroem kernel approximation\n* Fourier kernel approximation\n\nThe code was tested with python 3.6.\n\n\n## How the project is organized\n\nProject consist of three files:\n\n* _mnist_helpers.py_ - contains some visualization functions: MNIST digits visualization and confusion matrix\n* _svm_mnist_classification.py_ - script for SVM with RBF kernel classification\n* _svm_mnist_embedings.py_ - script for linear SVM with embedings\n\n### SVM with RBF kernel\n\nThe **svm_mnist_classification.py** script downloads the MNIST database and visualizes some random digits.\nNext, it standardizes the data (mean=0, std=1) and launch grid search with cross-validation for finding the best parameters.\n\n1. MNIST SVM kernel RBF Param search C=[0.1,0.5,1,5], gamma=[0.01,0.0.05,0.1,0.5].\n\nGrid search was done for params C and gamma, where C=[0.1,0.5,1,5], gamma=[0.01,0.0.05,0.1,0.5].\nI have examined only 4x4 different param pairs with 3 fold cross validation so far (4x4x3=48 models), \nthis procedure takes 3687.2min :) (2 days, 13:56:42.531223 exactly) on one core CPU.\n\nParam space was generated with numpy logspace and outer matrix multiplication. \n```\nC_range = np.outer(np.logspace(-1, 0, 2),np.array([1,5]))\n# flatten matrix, change to 1D numpy array\nC_range = C_range.flatten()\n\ngamma_range = np.outer(np.logspace(-2, -1, 2),np.array([1,5]))\ngamma_range = gamma_range.flatten()\n\n```\nOf course, you can broaden the range of parameters, but this will increase the computation time.\n\n\n![SVM RBF param space](https://plon.io/files/58d3af091b12ce00012bd6e1)\n\nGrid search is very time consuming process, so you can use my best parameters \n(from the range c=[0.1,5], gamma=[0.01,0.05]):\n* C = 5\n* gamma = 0.05\n* accuracy = 0.9852\n\n\n```\nConfusion matrix:\n[[1014    0    2    0    0    2    2    0    1    3]\n [   0 1177    2    1    1    0    1    0    2    1]\n [   2    2 1037    2    0    0    0    2    5    1]\n [   0    0    3 1035    0    5    0    6    6    2]\n [   0    0    1    0  957    0    1    2    0    3]\n [   1    1    0    4    1  947    4    0    5    1]\n [   2    0    1    0    2    0 1076    0    4    0]\n [   1    1    8    1    1    0    0 1110    2    4]\n [   0    4    2    4    1    6    0    1 1018    1]\n [   3    1    0    7    5    2    0    4    9  974]]\nAccuracy=0.985238095238\n```\n\n\n2. MNIST SVM kernel RBF Param search C=[0.1,0.5,1,5, 10, 50], gamma=[0.001, 0.005, 0.01,0.0.05,0.1,0.5].\n\nThis much broaden search 6x8 params with 3 fold cross validation gives 6x8x3=144 models, \nthis procedure takes **13024.3min**  (9 days, 1:33:58.999782 exactly) on one core CPU.\n\n![SVM RBF param space](https://plon.io/files/58e171451b12ce00012bd71d)\n\nBest parameters:\n* C = 5\n* gamma = 0.05\n* accuracy = 0.9852\n\n\n\n### Linear SVM with different embeddings\n\nLinear SVM's (SVM with linear kernels) have this advantages that there are many O(n)\ntraining algorithms. They are really fast in comparison with other nonlinear SVM (where most of them are O(n^2)).\nThis technique is really useful if you want to train on big data.\n\nLinear SVM algortihtms examples(papers and software):\n\n* [Pegasos](http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf)\n* [Stochastic gradient descent](http://leon.bottou.org/projects/sgd)\n* [Averaged Stochastic gradient descent](https://arxiv.org/abs/1107.2490)\n* [Liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/)\n* [Stochastic Gradient Descent with Barzilai\xe2\x80\x93Borwein update step for SVM](http://www.sciencedirect.com/science/article/pii/S0020025515002467)\n* [Primal SVM by Olivier Chappelle](http://olivier.chapelle.cc/primal/) - there also exists [Primal SVM in Python](https://github.com/ksopyla/primal_svm)\n\nUnfortunately, linear SVM isn't powerful enough to classify data with accuracy \ncomparable to RBF SVM.\n\nLearning SVM with RBF kernel could be time-consuming. In order to be more expressive, we try to approximate\nnonlinear kernel, map vectors into higher dimensional space explicitly and use fast linear SVM in this new space. This works extremely well!\n\n\nThe script _svm_mnist_embedings.py_ presents accuracy summary and training times for \nfull RBF kernel, linear SVC, and linear SVC with two kernel approximation \nNystroem and Fourier.\n\n\n\n\n## Further improvements\n \n* Augmenting the training set with artificial samples\n* Using Randomized param search\n\n\n## Useful SVM MNIST learning materials\n\n* [MNIST handwritten digit recognition](http://brianfarris.me/static/digit_recognizer.html) - author compares an accuracy of a few machine learning classification algorithms (Random Forest, Stochastic Gradient Descent, Support Vector Machine, Nearest Neighbors)\n* [Digit Recognition using OpenCV, sklearn and Python](http://hanzratech.in/2015/02/24/handwritten-digit-recognition-using-opencv-sklearn-and-python.html) - this blog post presents using HOG features and a multiclass Linear SVM.\n* [Grid search for RBF SVM parameters](http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)\n* [Fast and Accurate Digit Classification- technical report](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-159.html) - there is also download page with custom [LibLinear intersection kernel](http://ttic.uchicago.edu/~smaji/projects/digits/)\n* [Random features for large-scale kernel machines](http://www.robots.ox.ac.uk/~vgg/rg/papers/randomfeatures.pdf) Rahimi, A. and Recht, B. - Advances in neural information processing 2007,\n* [Efficient additive kernels via explicit feature maps](http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf) Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010\n* [Generalized RBF feature maps for Efficient Detection](http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/sreekanth10generalized.pdf) Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010\n\n \n"