b'\n\nHunga-Bunga\n============\n\nBrute Force all scikit-learn models and all scikit-learn parameters with **fit** **predict**.\n\n\n\n-----\n##### Lets brute force all sklearn models with all of sklearn parameters!  Ahhh Hunga Bunga!!\n\n```python\nfrom hunga_bunga import HungaBungaClassifier, HungaBungaRegressor\n```\n\n##### And then simply: \n\n<p align="center">\n  <img src="https://github.com/ypeleg/HungaBunga/blob/master/HungaBunga.png?raw=true" width="400">\n</p>\n\n-----\n\n\n\n#### What?\nYes.\n\n#### No! Really! What?\nMany believe that\n\n> most of the work of supervised (non-deep) Machine Learning lies in feature engineering, whereas the model-selection process is just running through all the models or just take xgboost.\n\nSo here is an automation for that.\n\n## HOW IT WORKS\nRuns through all `sklearn` models (both classification and regression), with **all possible hyperparameters**, and rank using cross-validation.\n\n## MODELS\nRuns **all the model** available on `sklearn` for supervised learning [here](http://scikit-learn.org/stable/supervised_learning.html). The categories are:\n\n* Generalized Linear Models\n* Kernel Ridge\n* Support Vector Machines\n* Nearest Neighbors\n* Gaussian Processes\n* Naive Bayes\n* Trees\n* Neural Networks\n* Ensemble methods\n\nNote: Some models were dropped out (nearly none of them..) and some crash or cause exceptions from time to time. It takes REALLY long to test this out so clearing exceptions took me a while.\n\n## Installation \n\n```python\npip install hunga-bunga\n```\n\nDependencies\n~~~~~~~~~~~~\n\n- Python (>= 2.7)\n- NumPy (>= 1.11.0)\n- SciPy (>= 0.17.0)\n- joblib (>= 0.11)\n- scikit-learn (>=0.20.0)\n- tabulate (>=0.8.2)\n- tqdm (>=4.28.1)\n\n~~~~~~~~~~~~\n\n\n\n## Option I (Recommended): brain = False\n\n\nAs any other sklearn model \n\n```python\nclf = HungaBungaClassifier()\nclf.fit(x, y)\nclf.predict(x)\n```\n    \nAnd import from here\n\n```python\nfrom hunga_bunga import HungaBungaClassifier, HungaBungaRegressor\n```\n\n## Option II: brain = True\n\n\nAs any other sklearn model \n\n```\nclf = HungaBungaClassifier(brain=True)\nclf.fit(x, y)\n```\n\nThe output looks this:\n\n| Model                       |  accuracy     |  Time/clf (s)|\n|---------------------------- |:-------------:|:-------------:|\n|SGDClassifier                |     0.967     |      0.001   |\n|LogisticRegression           |     0.940      |      0.001   |\n|Perceptron                   |     0.900       |      0.001   |\n|PassiveAggressiveClassifier  |     0.967     |      0.001   |\n|MLPClassifier                |     0.827     |      0.018   |\n|KMeans                       |     0.580      |      0.010    |\n|KNeighborsClassifier         |     0.960      |      0.000       |\n|NearestCentroid              |     0.933     |      0.000       |\n|RadiusNeighborsClassifier    |     0.927     |      0.000       |\n|SVC                          |     0.960      |      0.000       |\n|NuSVC                        |     0.980      |      0.001   |\n|LinearSVC                    |     0.940      |      0.005   |\n|RandomForestClassifier       |     0.980      |      0.015   |\n|DecisionTreeClassifier       |     0.960      |      0.000       |\n|ExtraTreesClassifier         |     0.993     |      0.002   |\n\n*The winner is: ExtraTreesClassifier with score 0.993.*\n\n'