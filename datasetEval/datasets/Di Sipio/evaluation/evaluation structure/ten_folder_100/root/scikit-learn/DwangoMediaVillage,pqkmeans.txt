b"# PQk-means\n\n[**Project**](http://yusukematsui.me/project/pqkmeans/pqkmeans.html)\n| [**Paper**](https://dl.acm.org/ft_gateway.cfm?id=3123430)\n| [**Tutorial**](./tutorial)\n\n\nA 2D example using both k-means and PQk-means | Large-scale evaluation\n:---:|:---:\n![](http://yusukematsui.me/project/pqkmeans/img/teaser.png)  |  ![](http://yusukematsui.me/project/pqkmeans/img/eval.png )\n\n\n\n[PQk-means [Matsui, Ogaki, Yamasaki, and Aizawa, ACMMM 17]](http://yusukematsui.me/project/pqkmeans/pqkmeans.html) is a Python library for efficient clustering of large-scale data.\nBy first compressing input vectors into short product-quantized (PQ) codes,\nPQk-means achieves fast and memory-efficient clustering, even for\nhigh-dimensional vectors.\nSimilar to k-means, PQk-means repeats the assignment and update steps,\nboth of which can be performed in the PQ-code domain.\n\n\n\nFor a comparison, we provide the ITQ encoding for the binary conversion and \n[Binary k-means [Gong+, CVPR 15]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Gong_Web_Scale_Photo_2015_CVPR_paper.html) for the clustering of binary codes.\n\nThe library is written in C++ for the main algorithm with wrappers for Python. \nAll encoding/clustering codes are compatible with scikit-learn.\n\n## Summary of features\n- Approximation of k-means\n- Tens to hundreds of times faster than k-means\n- Tens to hundreds of times more memory efficient than k-means\n- Compatible with scikit-learn\n- Portable; one-line installation\n\n## Installation\n#### Requisites\n- CMake\n    - `brew install cmake` for OS X\n    - `sudo apt install cmake` for Ubuntu\n- OpenMP (Optional)\n    - If openmp is installed, it will be automatically used to parallelize the algorithm for faster calculation.\n\n#### Build & install\nYou can install the library from PyPI:\n```\npip install pqkmeans\n```\nOr, if you would like to use the current master version, you can manually build and install the library by:\n```\ngit clone --recursive https://github.com/DwangoMediaVillage/pqkmeans.git\ncd pqkmeans\npython setup.py install\n```\n## Run samples\n\n\n```\n# with artificial data\npython bin/run_experiment.py --dataset artificial --algorithm bkmeans pqkmeans --k 100\n# with texmex dataset (http://corpus-texmex.irisa.fr/)\npython bin/run_experiment.py --dataset siftsmall --algorithm bkmeans pqkmeans --k 100\n```\n\n## Test\n```\npython setup.py test\n```\n\n\n\n## Usage\n#### For PQk-means\n\n```python\nimport pqkmeans\nimport numpy as np\nX = np.random.random((100000, 128)) # 128 dimensional 100,000 samples\n\n# Train a PQ encoder.\n# Each vector is divided into 4 parts and each part is\n# encoded with log256 = 8 bit, resulting in a 32 bit PQ code.\nencoder = pqkmeans.encoder.PQEncoder(num_subdim=4, Ks=256)\nencoder.fit(X[:1000])  # Use a subset of X for training\n\n# Convert input vectors to 32-bit PQ codes, where each PQ code consists of four uint8.\n# You can train the encoder and transform the input vectors to PQ codes preliminary.\nX_pqcode = encoder.transform(X)\n\n# Run clustering with k=5 clusters.\nkmeans = pqkmeans.clustering.PQKMeans(encoder=encoder, k=5)\nclustered = kmeans.fit_predict(X_pqcode)\n\n# Then, clustered[0] is the id of assigned center for the first input PQ code (X_pqcode[0]).\n```\n\nNote that an instance of PQ-encoder (`encoder`) and an instance of clustering (`kmeans`) can be pickled and reused later.\n\n```python\nimport pickle\n\n# An instance of PQ-encoder.\npickle.dump(encoder, open('encoder.pkl', 'wb'))\nencoder_dumped = pickle.load(open('encoder.pkl', 'rb'))\n\n# An instance of clustering. This can be reused as a vector quantizer later.\npickle.dump(kmeans, open('kmeans.pkl', 'wb'))\nkmeans_dumped = pickle.load(open('kmeans.pkl', 'rb'))\n```\n\n\n\n#### For Bk-means\n\nIn almost the same manner as for PQk-means,\n\n```python\nimport pqkmeans\nimport numpy as np\nX = np.random.random((100000, 128)) # 128 dimensional 100,000 samples\n\n# Train an ITQ binary encoder\nencoder = pqkmeans.encoder.ITQEncoder(num_bit=32)\nencoder.fit(X[:1000])  # Use a subset of X for training\n\n# Convert input vectors to binary codes\nX_itq = encoder.transform(X)\n\n# Run clustering\nkmeans = pqkmeans.clustering.BKMeans(k=5, input_dim=32)\nclustered = kmeans.fit_predict(X_itq)\n```\nPlease see more examples on a [**tutorial**](./tutorial)\n\n## Note\n- This repository contains the re-implemented version of the PQk-means with the Python interface. There can be the difference between this repository and the pure c++ implementation used in the paper.\n- We tested this library with Python3, on OS X and Ubuntu 16.04. \n\n## Authors\n- [Keisuke Ogaki](https://github.com/kogaki) designed the whole structure of the library, and implemented most of the Bk-means clustering\n- [Yusuke Matsui](http://yusukematsui.me/) implemented most of the PQk-means clustering\n\n## Reference\n\n    @inproceedings{pqkmeans,\n\t    author = {Yusuke Matsui and Keisuke Ogaki and Toshihiko Yamasaki and Kiyoharu Aizawa},\n\t    title = {PQk-means: Billion-scale Clustering for Product-quantized Codes},\n        booktitle = {ACM International Conference on Multimedia (ACMMM)},\n        year = {2017},\n    }\n\n\n\n\n## Todo\n- Evaluation script for billion-scale data\n- Nearest neighbor search with PQTable\n- Documentation\n"