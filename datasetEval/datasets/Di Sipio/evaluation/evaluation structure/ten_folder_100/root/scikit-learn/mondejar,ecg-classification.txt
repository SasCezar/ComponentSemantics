b'# ECG Classification \n\nThe code contains the implementation of a method for the automatic classification of electrocardiograms (ECG) based on the combination of multiple Support Vector Machines (SVMs). The method relies on the time intervals between consequent beats\nand their morphology for the ECG characterisation.  Different descriptors based on wavelets, local binary patterns\n(LBP), higher order statistics (HOS) and several amplitude values were employed. \n\nFor a detailed explanation refer to the paper: [http://www.sciencedirect.com/science/article/pii/S1746809418301976](http://www.sciencedirect.com/science/article/pii/S1746809418301976)\n\nIf you use this code for your publications, please cite it as:\n\n    @article{MONDEJARGUERRA201941,\n    author = {Mond{\\\'{e}}jar-Guerra, V and Novo, J and Rouco, J and Penedo, M G and Ortega, M},\n    doi = {https://doi.org/10.1016/j.bspc.2018.08.007},\n    issn = {1746-8094},\n    journal = {Biomedical Signal Processing and Control},\n    pages = {41--48},\n    title = {{Heartbeat classification fusing temporal and morphological information of ECGs via ensemble of classifiers}},\n    volume = {47},\n    year = {2019}\n    }\n\n\n\n## Requirements\n\nPython implementation is the most updated version of the repository. Matlab implementation is independent. Both implementations are tested under Ubuntu 16.04. \n\n### [Python](python)\n\n- [Numpy](https://docs.scipy.org/doc/numpy-1.13.0/user/install.html)\n- [Scikit learn](http://scikit-learn.org/stable/install.html)\n- [Matplotlib](https://matplotlib.org/) (Optional)\n        \n### [Matlab](matlab)\nPerformed using Matlab 2016b 64 bits \n\n- [LibSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/#download)\n\n*Implementation for [TensorFlow](tensorflow) is in early stage and will not be maintained by the author.*\n\n\n## Steps (How to run)\n \n1. Download the dataset:\n    - a) Download via Kaggle:\n\n        The raw signals files (.csv) and annotations files can be downloaded from [kaggle.com/mondejar/mitbih-database](https://www.kaggle.com/mondejar/mitbih-database)\n\n    - b) Download via WFDB:\n\n        https://www.physionet.org/faq.shtml#downloading-databases\n\n        Using the comand **rsync** you can check the datasets availability:\n\n        ```\n        rsync physionet.org::\n        ```\n        The terminal will show all the available datasets:\n        ```\n        physionet      \tPhysioNet web site, volume 1 (about 23 GB)\n        physionet-small\tPhysioNet web site, excluding databases (about 5 GB)\n        ...\n        ...\n        umwdb          \tUnconstrained and Metronomic Walking Database (1 MB)\n        vfdb           \tMIT-BIH Malignant Ventricular Ectopy Database (33 MB)\n        ```\n\n        Then select the desired dataset as:\n        ```\n        rsync -Cavz physionet.org::mitdb /home/mondejar/dataset/ECG/mitdb\n        ```\n\n        ```\n        rsync -Cavz physionet.org::incartdb /home/mondejar/dataset/ECG/incartdb\n        ```\n\n        Finally to convert the data as plain text files use [convert_wfdb_data_2_csv.py](https://github.com/mondejar/WFDB_utils_and_others/blob/master/convert_wfdb_data_2_csv.py). One file with the raw data and one file for annotations ground truth. \n\n        Also check the repo [WFDB_utils_and_others](https://github.com/mondejar/WFDB_utils_and_others) for more info about WFDB database conversion and the original site from [Physionet_tools](https://www.physionet.org/physiotools/wag/wag.htm).\n\n2. Run:\n\n    Run the file *run_train_SVM.py* and adapt the desired configuration to call *train_SVM.py* file. This call method will train the SVM model using the training set and evaluates the model on a different test set. \n\n    Check and adjust the path dirs on *train_SVM.py* file.\n\n4. Combining multiples classifiers:\n    \n    Run the file *basic_fusion.py* to combine the decisions of previously trained SVM models. \n\n\n## Methodology\n\nThe data is splited following the **inter-patient** scheme proposed by [Chazal *et al*](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1306572)., i.e the training and eval set not contain any patient in common.\n\nThis code classifies the signal at beat-level following the class labeling of the [AAMI recomendation](###aami-recomendation-for-mit). \n\n### 1 Preprocess:\nFirst, the baseline of the signal is substracted. Additionally, some noise removal can be done.\n\nTwo median filters are applied for this purpose, of 200-ms and 600-ms.\nNote that this values depend on the frecuency sampling of the signal.\n\n\n```python\n    from scipy.signal import medfilt\n    ...\n    \n    # median_filter1D\n    baseline = medfilt(MLII, 71) \n    baseline = medfilt(baseline, 215) \n```\n            \nThe signal resulting from the second filter operation contains the baseline wanderings and can be subtracted from the original signal.\n```python        \n    # Remove Baseline\n    for i in range(0, len(MLII)):\n        MLII[i] = MLII[i] - baseline[i]\n```\n\n### 2 Segmentation: Beat Detection\nIn this work the annotations of the MIT-BIH arrhyhtmia was used in order to detect the R-peak positions. However, in practise they can be detected using the following software (see [Software references: Beat Detection](#software-references:-beat-detection)). \n\n### 3 Feature Descriptor\nIn order to describe the beats for classification purpose, we employ the following  features:\n\n1. **Morphological**: for this features a window of [-90, 90] was centred along the R-peak:\n\n    0. **RAW-Signal** (180): is the most simplier descriptor. Just employ the amplitude values from the signal delimited by the window.\n    \n    1. **Wavelets** (23): The wavelet transforms have the capability to allow information extraction from both frequency and time domains, which make them suitable for ECG description. The signal is decomposed using *wave_decomposition* function using family *db1*  and 3 levels. \n\n    ```python\n        import pywt\n        ...\n\n        db1 = pywt.Wavelet(\'db1\')\n        coeffs = pywt.wavedec(beat, db1, level=3)\n        wavel = coeffs[0]\n    ```\n\n    2. **HOS** (10): extracted from 3-4th order cumulant, skewness and kurtosis. \n    ```python\n        import scipy.stats\n        ...\n        \n        n_intervals = 6\n        lag = int(round( (winL + winR )/ n_intervals))\n        ...\n        # For each beat \n        for i in range(0, n_intervals-1):\n            pose = (lag * (i+1))\n            interval = beat[(pose -(lag/2) ):(pose + (lag/2))]\n            # Skewness  \n            hos_b[i] = scipy.stats.skew(interval, 0, True)\n\n            # Kurtosis\n            hos_b[5+i] = scipy.stats.kurtosis(interval, 0, False, True)\n    ```\n    3. **U-LBP 1D (59)** 1D version of the popular LBP descriptor. Using the uniform patterns with neighbours = 8\n    ```python\n        import numpy as np\n        ...\n\n        hist_u_lbp = np.zeros(59, dtype=float)\n\n        for i in range(neigh/2, len(signal) - neigh/2):\n            pattern = np.zeros(neigh)\n            ind = 0\n            for n in range(-neigh/2,0) + range(1,neigh/2+1):\n                if signal[i] > signal[i+n]:\n                    pattern[ind] = 1          \n                ind += 1\n            # Convert pattern to id-int 0-255 (for neigh =8)\n            pattern_id = int("".join(str(c) for c in pattern.astype(int)), 2)\n\n            # Convert id to uniform LBP id 0-57 (uniform LBP)  58: (non uniform LBP)\n            if pattern_id in uniform_pattern_list:\n                pattern_uniform_id = int(np.argwhere(uniform_pattern_list == pattern_id))\n            else:\n                pattern_uniform_id = 58 # Non uniforms patternsuse\n\n            hist_u_lbp[pattern_uniform_id] += 1.0\n    ```\n\n    4. **My Descriptor (4)**: computed from the Euclidean distance of the  R-peak  and  four  points  extracted  from  the 4 following intervals:\n        - max([0, 40])\n        - min([75, 85])\n        - min([95, 105])\n        - max([150, 180])\n\n    ```python\n        import operator\n        ...\n\n        R_pos = int((winL + winR) / 2)\n\n        R_value = beat[R_pos]\n        my_morph = np.zeros((4))\n        y_values = np.zeros(4)\n        x_values = np.zeros(4)\n        # Obtain (max/min) values and index from the intervals\n        [x_values[0], y_values[0]] = max(enumerate(beat[0:40]), key=operator.itemgetter(1))\n        [x_values[1], y_values[1]] = min(enumerate(beat[75:85]), key=operator.itemgetter(1))\n        [x_values[2], y_values[2]] = min(enumerate(beat[95:105]), key=operator.itemgetter(1))\n        [x_values[3], y_values[3]] = max(enumerate(beat[150:180]), key=operator.itemgetter(1))\n        \n        x_values[1] = x_values[1] + 75\n        x_values[2] = x_values[2] + 95\n        x_values[3] = x_values[3] + 150\n        \n        # Norm data before compute distance\n        x_max = max(x_values)\n        y_max = max(np.append(y_values, R_value))\n        x_min = min(x_values)\n        y_min = min(np.append(y_values, R_value))\n        \n        R_pos = (R_pos - x_min) / (x_max - x_min)\n        R_value = (R_value - y_min) / (y_max - y_min)\n                    \n        for n in range(0,4):\n            x_values[n] = (x_values[n] - x_min) / (x_max - x_min)\n            y_values[n] = (y_values[n] - y_min) / (y_max - y_min)\n            x_diff = (R_pos - x_values[n]) \n            y_diff = R_value - y_values[n]\n            my_morph[n] =  np.linalg.norm([x_diff, y_diff])\n    ```\n\n2. **Interval RR** (4): intervals computed from the time between consequent beats. There are the most common feature employed for ECG classification. \n    1. pre_RR\n    2. post_RR\n    3. local_RR\n    4. global_RR\n\n3. **Normalized RR** (4): RR interval normalized by the division with the AVG value from each patient.\n    1. pre_RR / AVG(pre_RR)\n    2. post_RR / AVG(post_RR)\n    3. local_RR / AVG(local_Python (Scikit-learn)  \n    4. global_RR / AVG(global_RR)   \n\n **NOTE**: \n *Beats having a R\xe2\x80\x93R interval smaller than 150 ms or higher than 2 s most probably involve segmentation errors and are discarded*. "Weighted Conditional Random Fields for Supervised Interpatient Heartbeat Classification"* \n\n### 4 Normalization of the features\n\nBefore train the models. All the input data was standardized with [z-score](https://en.wikipedia.org/wiki/Standard_score), i.e., the values\nof each dimension are divided by its standard desviation and substracted by its mean.\n```python\n    import sklearn\n    from sklearn.externals import joblib\n    from sklearn.preprocessing import StandardScaler\n    from sklearn import svm\n    ...\n\n    scaler = StandardScaler()\n    scaler.fit(tr_features)\n    tr_features_scaled = scaler.transform(tr_features)\n\n    # scaled: zero mean unit variance ( z-score )\n    eval_features_scaled = scaler.transform(eval_features)\n```\n\n### 5 Training and Test\n\nIn scikit-learn the multiclass SVM support is handled according to a [one-vs-one](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) scheme.\n\nSince   the   MIT-BIH   database presents  high  imbalanced  data,  several  weights\nequal  to the  ratio  between  the  two  classes  of  each model  were\nemployed to compensate this differences.\n\nThe  Radial  Basis  Function  (RBF) kernel was employed.\n\n```python\n    class_weights = {}\n    for c in range(4):\n        class_weights.update({c:len(tr_labels) / float(np.count_nonzero(tr_labels == c))})\n\n\n    svm_model = svm.SVC(C=C_value, kernel=\'rbf\', degree=3, gamma=\'auto\', \n                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n                    cache_size=200, class_weight=class_weights, verbose=False, \n                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n\n    svm_model.fit(tr_features_scaled, tr_labels) \n```\n\n\nFor evaluating the model, the jk index [Mar et. al](https:doi.org/10.1109/TBME.2011.2113395)) were employed as performance measure\n\n```python\n\n    decision_ovo        = svm_model.decision_function(eval_features_scaled)\n    predict_ovo, counter    = ovo_voting_exp(decision_ovo, 4)\n\n    perf_measures = compute_AAMI_performance_measures(predict_ovo, labels)\n```\n\n### 6 Combining Ensemble of SVM\n\nSeveral basic combination rules can be employed to combine the decision from different SVM model configurations in a single prediction (see basic_fusion.py)\n\n\n### 7 Comparison with state-of-the-art on MITBIH database:\n\n\n| Classifier          | Acc. | Sens. | jk index |\n|---------------------|------|-------|----------|\n| Our Ensemble of SVMs| **0.945**| 0.703 | **0.773** |\n| [Zhang et al.](https://doi.org/10.1016/j.compbiomed.2013.11.019)        | 0.883| **0.868** | 0.663 |\n| Out Single SVM      | 0.884| 0.696 | 0.640 |\n| [Mar et al.](https://doi.org/10.1109/TBME.2011.2113395)| 0.899| 0.802 | 0.649 |\n| [Chazal et al.](https://doi.org/10.1109/TBME.2004.827359)       | 0.862| 0.832 | 0.612 |\n\n# About datasets:\n\nhttps://physionet.org/cgi-bin/atm/ATM\n\n# MIT-Arrythmia Database\n\n<b>360HZ</b>\n\n48 Samples of 30 minutes, 2 leads \n47 Patients:\n\n* 100 series: 23 samples\n* 200 series: 25 samples. **Contains uncommon but clinically important arrhythmias**\n\n| Symbol|   Meaning                                   |\n|-------|---------------------------------------------|\n|\xc2\xb7 or N |\tNormal beat                                 |\n|L      |   Left bundle branch block beat             |\n|R      |\tRight bundle branch block beat              |\n|A      |\tAtrial premature beat                       |\n|a      |\tAberrated atrial premature beat             |\n|J      |\tNodal (junctional) premature beat           |\n|S      |\tSupraventricular premature beat             |\n|V      |\tPremature ventricular contraction           |\n|F      |\tFusion of ventricular and normal beat       |\n|[      |\tStart of ventricular flutter/fibrillation   |\n|!      |\tVentricular flutter wave                    |\n|]      |\tEnd of ventricular flutter/fibrillation     |\n|e      |\tAtrial escape beat                          |\n|j      |\tNodal (junctional) escape beat              |\n|E      |\tVentricular escape beat                     |\n|/      |\tPaced beat                                  |\n|f      |\tFusion of paced and normal beat             |\n|x      |\tNon-conducted P-wave (blocked APB)          |\n|Q      |\tUnclassifiable beat                         |\n||      |\tIsolated QRS-like artifact                  |\n\n[beats and rhythms](https://physionet.org/physiobank/database/html/mitdbdir/tables.htm#allrhythms)\n\n||Rhythm annotations appear below the level used for beat annotations|\n|-|-------------------------------------------------------------------|\n|(AB |\t    Atrial bigeminy|\n|(AFIB |\tAtrial fibrillation|\n|(AFL|\tAtrial flutter|\n|(B|\t    Ventricular bigeminy|\n|(BII|\t2\xc2\xb0 heart block|\n|(IVR|\tIdioventricular rhythm|\n|(N|\t    Normal sinus rhythm|\n|(NOD|\tNodal (A-V junctional) rhythm|\n|(P|\t    Paced rhythm|\n|(PREX|\tPre-excitation (WPW)|\n|(SBR|\tSinus bradycardia|\n|(SVTA|\tSupraventricular tachyarrhythmia|\n|(T|\t    Ventricular trigeminy|\n|(VFL|\tVentricular flutter|\n|(VT|\t    Ventricular tachycardia\n\n### AAMI recomendation for MIT \nThere are 15 recommended classes for arrhythmia that are classified into 5 superclasses: \n\n| SuperClass| | | | | | | \n|------|--------|---|---|---|---|-|\n| N  (Normal)  | N      | L | R |  |  | |\n| SVEB (Supraventricular ectopic beat) | A      | a | J | S |  e | j |\n| VEB  (Ventricular ectopic beat)| V      | E |   |   |   | |\n| F    (Fusion beat) | F      |   |   |   |   | |\n| Q   (Unknown beat)  | P      | / | f | u |   |    |\n\n### Inter-patient train/test split ([Chazal *et al*](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1306572)):\nDS_1 Train: 101, 106, 108, 109, 112, 114, 115, 116, 118, 119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220, 223, 230\n\n| Class|N|SVEB|VEB|F|Q|\n|------|-|-|-|-|-|\n| instances| 45842|944|3788|414|0|    \n \nDS_2 Test: = 100, 103, 105, 111, 113, 117, 121, 123, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233, 234\n\n| Class|N|SVEB|VEB|F|Q|\n|------|-|-|-|-|-|\n| instances| 44743|1837|3447|388|8|    \n \n# INCART Database \nhttps://www.physionet.org/pn3/incartdb/\n\n<b>257HZ</b>\n\n75 records of 30 minutes, 12 leads [-4000, 4000]\n\nGains varying from 250 to 1100 analog-to-digital converter units per millivolt.\nGains for each record are specified in its .hea file. \n\nThe reference annotation files contain over 175,000 beat annotations in all.\n\nThe original records were collected from patients undergoing tests for coronary artery disease (17 men and 15 women, aged 18-80; mean age: 58). None of the patients had pacemakers; most had ventricular ectopic beats. In selecting records to be included in the database, preference was given to subjects with ECGs consistent with ischemia, coronary artery disease, conduction abnormalities, and arrhythmias;observations of those selected included:\n\n\n\n# Software references: Beat Detection\n1. [*Pan Tompkins*](https://es.mathworks.com/matlabcentral/fileexchange/45840-complete-pan-tompkins-implementation-ecg-qrs-detector)\n    \n    [third_party/Pan_Tompkins_ECG_v7/pan_tompkin.m](third_party/Pan_Tompkins_ECG_v7/pan_tompkin.m)\n\n2. [*ecgpuwave*](third_party/README.md) Also gives QRS onset, ofset, T-wave and P-wave\nNOTE:*The beats whose Q and S points were not detected are considered as outliers and automatically rejected from our datasets.*\n\n3. [ex_ecg_sigprocessing](https://es.mathworks.com/help/dsp/examples/real-time-ecg-qrs-detection.html)\n\n4. osea\n\n\n# License\nThe code of this repository is available under [GNU GPLv3 license](https://www.gnu.org/licenses/gpl-3.0.html).\n'