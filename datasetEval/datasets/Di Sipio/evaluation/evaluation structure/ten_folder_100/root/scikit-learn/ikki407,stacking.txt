b'Stacking (stacked generalization)\n====\n\n[![PyPI version](https://badge.fury.io/py/stacking.svg)](https://badge.fury.io/py/stacking)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/ikki407/stacking/LICENSE)\n\n## Overview\n\n[ikki407/stacking](https://github.com/ikki407/stacking) - Simple and useful stacking library, written in Python.\n\nUser can use models of scikit-learn, XGboost, and Keras for stacking.  \nAs a feature of this library, **all out-of-fold predictions** can be saved for further analisys after training.\n\n## Description\n\n[Stacking](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking) (sometimes called stacked generalization) involves training a learning algorithm to combine the predictions of several other learning algorithms. The basic idea is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error. \n\n[This blog](http://mlwave.com/kaggle-ensembling-guide/) is very helpful to understand stacking and ensemble learning.\n\n\n## Usage\n\n**See working example:**\n \n * [binary classification](https://github.com/ikki407/stacking/tree/master/examples/binary_class)\n * [multi-class classification](https://github.com/ikki407/stacking/tree/master/examples/multi_class)\n * [regression](https://github.com/ikki407/stacking/tree/master/examples/regression)\n\nTo run these examples, just run `sh run.sh`. Note that: \n\n1. Set train and test dataset under data/input\n\n2. Created features from original dataset need to be under data/output/features\n\n3. Models for stacking are defined in `scripts.py` under scripts folder\n\n4. Need to define created features in that scripts\n\n5. Just run `sh run.sh` (`python scripts/XXX.py`).\n\n\n### Detailed Usage\n\n1. Set train dataset with its target data and test dataset.\n\n    ```python\n    FEATURE_LIST_stage1 = {\n                    \'train\':(\n                             INPUT_PATH + \'train.csv\',\n                             FEATURES_PATH + \'train_log.csv\',\n                            ),\n    \n                    \'target\':(\n                             INPUT_PATH + \'target.csv\',\n                            ),\n    \n                    \'test\':(\n                             INPUT_PATH + \'test.csv\',\n                             FEATURES_PATH + \'test_log.csv\',\n                            ),\n                    }\n    ```\n\n2. Define model classes that inherit `BaseModel` class, which are used in Stage 1, Stage 2, ..., Stage N.\n\n    ```python\n    # For Stage 1\n    PARAMS_V1 = {\n            \'colsample_bytree\':0.80,\n            \'learning_rate\':0.1,"eval_metric":"auc",\n            \'max_depth\':5, \'min_child_weight\':1,\n            \'nthread\':4,\n            \'objective\':\'binary:logistic\',\'seed\':407,\n            \'silent\':1, \'subsample\':0.60,\n            }\n    \n    class ModelV1(BaseModel):\n            def build_model(self):\n                return XGBClassifier(params=self.params, num_round=10)\n    \n    ...\n    \n    # For Stage 2\n    PARAMS_V1_stage2 = {\n                        \'penalty\':\'l2\',\n                        \'tol\':0.0001, \n                        \'C\':1.0, \n                        \'random_state\':None, \n                        \'verbose\':0, \n                        \'n_jobs\':8\n                        }\n    \n    class ModelV1_stage2(BaseModel):\n            def build_model(self):\n                return LR(**self.params)\n    ```\n    \n3. Train each models of Stage 1 for stacking.\n\n    ```python\n    m = ModelV1(name="v1_stage1",\n                flist=FEATURE_LIST_stage1,\n                params = PARAMS_V1,\n                kind = \'st\'\n                )\n    m.run()\n    \n    ...\n    ```\n\n4. Train each model(s) of Stage 2 by using the prediction of Stage-1 models.\n\n    ```python\n    FEATURE_LIST_stage2 = {\n                \'train\': (\n                         TEMP_PATH + \'v1_stage1_all_fold.csv\',\n                         TEMP_PATH + \'v2_stage1_all_fold.csv\',\n                         TEMP_PATH + \'v3_stage1_all_fold.csv\',\n                         TEMP_PATH + \'v4_stage1_all_fold.csv\',\n                         ...\n                         ),\n    \n                \'target\':(\n                         INPUT_PATH + \'target.csv\',\n                         ),\n    \n                \'test\': (\n                        TEMP_PATH + \'v1_stage1_test.csv\',\n                        TEMP_PATH + \'v2_stage1_test.csv\',\n                        TEMP_PATH + \'v3_stage1_test.csv\',\n                        TEMP_PATH + \'v4_stage1_test.csv\',\n                        ...                     \n                        ),\n                }\n    \n    # Models\n    m = ModelV1_stage2(name="v1_stage2",\n                    flist=FEATURE_LIST_stage2,\n                    params = PARAMS_V1_stage2,\n                    kind = \'st\',\n                    )\n    m.run()\n    ```\n\n5. Final result is saved as `v1_stage2_TestInAllTrainingData.csv`.\n\n## Prerequisite\n\n- (MaxOS) Install xgboost first manually: `pip install xgboost`\n- (Optional) Install paratext: fast csv loading library\n    - From https://github.com/wiseio/paratext\n    - You need to install swig by using `brew install swig` before installing paratext if brew have been installed\n\n\n## Installation\nTo install stacking, `cd` to the stacking folder and run the install command**(up-to-date version, recommended)**:\n```\nsudo python setup.py install\n```\n\nYou can also install stacking from PyPI:\n```\npip install stacking\n```\n\n\n## Files\n\n- [stacking/base.py](https://github.com/ikki407/stacking/blob/master/stacking/base.py) : stacking module\n- examples/\n - [binary_class](https://github.com/ikki407/stacking/tree/master/examples/binary_class) : binary classification\n - [multi_class](https://github.com/ikki407/stacking/tree/master/examples/multi_class) : multi-class classification\n - [regression](https://github.com/ikki407/stacking/tree/master/examples/regression) : regression\n\n\n## Details of scripts\n\n- base.py: \n  - Base models for stacking are defined here (using sklearn.base.BaseEstimator).\n  - Some models are defined here. e.g., XGBoost, Keras, Vowpal Wabbit.\n  - These models are wrapped as scikit-learn like (using sklearn.base.ClassifierMixin, sklearn.base.RegressorMixin).\n  - That is, model class has some methods, fit(), predict_proba(), and predict().\n\nNew user-defined models can be added here.\n\nScikit-learn models can be used.\n\nBase model have some arguments.\n\n- \'s\': Stacking. Saving oof(out-of-fold) prediction(`{model_name}_all_fold.csv`) and average of test prediction based on train-fold models(`{model_name}_test.csv`). These files will be used for next level stacking.\n\n- \'t\': Training with all data and predict test(`{model_name}_TestInAllTrainingData.csv`). In this training, no validation data are used.\n\n- \'st\': Stacking and then training with all data and predict test (\'s\' and \'t\').\n\n- \'cv\': Only cross validation without saving the prediction.\n\n\nDefine several models and its parameters used for stacking.\nDefine task details on the top of script.\nTrain and test feature set are defined here. \nNeed to define CV-fold index.\n\nAny level stacking can be defined.\n\n![PredictionFiles](stacking.png "Prediction files")\n\n## Reference\n\n[1] [Wolpert, David H. Stacked generalization, Neural Networks, 5(2), 241-259](http://machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf)\n\n[2] [Ensemble learning(Stacking)](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking)\n\n[3] [KAGGLE ENSEMBLING GUIDE](http://mlwave.com/kaggle-ensembling-guide/)\n\n'