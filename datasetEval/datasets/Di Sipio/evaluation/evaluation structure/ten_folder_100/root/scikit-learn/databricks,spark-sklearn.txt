b"Scikit-learn integration package for Apache Spark\n=================================================\n\nThis package contains some tools to integrate the `Spark computing framework <https://spark.apache.org/>`_\nwith the popular `scikit-learn machine library <https://scikit-learn.org/stable/>`_. Among other things, it can:\n\n- train and evaluate multiple scikit-learn models in parallel. It is a distributed analog to the\n  `multicore implementation <https://pythonhosted.org/joblib/parallel.html>`_ included by default in ``scikit-learn``\n- convert Spark's Dataframes seamlessly into numpy ``ndarray`` or sparse matrices\n- (experimental) distribute Scipy's sparse matrices as a dataset of sparse vectors\n\nIt focuses on problems that have a small amount of data and that can be run in parallel.\nFor small datasets, it distributes the search for estimator parameters (``GridSearchCV`` in scikit-learn),\nusing Spark. For datasets that do not fit in memory, we recommend using the `distributed implementation in\n`Spark MLlib <https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html>`_.\n\nThis package distributes simple tasks like grid-search cross-validation.\nIt does not distribute individual learning algorithms (unlike Spark MLlib).\n\nInstallation\n------------\n\nThis package is available on PYPI:\n\n::\n\n\tpip install spark-sklearn\n\nThis project is also available as `Spark package <https://spark-packages.org/package/databricks/spark-sklearn>`_.\n\nThe developer version has the following requirements:\n\n- scikit-learn 0.18 or 0.19. Later versions may work, but tests currently are incompatible with 0.20.\n- Spark >= 2.1.1. Spark may be downloaded from the `Spark website <https://spark.apache.org/>`_.\n  In order to use this package, you need to use the pyspark interpreter or another Spark-compliant python\n  interpreter. See the `Spark guide <https://spark.apache.org/docs/latest/programming-guide.html#overview>`_\n  for more details.\n- `nose <https://nose.readthedocs.org>`_ (testing dependency only)\n- pandas, if using the pandas integration or testing. pandas==0.18 has been tested.\n\nIf you want to use a developer version, you just need to make sure the ``python/`` subdirectory is in the\n``PYTHONPATH`` when launching the pyspark interpreter:\n\n::\n\n\tPYTHONPATH=$PYTHONPATH:./python:$SPARK_HOME/bin/pyspark\n\nYou can directly run tests:\n\n::\n\n    cd python && ./run-tests.sh\n\nThis requires the environment variable ``SPARK_HOME`` to point to your local copy of Spark.\n\nExample\n-------\n\nHere is a simple example that runs a grid search with Spark. See the `Installation <#installation>`_ section\non how to install the package.\n\n.. code:: python\n\n    from sklearn import svm, datasets\n    from spark_sklearn import GridSearchCV\n    iris = datasets.load_iris()\n    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n    svr = svm.SVC(gamma='auto')\n    clf = GridSearchCV(sc, svr, parameters)\n    clf.fit(iris.data, iris.target)\n\nThis classifier can be used as a drop-in replacement for any scikit-learn classifier, with the same API.\n\nDocumentation\n-------------\n\n`API documentation <http://databricks.github.io/spark-sklearn-docs>`_ is currently hosted on Github pages. To\nbuild the docs yourself, see the instructions in ``docs/``.\n\n.. image:: https://travis-ci.org/databricks/spark-sklearn.svg?branch=master\n    :target: https://travis-ci.org/databricks/spark-sklearn\n"