b'# libfaceid, a Face Recognition library for everybody\r\n\r\n<p>\r\n    <b> FaceRecognition Made Easy.</b> libfaceid is a Python library for facial recognition that seamlessly integrates multiple face detection and face recognition models. \r\n</p>\r\n<p>\r\n    <b> From Zero to Hero.</b> Learn the basics of Face Recognition and experiment with different models.\r\n    libfaceid enables beginners to learn various models and simplifies prototyping of facial recognition solutions by providing a comprehensive list of models to choose from.\r\n    Multiple models for detection and encoding/embedding including classification models are supported from the basic models (Haar Cascades + LBPH) to the more advanced models (MTCNN + FaceNet).\r\n    The models are seamlessly integrated so that user can mix and match models. Each detector model has been made compatible with each embedding model to abstract you from the differences.\r\n    Each model differs in speed, accuracy, memory requirements and 3rd-party library dependencies.\r\n    This enables users to easily experiment with various solutions appropriate for their specific use cases and system requirements.\r\n    In addition, face liveness detection models are also provided for anti-face spoofing attacks (photo-based, video-based, 3d-mask-based attacks).\r\n</p>\r\n<p>\r\n    <b> Awesome Design.</b> The library is designed so that it is easy to use, modular and robust.\r\n    Selection of model is done via the constructors while the expose function is simply detect() or estimate() making usage very easy.\r\n    The files are organized into modules so it is very intuitive to understand and debug.\r\n    The robust design allows supporting new models in the future to be very straightforward.\r\n</p> \r\n<p>\r\n    <b> Extra Cool Features.</b> The library contains models for predicting your age, gender, emotion and facial landmarks.\r\n    It also contains TTS text-to-speech (speech synthesizer) and STT speech-to-text (speech recognition) models for voice-enabled and voice-activated capabilities.\r\n    Voice-enabled feature allows system to speak your name after recognizing your face.\r\n    Voice-activated feature allows system to listen for a specified word or phrase to trigger the system to do something (wake-word/trigger-word/hotword detection).\r\n    Web app is also supported for some test applications using Flask so you would be able to view the video capture remotely on another computer in the same network via a web browser. \r\n</p>\r\n\r\n\r\n![](https://github.com/richmondu/libfaceid/blob/master/templates/teaser/libfaceid.jpg)\r\n![](https://github.com/richmondu/libfaceid/blob/master/templates/teaser/libfaceid2.jpg)\r\n![](https://github.com/richmondu/libfaceid/blob/master/templates/teaser/libfaceid3.jpg)\r\n![](https://github.com/richmondu/libfaceid/blob/master/templates/teaser/libfaceid4.jpg)\r\n![](https://github.com/richmondu/libfaceid/blob/master/templates/teaser/libfaceid5.jpg)\r\n\r\n\r\n# News:\r\n\r\n| Date | Milestones |\r\n| --- | --- |\r\n| 2018, Dec 29 | Integrated [Colorspace histogram concatenation](https://github.com/ee09115/spoofing_detection) for anti-face spoofing (face liveness detection) |\r\n| 2018, Dec 26 | Integrated Google Cloud\'s STT speech-to-text (speech recognition) for voice-activated capability |\r\n| 2018, Dec 19 | Integrated Google\'s [Tacotron](https://github.com/keithito/tacotron) TTS text-to-speech (speech synthesis) for voice-enabled capability |\r\n| 2018, Dec 13 | Integrated Google\'s [FaceNet](https://github.com/davidsandberg/facenet) face embedding |\r\n| 2018, Nov 30 | Committed libfaceid to Github |\r\n\r\n\r\n# Background:\r\n\r\n<p>\r\nWith Apple incorporating face recognition technology in iPhone X last year, 2017 \r\nand with China implementing nation-wide wide-spread surveillance for social credit system in a grand scale, \r\nFace Recognition has become one of the most popular technologies where Deep Learning is used. \r\nFace recognition is used for identity authentication, access control, passport verification in airports, \r\nlaw enforcement, forensic investigations, social media platforms, disease diagnosis, police surveillance, \r\ncasino watchlists and many more.\r\n</p>\r\n\r\n<p>\r\nModern state of the art Face Recognition solutions leverages graphics processor technologies, GPU, \r\nwhich has dramatically improved over the decades. (In particular, Nvidia released the CUDA framework which allowed C and C++ applications to utilize the GPU for massive parallel computing.)\r\nIt utilizes Deep Learning (aka Neural Networks) which requires GPU power to perform massive compute operations in parallel. \r\nDeep Learning is one approach to Artificial Intelligence that simulates how the brain functions by teaching software through examples, several examples (big data), instead of harcoding the logic rules and decision trees in the software. \r\n(One important contribution in Deep Learning is the creation of ImageNet dataset. It pioneered the creation of millions of images, a big data collection of images that were labelled and classified to teach computer for image classifications.) \r\nNeural networks are basically layers of nodes where each nodes are connected to nodes in the next layer feeding information. \r\nDeepnets are very deep neural networks with several layers made possible using GPU compute power. \r\nMany neural networks topologies exists such as Convolutional Neural Networks (CNN) architecture \r\nwhich particulary applies to Computer Vision, from image classification to face recognition.\r\n</p>\r\n\r\n\r\n# Introduction:\r\n\r\n<p>\r\n    \r\nA facial recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source. At a minimum, a simple real-time facial recognition system is composed of the following pipeline:\r\n\r\n0. <b>Face Enrollment.</b> Registering faces to a database which includes pre-computing the face embeddings and training a classifier on top of the face embeddings of registered individuals. \r\n1. <b>Face Capture.</b> Reading a frame image from a camera source.\r\n2. <b>Face Detection.</b> Detecting faces in a frame image.\r\n3. <b>Face Encoding/Embedding.</b> Generating a mathematical representation of each face (coined as embedding) in the frame image.\r\n4. <b>Face Identification.</b> Infering each face embedding in an image with face embeddings of known people in a database.\r\n\r\nMore complex systems include features such as <b>Face Liveness Detection</b> (to counter spoofing attacks via photo, video or 3d mask), face alignment, <b>face augmentation</b> (to increase the number of dataset of images) and face verification (to confirm prediction by comparing cosine similarity or euclidean distance with each database embedding).\r\n</p>\r\n\r\n\r\n# Problem:\r\n\r\n<p>\r\nlibfaceid democratizes learning Face Recognition. Popular models such as FaceNet and OpenFace are not straightforward to use and don\'t provide easy-to-follow guidelines on how to install and setup. So far, dlib has been the best in terms of documentation and usage but installation is not straightforward, it is slow on CPU and is highly abstracted (abstracts OpenCV as well). Simple models such as OpenCV is good but too basic and lacks documentation of the parameter settings, on classification algorithms and end-to-end pipeline. Pyimagesearch has been great having several tutorials with easy to understand explanations but not much emphasis on model comparisons and seems to aim to sell books so intentions to help the community are not so pure after all (I hate the fact that you need to wait for 2 marketing emails to arrive just to download the source code for the tutorials. But I love the fact that he replies to all questions in the threads). With all this said, I\'ve learned a lot from all these resources so I\'m sure you will learn a lot too. \r\n\r\nlibfaceid was created to somehow address these problems and fill-in the gaps from these resources. It seamlessly integrates multiple models for each step of the pipeline enabling anybody specially beginners in Computer Vision and Deep Learning to easily learn and experiment with a comprehensive face recognition end-to-end pipeline models. No strings attached. Once you have experimented will all the models and have chosen specific models for your specific use-case and system requirements, you can explore the more advanced models like FaceNet.\r\n\r\n</p>\r\n\r\n\r\n# Design:\r\n\r\n<p>\r\nlibfaceid is designed so that it is easy to use, modular and robust. Selection of model is done via the constructors while the expose function is simply detect() or estimate() making usage very easy. The files are organized into modules so it is very intuitive to understand and debug. The robust design allows supporting new models in the future to be very straightforward.\r\n\r\nOnly pretrained models will be supported. [Transfer learning](http://cs231n.github.io/transfer-learning/) is the practice of applying a pretrained model (that is trained on a very large dataset) to a new dataset. It basically means that it is able to generalize models from one dataset to another when it has been trained on a very large dataset, such that it is \'experienced\' enough to generalize the learnings to new environment to new datasets. It is one of the major factors in the explosion of popularity in Computer Vision, not only for face recognition but most specially for object detection. And just recently, mid-2018 this year, transfer learning has been making good advances to Natural Language Processing ( [BERT by Google](https://github.com/google-research/bert) and [ELMo by Allen Institute](https://allennlp.org/elmo) ). Transfer learning is really useful and it is the main goal that the community working on Reinforcement Learning wants to achieve for robotics.\r\n</p>\r\n\r\n\r\n# Features:\r\n\r\nHaving several dataset of images per person is not possible for some use cases of Face Recognition. So finding the appropriate model for that balances accuracy and speed on target hardware platform (CPU, GPU, embedded system) is necessary. The trinity of AI is Data, Algorithms and Compute. libfaceid allows selecting each model/algorithm in the pipeline.\r\n\r\nlibfaceid library supports several models for each step of the Face Recognition pipeline. Some models are faster while some models are more accurate. You can mix and match the models for your specific use-case, hardware platform and system requirements. \r\n\r\n### Face Detection models for detecting face locations\r\n- [Haar Cascade Classifier via OpenCV](https://github.com/opencv/opencv/blob/master/samples/python/facedetect.py)\r\n- [Histogram of Oriented Gradients (HOG) via DLIB](http://dlib.net/face_detector.py.html)\r\n- [Deep Neural Network via DLIB](http://dlib.net/cnn_face_detector.py.html)\r\n- [Single Shot Detector with ResNet-10 via OpenCV](https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/resnet_ssd_face_python.py)\r\n- [Multi-task Cascaded CNN (MTCNN) via Tensorflow](https://github.com/ipazc/mtcnn/blob/master/tests/test_mtcnn.py)\r\n- [FaceNet MTCNN via Tensorflow](https://github.com/davidsandberg/facenet)\r\n\r\n### Face Encoding models for generating face embeddings on detected faces\r\n- [Local Binary Patterns Histograms (LBPH) via OpenCV](https://www.python36.com/face-recognition-using-opencv-part-3/)\r\n- [OpenFace via OpenCV](https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/)\r\n- [ResNet-34 via DLIB](http://dlib.net/face_recognition.py.html)\r\n- [FaceNet (Inception ResNet v1) via Tensorflow](https://github.com/davidsandberg/facenet)\r\n- [VGG-Face (VGG-16, ResNet-50) via Keras](https://github.com/rcmalli/keras-vggface) - TODO\r\n- [OpenFace via Torch and Lua](https://github.com/cmusatyalab/openface) - TODO\r\n\r\n### Classification algorithms for Face Identification using face embeddings\r\n- [Na\xc3\xafve Bayes](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)\r\n- Linear SVM\r\n- RVF SVM\r\n- Nearest Neighbors\r\n- Decision Tree\r\n- Random Forest\r\n- Neural Net\r\n- Adaboost\r\n- QDA\r\n\r\n### Face Liveness Detection models for preventing spoofing attacks\r\n- [Eye Movement](https://www.pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/)\r\n- [Mouth Movement](https://github.com/mauckc/mouth-open)\r\n- [Colorspace Histogram Concatenation](https://github.com/ee09115/spoofing_detection)\r\n\r\n### Additional models (bonus features for PR): \r\n- TTS Text-To-Speech <b>(speech synthesis)</b> models for voice-enabled capability\r\n    - [PyTTSX3](https://pypi.org/project/pyttsx3/)\r\n    - [Tacotron](https://github.com/keithito/tacotron)\r\n    - [gTTS](https://pypi.org/project/gTTS/)\r\n- STT Speech-To-Text <b>(speech recognition)</b> models for voice-activated capability\r\n    - [GoogleCloud](https://pypi.org/project/SpeechRecognition/)\r\n    - [Wit.ai](https://wit.ai/)\r\n    - [Houndify](https://www.houndify.com/)\r\n    - PocketSphinx - TODO\r\n    - Snoyboy - TODO\r\n    - Precise - TODO\r\n- Face Pose estimator models for predicting face landmarks <b>(face landmark detection)</b>\r\n- Face Age estimator models for predicting age <b>(age detection)</b>\r\n- Face Gender estimator models for predicting gender <b>(gender detection)</b>\r\n- Face Emotion estimator models for predicting facial expression <b>(emotion detection)</b>\r\n\r\n\r\n# Compatibility:\r\n\r\n<p>\r\nThe library and example applications have been tested on Raspberry Pi 3B+ (Python 3.5.3) and Windows 7 (Python 3.6.6)\r\nusing <b>OpenCV</b> 3.4.3.18, <b>Tensorflow</b> 1.8.0 and <b>Keras</b> 2.0.8. \r\nFor complete dependencies, refer to requirements.txt. \r\nTested with built-in laptop camera and with a Logitech C922 Full-HD USB webcam.\r\n\r\nI encountered DLL issue with OpenCV 3.4.3.18 on my Windows 7 laptop. \r\nIf you encounter such issue, use OpenCV 3.4.1.15 or 3.3.1.11 instead.\r\nAlso note that opencv-python and opencv-contrib-python must always have the same version.\r\n</p>\r\n\r\n\r\n# Usage:\r\n\r\n### Installation:\r\n\r\n        1. Install Python 3 and Python PIP\r\n           Use Python 3.5.3 for Raspberry Pi 3B+ and Python 3.6.6 for Windows\r\n        2. Install the required Python PIP package dependencies using requirements.txt\r\n           pip install -r requirements.txt\r\n\r\n           This will install the following dependencies below:\r\n           opencv-python==3.4.3.18\r\n           opencv-contrib-python==3.4.3.18\r\n           numpy==1.15.4\r\n           imutils==0.5.1\r\n           scipy==1.1.0\r\n           scikit-learn==0.20.0\r\n           mtcnn==0.0.8\r\n           tensorflow==1.8.0\r\n           keras==2.0.8\r\n           h5py==2.8.0\r\n           facenet==1.0.3\r\n           flask==1.0.2\r\n           dlib==19.16.0 # requires CMake\r\n           \r\n           // Installing dlib\r\n           1. Install cmake from https://cmake.org/download/ OR \r\n           2. pip install https://files.pythonhosted.org/packages/0e/ce/f8a3cff33ac03a8219768f0694c5d703c8e037e6aba2e865f9bae22ed63c/dlib-19.8.1-cp36-cp36m-win_amd64.whl#sha256=794994fa2c54e7776659fddb148363a5556468a6d5d46be8dad311722d54bfcf \r\n\r\n\r\n        3. Optional: Install the required Python PIP package dependencies for speech synthesizer and speech recognition for voice capability \r\n           pip install -r requirements_with_voicecapability.txt\r\n\r\n           This will install additional dependencies below:\r\n           playsound==1.2.2\r\n           inflect==0.2.5\r\n           librosa==0.4.2\r\n           unidecode==0.4.20\r\n           pyttsx3==2.7\r\n           gtts==2.0.3\r\n           speechrecognition==3.8.1\r\n\r\n           Additional items to install: \r\n           On Windows, install pypiwin32 using "pip install pypiwin32==223"\r\n           On RPI, \r\n               sudo apt-get install espeak\r\n               sudo apt-get install python-espeak\r\n               sudo apt-get install portaudio19-dev\r\n               pip3 install pyaudio\r\n               [Microphone Setup on RPI](https://iotbytes.wordpress.com/connect-configure-and-test-usb-microphone-and-speaker-with-raspberry-pi/)\r\n\r\n\r\n### Quickstart (Dummy Guide):\r\n\r\n        1. Add your dataset\r\n           ex. datasets/person1/1.jpg, datasets/person2/1.jpg\r\n        2. Train your model with your dataset\r\n           Update training.bat to specify your chosen models\r\n           Run training.bat\r\n        3. Test your model\r\n           Update testing_image.bat to specify your chosen models\r\n           Run testing_image.bat\r\n\r\n\r\n### Folder structure:\r\n\r\n        libfaceid\r\n        |\r\n        |   agegenderemotion_webcam.py\r\n        |   testing_image.py\r\n        |   testing_webcam.py\r\n        |   testing_webcam_livenessdetection.py\r\n        |   testing_webcam_voiceenabled.py\r\n        |   testing_webcam_voiceenabled_voiceactivated.py\r\n        |   training.py\r\n        |   requirements.txt\r\n        |   requirements_with_voicecapability.txt\r\n        |   \r\n        +---libfaceid\r\n        |   |   age.py\r\n        |   |   classifier.py\r\n        |   |   detector.py\r\n        |   |   emotion.py\r\n        |   |   encoder.py\r\n        |   |   gender.py\r\n        |   |   liveness.py\r\n        |   |   pose.py\r\n        |   |   speech_synthesizer.py\r\n        |   |   speech_recognizer.py\r\n        |   |   __init__.py\r\n        |   |   \r\n        |   \\---tacotron\r\n        |           \r\n        +---models\r\n        |   +---detection\r\n        |   |       deploy.prototxt\r\n        |   |       haarcascade_frontalface_default.xml\r\n        |   |       mmod_human_face_detector.dat\r\n        |   |       res10_300x300_ssd_iter_140000.caffemodel\r\n        |   |       \r\n        |   +---encoding\r\n        |   |       dlib_face_recognition_resnet_model_v1.dat\r\n        |   |       facenet_20180402-114759.pb\r\n        |   |       openface_nn4.small2.v1.t7\r\n        |   |       shape_predictor_5_face_landmarks.dat\r\n        |   |           \r\n        |   +---estimation\r\n        |   |       age_deploy.prototxt\r\n        |   |       age_net.caffemodel\r\n        |   |       emotion_deploy.json\r\n        |   |       emotion_net.h5\r\n        |   |       gender_deploy.prototxt\r\n        |   |       gender_net.caffemodel\r\n        |   |       shape_predictor_68_face_landmarks.dat\r\n        |   |       shape_predictor_68_face_landmarks.jpg\r\n        |   |               \r\n        |   +---liveness\r\n        |   |       colorspace_ycrcbluv_print.pkl\r\n        |   |       colorspace_ycrcbluv_replay.pkl\r\n        |   |       shape_predictor_68_face_landmarks.dat\r\n        |   |               \r\n        |   +---synthesis\r\n        |   |   \\---tacotron-20180906\r\n        |   |           model.ckpt.data-00000-of-00001\r\n        |   |           model.ckpt.index\r\n        |   |           \r\n        |   \\---training // This is generated during training (ex. facial_recognition_training.py)\r\n        |           dlib_le.pickle\r\n        |           dlib_re.pickle\r\n        |           facenet_le.pickle\r\n        |           facenet_re.pickle\r\n        |           lbph.yml\r\n        |           lbph_le.pickle\r\n        |           openface_le.pickle\r\n        |           openface_re.pickle\r\n        |\r\n        +---audiosets // This is generated during training (ex. facial_recognition_training.py)\r\n        |       Person1.wav or Person1.mp3\r\n        |       Person2.wav or Person2.mp3\r\n        |       Person3.wav or Person3.mp3\r\n        |       \r\n        +---datasets // This is generated by user\r\n        |   +---Person1\r\n        |   |       1.jpg\r\n        |   |       2.jpg\r\n        |   |       ...\r\n        |   |       X.jpg\r\n        |   |       \r\n        |   +---Person2\r\n        |   |       1.jpg\r\n        |   |       2.jpg\r\n        |   |       ...\r\n        |   |       X.jpg\r\n        |   |       \r\n        |   \\---Person3\r\n        |           1.jpg\r\n        |           2.jpg\r\n        |           ...\r\n        |           X.jpg\r\n        |           \r\n        \\---templates\r\n\r\n\r\n### Pre-requisites:\r\n\r\n        1. Add the dataset of images under the datasets directory\r\n           The datasets folder should be in the same location as the test applications.\r\n           Having more images per person makes accuracy much better.\r\n           If only 1 image is possible, then do data augmentation.\r\n             Example:\r\n             datasets/Person1 - contain images of person name Person1\r\n             datasets/Person2 - contain images of person named Person2 \r\n             ...\r\n             datasets/PersonX - contain images of person named PersonX \r\n        2. Train the model using the datasets. \r\n           Can use training.py\r\n           Make sure the models used for training is the same for actual testing for better accuracy.\r\n\r\n\r\n### Examples:\r\n\r\n        detector models:           0-HAARCASCADE, 1-DLIBHOG, 2-DLIBCNN, 3-SSDRESNET, 4-MTCNN, 5-FACENET\r\n        encoder models:            0-LBPH, 1-OPENFACE, 2-DLIBRESNET, 3-FACENET\r\n        classifier algorithms:     0-NAIVE_BAYES, 1-LINEAR_SVM, 2-RBF_SVM, 3-NEAREST_NEIGHBORS, 4-DECISION_TREE, 5-RANDOM_FOREST, 6-NEURAL_NET, 7-ADABOOST, 8-QDA\r\n        liveness models:           0-EYESBLINK_MOUTHOPEN, 1-COLORSPACE_YCRCBLUV\r\n        speech synthesizer models: 0-TTSX3, 1-TACOTRON, 2-GOOGLECLOUD\r\n        speech recognition models: 0-GOOGLECLOUD, 1-WITAI, 2-HOUNDIFY\r\n        camera resolution:         0-QVGA, 1-VGA, 2-HD, 3-FULLHD\r\n\r\n        1. Training with datasets\r\n            Usage: python training.py --detector 0 --encoder 0 --classifier 0\r\n            Usage: python training.py --detector 0 --encoder 0 --classifier 0 --setsynthesizer True --synthesizer 0\r\n\r\n        2. Testing with images\r\n            Usage: python testing_image.py --detector 0 --encoder 0 --image datasets/rico/1.jpg\r\n\r\n        3. Testing with a webcam\r\n            Usage: python testing_webcam.py --detector 0 --encoder 0 --webcam 0 --resolution 0\r\n            Usage: python testing_webcam_flask.py\r\n                   Then open browser and type http://127.0.0.1:5000 or http://ip_address:5000\r\n                \r\n        4. Testing with a webcam with anti-spoofing attacks\r\n            Usage: python testing_webcam_livenessdetection.py --detector 0 --encoder 0 --liveness 0 --webcam 0 --resolution 0\r\n\r\n        5. Testing with voice-control\r\n            Usage: python testing_webcam_voiceenabled.py --detector 0 --encoder 0 --speech_synthesizer 0 --webcam 0 \r\n            Usage: python testing_webcam_voiceenabled_voiceactivated.py --detector 0 --encoder 0 --speech_synthesizer 0 --speech_recognition 0 --webcam 0 --resolution 0\r\n\r\n        6. Testing age/gender/emotion detection\r\n            Usage: python agegenderemotion_webcam.py --detector 0 --webcam 0 --resolution 0\r\n            Usage: python agegenderemotion_webcam_flask.py\r\n                   Then open browser and type http://127.0.0.1:5000 or http://ip_address:5000\r\n\r\n\r\n### Training models with dataset of images:\r\n\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n        from libfaceid.classifier  import FaceClassifierModels\r\n\r\n        INPUT_DIR_DATASET         = "datasets"\r\n        INPUT_DIR_MODEL_DETECTION = "models/detection/"\r\n        INPUT_DIR_MODEL_ENCODING  = "models/encoding/"\r\n        INPUT_DIR_MODEL_TRAINING  = "models/training/"\r\n\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=True)\r\n        face_encoder.train(face_detector, path_dataset=INPUT_DIR_DATASET, verify=verify, classifier=FaceClassifierModels.NAIVE_BAYES)\r\n\r\n        // generate audio samples for image datasets using text to speech synthesizer\r\n        OUTPUT_DIR_AUDIOSET       = "audiosets/"\r\n        INPUT_DIR_MODEL_SYNTHESIS = "models/synthesis/"\r\n        from libfaceid.speech_synthesizer import SpeechSynthesizerModels, SpeechSynthesizer\r\n        speech_synthesizer = SpeechSynthesizer(model=SpeechSynthesizerModels.DEFAULT, path=INPUT_DIR_MODEL_SYNTHESIS, path_output=OUTPUT_DIR_AUDIOSET)\r\n        speech_synthesizer.synthesize_datasets(INPUT_DIR_DATASET)\r\n\r\n\r\n### Face Recognition on images:\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n\r\n        INPUT_DIR_MODEL_DETECTION = "models/detection/"\r\n        INPUT_DIR_MODEL_ENCODING  = "models/encoding/"\r\n        INPUT_DIR_MODEL_TRAINING  = "models/training/"\r\n\r\n        image = cv2.VideoCapture(imagePath)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\r\n\r\n        frame = image.read()\r\n        faces = face_detector.detect(frame)\r\n        for (index, face) in enumerate(faces):\r\n            face_id, confidence = face_encoder.identify(frame, face)\r\n            label_face(frame, face, face_id, confidence)\r\n        cv2.imshow(window_name, frame)\r\n        cv2.waitKey(5000)\r\n\r\n        image.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n### Basic Real-Time Face Recognition (w/a webcam):\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n\r\n        INPUT_DIR_MODEL_DETECTION = "models/detection/"\r\n        INPUT_DIR_MODEL_ENCODING  = "models/encoding/"\r\n        INPUT_DIR_MODEL_TRAINING  = "models/training/"\r\n\r\n        camera = cv2.VideoCapture(webcam_index)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\r\n\r\n        while True:\r\n            frame = camera.read()\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n                face_id, confidence = face_encoder.identify(frame, face)\r\n                label_face(frame, face, face_id, confidence)\r\n            cv2.imshow(window_name, frame)\r\n            cv2.waitKey(1)\r\n\r\n        camera.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n### Real-Time Face Recognition With Liveness Detection (w/a webcam):\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n        from libfaceid.liveness import FaceLivenessModels, FaceLiveness\r\n\r\n        INPUT_DIR_MODEL_DETECTION  = "models/detection/"\r\n        INPUT_DIR_MODEL_ENCODING   = "models/encoding/"\r\n        INPUT_DIR_MODEL_TRAINING   = "models/training/"\r\n        INPUT_DIR_MODEL_ESTIMATION = "models/estimation/"\r\n        INPUT_DIR_MODEL_LIVENESS   = "models/liveness/"\r\n\r\n        camera = cv2.VideoCapture(webcam_index)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\r\n        face_liveness = FaceLiveness(model=model_liveness, path=INPUT_DIR_MODEL_ESTIMATION)\r\n        face_liveness2 = FaceLiveness(model=FaceLivenessModels.COLORSPACE_YCRCBLUV, path=INPUT_DIR_MODEL_LIVENESS)\r\n\r\n        while True:\r\n            frame = camera.read()\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n\r\n                // Check if eyes are close and if mouth is open\r\n                eyes_close, eyes_ratio = face_liveness.is_eyes_close(frame, face)\r\n                mouth_open, mouth_ratio = face_liveness.is_mouth_open(frame, face)\r\n\r\n                // Detect if frame is a print attack or replay attack based on colorspace\r\n                is_fake_print  = face_liveness2.is_fake(frame, face)\r\n                is_fake_replay = face_liveness2.is_fake(frame, face, flag=1)\r\n\r\n                // Identify face only if it is not fake and eyes are open and mouth is close\r\n                if is_fake_print or is_fake_replay:\r\n                    face_id, confidence = ("Fake", None)\r\n                elif not eyes_close and not mouth_open:\r\n                    face_id, confidence = face_encoder.identify(frame, face)\r\n\r\n                label_face(frame, face, face_id, confidence)\r\n\r\n            // Monitor eye blinking and mouth opening for liveness detection\r\n            total_eye_blinks, eye_counter = monitor_eye_blinking(eyes_close, eyes_ratio, total_eye_blinks, eye_counter, eye_continuous_close)\r\n            total_mouth_opens, mouth_counter = monitor_mouth_opening(mouth_open, mouth_ratio, total_mouth_opens, mouth_counter, mouth_continuous_open)\r\n\r\n            cv2.imshow(window_name, frame)\r\n            cv2.waitKey(1)\r\n\r\n        camera.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n### Voice-Enabled Real-Time Face Recognition (w/a webcam):\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n        from libfaceid.speech_synthesizer import SpeechSynthesizerModels, SpeechSynthesizer\r\n\r\n        INPUT_DIR_MODEL_DETECTION = "models/detection/"\r\n        INPUT_DIR_MODEL_ENCODING  = "models/encoding/"\r\n        INPUT_DIR_MODEL_TRAINING  = "models/training/"\r\n        INPUT_DIR_AUDIOSET        = "audiosets"\r\n\r\n        camera = cv2.VideoCapture(webcam_index)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\r\n        speech_synthesizer = SpeechSynthesizer(model=SpeechSynthesizerModels.DEFAULT, path=None, path_output=None, training=False)\r\n\r\n        frame_count = 0\r\n        while True:\r\n            frame = camera.read()\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n                face_id, confidence = face_encoder.identify(frame, face)\r\n                label_face(frame, face, face_id, confidence)\r\n                if (frame_count % 120 == 0):\r\n                    // Speak the person\'s name\r\n                    speech_synthesizer.playaudio(INPUT_DIR_AUDIOSET, face_id, block=False)\r\n            cv2.imshow(window_name, frame)\r\n            cv2.waitKey(1)\r\n            frame_count += 1\r\n\r\n        camera.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n### Voice-Activated and Voice-Enabled Real-Time Face Recognition (w/a webcam):\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.encoder  import FaceEncoderModels, FaceEncoder\r\n        from libfaceid.speech_synthesizer import SpeechSynthesizerModels, SpeechSynthesizer\r\n        from libfaceid.speech_recognizer  import SpeechRecognizerModels,  SpeechRecognizer\r\n\r\n        trigger_word_detected = False\r\n        def speech_recognizer_callback(word):\r\n            print("Trigger word detected! \'{}\'".format(word))\r\n            trigger_word_detected = True\r\n\r\n        INPUT_DIR_MODEL_DETECTION = "models/detection/"\r\n        INPUT_DIR_MODEL_ENCODING  = "models/encoding/"\r\n        INPUT_DIR_MODEL_TRAINING  = "models/training/"\r\n        INPUT_DIR_AUDIOSET        = "audiosets"\r\n\r\n        camera = cv2.VideoCapture(webcam_index)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_encoder  = FaceEncoder(model=FaceEncoderModels.DEFAULT, path=INPUT_DIR_MODEL_ENCODING, path_training=INPUT_DIR_MODEL_TRAINING, training=False)\r\n        speech_synthesizer = SpeechSynthesizer(model=SpeechSynthesizerModels.DEFAULT, path=None, path_output=None, training=False)\r\n        speech_recognizer  = SpeechRecognizer(model=SpeechRecognizerModels.DEFAULT, path=None)\r\n\r\n        // Wait for trigger word/wake word/hot word before starting face recognition\r\n        TRIGGER_WORDS = ["Hey Google", "Alexa", "Activate", "Open Sesame"]\r\n        print("\\nWaiting for a trigger word: {}".format(TRIGGER_WORDS))\r\n        speech_recognizer.start(TRIGGER_WORDS, speech_recognizer_callback)\r\n        while (trigger_word_detected == False):\r\n            time.sleep(1)\r\n        speech_recognizer.stop()\r\n\r\n        // Start face recognition\r\n        frame_count = 0\r\n        while True:\r\n            frame = camera.read()\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n                face_id, confidence = face_encoder.identify(frame, face)\r\n                label_face(frame, face, face_id, confidence)\r\n                if (frame_count % 120 == 0):\r\n                    // Speak the person\'s name\r\n                    speech_synthesizer.playaudio(INPUT_DIR_AUDIOSET, face_id, block=False)\r\n            cv2.imshow(window_name, frame)\r\n            cv2.waitKey(1)\r\n            frame_count += 1\r\n\r\n        camera.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n### Real-Time Face Pose/Age/Gender/Emotion Estimation (w/a webcam):\r\n\r\n        import cv2\r\n        from libfaceid.detector import FaceDetectorModels, FaceDetector\r\n        from libfaceid.pose import FacePoseEstimatorModels, FacePoseEstimator\r\n        from libfaceid.age import FaceAgeEstimatorModels, FaceAgeEstimator\r\n        from libfaceid.gender import FaceGenderEstimatorModels, FaceGenderEstimator\r\n        from libfaceid.emotion import FaceEmotionEstimatorModels, FaceEmotionEstimator\r\n\r\n        INPUT_DIR_MODEL_DETECTION       = "models/detection/"\r\n        INPUT_DIR_MODEL_ENCODING        = "models/encoding/"\r\n        INPUT_DIR_MODEL_TRAINING        = "models/training/"\r\n        INPUT_DIR_MODEL_ESTIMATION      = "models/estimation/"\r\n\r\n        camera = cv2.VideoCapture(webcam_index)\r\n        face_detector = FaceDetector(model=FaceDetectorModels.DEFAULT, path=INPUT_DIR_MODEL_DETECTION)\r\n        face_pose_estimator = FacePoseEstimator(model=FacePoseEstimatorModels.DEFAULT, path=INPUT_DIR_MODEL_ESTIMATION)\r\n        face_age_estimator = FaceAgeEstimator(model=FaceAgeEstimatorModels.DEFAULT, path=INPUT_DIR_MODEL_ESTIMATION)\r\n        face_gender_estimator = FaceGenderEstimator(model=FaceGenderEstimatorModels.DEFAULT, path=INPUT_DIR_MODEL_ESTIMATION)\r\n        face_emotion_estimator = FaceEmotionEstimator(model=FaceEmotionEstimatorModels.DEFAULT, path=INPUT_DIR_MODEL_ESTIMATION)\r\n\r\n        while True:\r\n            frame = camera.read()\r\n            faces = face_detector.detect(frame)\r\n            for (index, face) in enumerate(faces):\r\n                age = face_age_estimator.estimate(frame, face_image)\r\n                gender = face_gender_estimator.estimate(frame, face_image)\r\n                emotion = face_emotion_estimator.estimate(frame, face_image)\r\n                shape = face_pose_estimator.detect(frame, face)\r\n                face_pose_estimator.add_overlay(frame, shape)\r\n                label_face(age, gender, emotion)\r\n            cv2.imshow(window_name, frame)\r\n            cv2.waitKey(1)\r\n\r\n        camera.release()\r\n        cv2.destroyAllWindows()\r\n\r\n\r\n\r\n# Case Study - Face Recognition for Identity Authentication:\r\n\r\nOne of the use cases of face recognition is for security identity authentication.\r\nThis is a convenience feature to authenticate with system using one\'s face instead of inputting passcode or scanning fingerprint. Passcode is often limited by the maximum number of digits allowed while fingerprint scanning often has problems with wet fingers or dry skin. Face authentication offers a more reliable and secure way to authenticate.\r\n\r\nWhen used for identity authentication, face recognition specifications will differ a lot from general face recognition systems like Facebook\'s automated tagging and Google\'s search engine; it will be more like Apple\'s Face ID in IPhone X. Below are guidelines for drafting specifications for your face recognition solution. Note that [Apple\'s Face ID technology](https://support.apple.com/en-us/HT208109) will be used as the primary baseline in this case study of identity authentication use case of face recognition. Refer to this [Apple\'s Face ID white paper](https://www.apple.com/business/site/docs/FaceID_Security_Guide.pdf) for more information.\r\n\r\n\r\n### Face Enrollment\r\n\r\n- Should support dynamic enrollment of faces. Tied up with the maximum number of users the existing system supports.\r\n- Should ask user to move/rotate face (in a circular motion) in order to capture different angles of the face. This gives the system enough flexbility to recognize you at different face angles.\r\n- IPhone X Face ID face enrollment is done twice for some reason. It is possible that the first scan is for liveness detection only.\r\n- How many images should be captured? We can store as much image as possible for better accuracy but memory footprint is the limiting factor. Estimate based on size of 1 picture and the maximum number of users.\r\n- For security purposes and memory related efficiency, images used during enrollment should not be saved. \r\nOnly the mathematical representations (128-dimensional vector) of the face should be used.\r\n\r\n\r\n### Face Capture\r\n\r\n- Camera will be about 1 foot away from user (Apple Face ID: 10-20 inches).\r\n- Camera resolution will depend on display panel size and display resolutions. QVGA size is acceptable for embedded solutions. \r\n- Take into consideration a bad lighting and extremely dark situation. Should camera have a good flash/LED to emit some light. Iphone X has an infrared light to better perform on dark settings.\r\n\r\n\r\n### Face Detection\r\n\r\n- Only 1 face per frame is detected.\r\n- Face is expected to be within a certain location (inside a fixed box or circular region).\r\n- Detection of faces will be triggered by a user action - clicking some button. (Not automatic detection).\r\n- Face alignment may not be helpful as users can be enforced or directed to have his face inside a fixed box or circular region so face is already expected to be aligned for the most cases. But if adding this feature does not affect speed performance, then face alignment ahould be added if possible.\r\n- Should verify if face is alive via anti-spoofing techniques against picture-based attacks, video-based attacks and 3D mask attacks. Two popular example of liveness detection is detecting eye blinking and mouth opening. \r\n\r\n\r\n### Face Encoding/Embedding\r\n\r\n- Speed is not a big factor. Face embedding and face identification can take 3-5 seconds.\r\n- Accuracy is critically important. False match rate should be low as much as possible. \r\n- Can do multiple predictions and get the highest count. Or apply different models for predictions for double checking.\r\n\r\n\r\n### Face Identification\r\n\r\n- Recognize only when eyes are not closed and mouth is not open\r\n- Images per person should at least be 50 images. Increase the number of images per person by cropping images with different face backgound margin, slight rotations, flipping and scaling.\r\n- Classification model should consider the maximum number of users to support. For example, SVM is known to be good for less than 100k classes/persons only.\r\n- Should support unknown identification by setting a threshold on the best prediction. If best prediction is too low, then consider as Unknown.\r\n- Set the number of consecutive failed attempts allowed before disabling face recognition feature. Should fallback to passcode authentication if identification encounters trouble recognizing people.\r\n- Images used for successful scan should be added to the existing dataset images during face enrollment making it adaptive and updated so that a person can be recognized with better accuracy in the future even with natural changes in the face appearance (hairstyle, mustache, pimples, etc.)\r\n\r\nIn addition to these guidelines, the face recognition solution should provide a way to disable/enable this feature as well as resetting the stored datasets during face enrollment.\r\n\r\n\r\n\r\n# Case Study - Face Recognition for Home/Office/Hotel Greeting System:\r\n\r\nOne of the use cases of face recognition is for greeting system used in smart homes, office and hotels.\r\nTo enable voice capability feature, we use text-to-speech synthesis to dynamically create audio files given some input text. \r\n\r\n### Speech Synthesis\r\n\r\nSpeech synthesis is the artificial simulation of human speech by a computer device.\r\nIt is mostly used for translating text into audio to make the system voice-enabled.\r\nProducts such as Apple\'s Siri, Microsoft\'s Cortana, Amazon Echo and Google Assistant uses speech synthesis.\r\nA good speech synthesizer is one that produces accurate outputs that naturally sounds like a real human in near real-time.\r\nState-of-the-art speech synthesis includes [Deepmind\'s WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) \r\nand [Google\'s Tacotron](https://www.isca-speech.org/archive/Interspeech_2017/abstracts/1452.html).\r\n\r\nSpeech Synthesis can be used for some use-cases of Face Recognition to enable voice capability feature.\r\nOne example is to greet user as he approaches the terminal or kiosk system.\r\nGiven some input text, the speech synthesizer can generate an audio which can be played upon recognizing a face.\r\nFor example, upon detecting person arrival, it can be set to say \'Hello PersonX, welcome back...\'. \r\nUpon departure, it can be set to say \'Goodbye PersonX, see you again soon...\'.\r\nIt can be used in smart homes, office lobbies, luxury hotel rooms, and modern airports. \r\n\r\n### Face Enrollment\r\n\r\n- For each person who registers/enrolls to the system, create an audio file "PersonX.wav" for some input text such as "Hello PersonX".\r\n  \r\n### Face Identification\r\n\r\n- When a person is identified to be part of the database, we play the corresponding audio file "PersonX.wav". \r\n\r\n\r\n\r\n# Performance Optimizations:\r\n\r\nSpeed and accuracy is often a trade-off. Performance can be optimized depending on your specific use-case and system requirements. Some models are optimized for speed while others are optimized for accuracy. Be sure to test all the provided models to determine the appropriate model for your specific use-case, target platform (CPU, GPU or embedded) and specific requirements. Below are additional suggestions to optimize performance.\r\n\r\n### Speed\r\n- Reduce the frame size for face detection.\r\n- Perform face recognition every X frames only\r\n- Use threading in reading camera source frames or in processing the camera frames.\r\n- Update the library and configure the parameters directly.\r\n\r\n### Accuracy\r\n- Add more datasets if possible (ex. do data augmentation). More images per person will often result to higher accuracy.\r\n- Add face alignment if faces in the datasets are not aligned or when faces may be unaligned in actual deployment.\r\n- Update the library and configure the parameters directly.\r\n\r\n\r\n\r\n# References:\r\n\r\nBelow are links to valuable resoures. Special thanks to all of these guys for sharing their work on Face Recognition. Without them, learning Face Recognition would be difficult.\r\n\r\n### Codes\r\n- [OpenCV tutorials by Adrian Rosebrock](https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/)\r\n- [Dlib by Davis King](https://github.com/davisking/dlib)\r\n- [Face Recognition (Dlib wrapper) by Adam Geitgey](https://github.com/ageitgey/face_recognition)\r\n- [FaceNet implementation by David Sandberg](https://github.com/davidsandberg/facenet)\r\n- [OpenFace (FaceNet implementation) by Satyanarayanan](https://github.com/cmusatyalab/openface)\r\n- [VGG-Face implementation by Refik Can Malli](https://github.com/rcmalli/keras-vggface)\r\n\r\nGoogle and Facebook have access to large database of pictures being the best search engine and social media platform, respectively. Below are the face recognition models they have designed for their own system. Be sure to take time to read these papers for better understanding of high-quality face recognition models. \r\n\r\n### Papers\r\n- [FaceNet paper by Google](https://arxiv.org/pdf/1503.03832.pdf)\r\n- [DeepFace paper by Facebook](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)\r\n\r\n\r\n\r\n# Contribute:\r\n\r\nHave a good idea for improving libfaceid? Please message me in [twitter](https://twitter.com/richmond_umagat).\r\nIf libfaceid has helped you in learning or prototyping face recognition system, please be kind enough to give this repository a \'Star\'.\r\n'