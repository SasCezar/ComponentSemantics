b"# m2cgen\n\n[![Build Status](https://travis-ci.org/BayesWitnesses/m2cgen.svg?branch=master)](https://travis-ci.org/BayesWitnesses/m2cgen)\n[![Coverage Status](https://coveralls.io/repos/github/BayesWitnesses/m2cgen/badge.svg?branch=master)](https://coveralls.io/github/BayesWitnesses/m2cgen?branch=master)\n[![License: MIT](https://img.shields.io/github/license/BayesWitnesses/m2cgen.svg)](https://github.com/BayesWitnesses/m2cgen/blob/master/LICENSE)\n[![Python Versions](https://img.shields.io/pypi/pyversions/m2cgen.svg?logo=python&logoColor=white)](https://pypi.org/project/m2cgen)\n[![PyPI Version](https://img.shields.io/pypi/v/m2cgen.svg?logo=pypi&logoColor=white)](https://pypi.org/project/m2cgen)\n\n**m2cgen** (Model 2 Code Generator) - is a lightweight library which provides an easy way to transpile trained statistical models into a native code (Python, C, Java, Go, JavaScript, Visual Basic).\n\n* [Installation](#installation)\n* [Supported Languages](#supported-languages)\n* [Supported Models](#supported-models)\n* [Classification Output](#classification-output)\n* [Usage](#usage)\n* [CLI](#cli)\n* [FAQ](#faq)\n\n## Installation\nSupported Python version is >= **3.4**.\n```\npip install m2cgen\n```\n\n\n## Supported Languages\n\n- C\n- Go\n- Java\n- JavaScript\n- Python\n- Visual Basic\n\n## Supported Models\n\n|  | Classification | Regression |\n| --- | --- | --- |\n| **Linear** | LogisticRegression, LogisticRegressionCV, RidgeClassifier, RidgeClassifierCV, SGDClassifier, PassiveAggressiveClassifier | LinearRegression, HuberRegressor, ElasticNet, ElasticNetCV, TheilSenRegressor, Lars, LarsCV, Lasso, LassoCV, LassoLars, LassoLarsIC, OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV, Ridge, RidgeCV, BayesianRidge, ARDRegression, SGDRegressor, PassiveAggressiveRegressor |\n| **SVM** | SVC, NuSVC, LinearSVC | SVR, NuSVR, LinearSVR |\n| **Tree** | DecisionTreeClassifier, ExtraTreeClassifier | DecisionTreeRegressor, ExtraTreeRegressor |\n| **Random Forest** | RandomForestClassifier, ExtraTreesClassifier | RandomForestRegressor, ExtraTreesRegressor |\n| **Boosting** | XGBClassifier(gbtree/dart booster only), LGBMClassifier(gbdt/dart booster only) | XGBRegressor(gbtree/dart booster only), LGBMRegressor(gbdt/dart booster only) |\n\n## Classification Output\n### Linear/Linear SVM\n#### Binary\nScalar value; signed distance of the sample to the hyperplane for the second class.\n#### Multiclass\nVector value; signed distance of the sample to the hyperplane per each class.\n#### Comment\nThe output is consistent with the output of ```LinearClassifierMixin.decision_function```.\n\n### SVM\n#### Binary\nScalar value; signed distance of the sample to the hyperplane for the second class.\n#### Multiclass\nVector value; one-vs-one score for each class, shape (n_samples, n_classes * (n_classes-1) / 2).\n#### Comment\nThe output is consistent with the output of ```BaseSVC.decision_function``` when the `decision_function_shape` is set to `ovo`.\n\n### Tree/Random Forest/XGBoost/LightGBM\n#### Binary\nVector value; class probabilities.\n#### Multiclass\nVector value; class probabilities.\n#### Comment\nThe output is consistent with the output of the `predict_proba` method of `DecisionTreeClassifier`/`ForestClassifier`/`XGBClassifier`/`LGBMClassifier`.\n\n## Usage\n\nHere's a simple example of how a linear model trained in Python environment can be represented in Java code:\n```python\nfrom sklearn.datasets import load_boston\nfrom sklearn import linear_model\nimport m2cgen as m2c\n\nboston = load_boston()\nX, y = boston.data, boston.target\n\nestimator = linear_model.LinearRegression()\nestimator.fit(X, y)\n\ncode = m2c.export_to_java(estimator)\n```\n\nGenerated Java code:\n```java\npublic class Model {\n\n    public static double score(double[] input) {\n        return (((((((((((((36.45948838508965) + ((input[0]) * (-0.10801135783679647))) + ((input[1]) * (0.04642045836688297))) + ((input[2]) * (0.020558626367073608))) + ((input[3]) * (2.6867338193449406))) + ((input[4]) * (-17.76661122830004))) + ((input[5]) * (3.8098652068092163))) + ((input[6]) * (0.0006922246403454562))) + ((input[7]) * (-1.475566845600257))) + ((input[8]) * (0.30604947898516943))) + ((input[9]) * (-0.012334593916574394))) + ((input[10]) * (-0.9527472317072884))) + ((input[11]) * (0.009311683273794044))) + ((input[12]) * (-0.5247583778554867));\n    }\n}\n```\n\n**You can find more examples of generated code for different models/languages [here](https://github.com/BayesWitnesses/m2cgen/tree/master/generated_code_examples).**\n\n## CLI\n\n`m2cgen` can be used as a CLI tool to generate code using serialized model objects (pickle protocol):\n```\n$ m2cgen <pickle_file> --language <language> [--indent <indent>]\n         [--class_name <class_name>] [--module_name <module_name>] [--package_name <package_name>]\n         [--recursion-limit <recursion_limit>]\n```\n\nPiping is also supported:\n```\n$ cat <pickle_file> | m2cgen --language <language>\n```\n\n## FAQ\n**Q: Generation fails with `RuntimeError: maximum recursion depth exceeded` error.**\n\nA: If this error occurs while generating code using an ensemble model, try to reduce the number of trained estimators within that model. Alternatively you can increase the maximum recursion depth with `sys.setrecursionlimit(<new_depth>)`.\n"