b'<div align="center">\n  <img src="images/logo.png" alt="prince_logo"/>\n</div>\n\n<br/>\n\n<div align="center">\n  <!-- Python version -->\n  <a href="https://pypi.python.org/pypi/prince">\n    <img src="https://img.shields.io/badge/python-3.x-blue.svg?style=for-the-badge" alt="PyPI version"/>\n  </a>\n  <!-- PyPi -->\n  <a href="https://pypi.org/project/prince/">\n    <img src="https://img.shields.io/pypi/v/prince.svg?style=for-the-badge" alt="pypi" />\n  </a>\n  <!-- Build status -->\n  <a href="https://travis-ci.org/MaxHalford/prince?branch=master">\n    <img src="https://img.shields.io/travis/MaxHalford/prince/master.svg?style=for-the-badge" alt="Build Status" />\n  </a>\n  <!-- Test coverage -->\n  <a href="https://coveralls.io/github/MaxHalford/prince?branch=master">\n    <img src="https://img.shields.io/codecov/c/gh/MaxHalford/prince.svg?style=for-the-badge" alt="Coverage Status"/>\n  </a>\n  <!-- License -->\n  <a href="https://opensource.org/licenses/MIT">\n    <img src="http://img.shields.io/:license-mit-ff69b4.svg?style=for-the-badge" alt="license"/>\n  </a>\n</div>\n\n<br/>\n\nPrince is a library for doing [factor analysis](https://www.wikiwand.com/en/Factor_analysis). This includes a variety of methods including [principal component analysis (PCA)](https://www.wikiwand.com/en/Principal_component_analysis) and [correspondence analysis (CA)](https://www.wikiwand.com/en/Correspondence_analysis). The goal is to provide an efficient implementation for each algorithm along with a scikit-learn API.\n\n## Table of contents\n\n- [Installation](#installation)\n- [Usage](#usage)\n  - [Guidelines](#guidelines)\n  - [Principal component analysis (PCA)](#principal-component-analysis-pca)\n  - [Correspondence analysis (CA)](#correspondence-analysis-ca)\n  - [Multiple correspondence analysis (MCA)](#multiple-correspondence-analysis-mca)\n  - [Multiple factor analysis (MFA)](#multiple-factor-analysis-mfa)\n  - [Factor analysis of mixed data (FAMD)](#factor-analysis-of-mixed-data-famd)\n- [Going faster](#going-faster)\n- [License](#license)\n\n## Installation\n\n:warning: Prince is only compatible with **Python 3**.\n\n:snake: Although it isn\'t a requirement, using [Anaconda](https://www.continuum.io/downloads) is highly recommended.\n\n**Via PyPI**\n\n```sh\n>>> pip install prince  # doctest: +SKIP\n```\n\n**Via GitHub for the latest development version**\n\n```sh\n>>> pip install git+https://github.com/MaxHalford/Prince  # doctest: +SKIP\n```\n\nPrince doesn\'t have any extra dependencies apart from the usual suspects (`sklearn`, `pandas`, `matplotlib`) which are included with Anaconda.\n\n## Usage\n\n```python\nimport numpy as np; np.random.set_state(42)  # This is for doctests reproducibility\n```\n\n### Guidelines\n\nEach estimator provided by `prince` extends scikit-learn\'s `TransformerMixin`. This means that each estimator implements a `fit` and a `transform` method which makes them usable in a transformation pipeline. The `fit` method is actually an alias for the `row_principal_components` method which returns the row principal components. However you can also access the column principal components with the `column_principal_components`.\n\nUnder the hood Prince uses a [randomised version of SVD](https://research.fb.com/fast-randomized-svd/). This is much faster than using the more commonly full approach. However the results may have a small inherent randomness. For most applications this doesn\'t matter and you shouldn\'t have to worry about it. However if you want reproducible results then you should set the `random_state` parameter.\n\nThe randomised version of SVD is an iterative method. Because each of Prince\'s algorithms use SVD, they all possess a `n_iter` parameter which controls the number of iterations used for computing the SVD. On the one hand the higher `n_iter` is the more precise the results will be. On the other hand increasing `n_iter` increases the computation time. In general the algorithm converges very quickly so using a low `n_iter` (which is the default behaviour) is recommended.\n\nYou are supposed to use each method depending on your situation:\n\n- All your variables are numeric: use principal component analysis (`prince.PCA`)\n- You have a contingency table: use correspondence analysis (`prince.CA`)\n- You have more than 2 variables and they are all categorical: use multiple correspondence analysis (`prince.MCA`)\n- You have groups of categorical **or** numerical variables: use multiple factor analysis (`prince.MFA`)\n- You have both categorical and numerical variables: use factor analysis of mixed data (`prince.FAMD`)\n\nThe next subsections give an overview of each method along with usage information. The following papers give a good overview of the field of factor analysis if you want to go deeper:\n\n- [A Tutorial on Principal Component Analysis](https://arxiv.org/pdf/1404.1100.pdf)\n- [Theory of Correspondence Analysis](http://statmath.wu.ac.at/courses/CAandRelMeth/caipA.pdf)\n- [Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions](https://arxiv.org/pdf/0909.4061.pdf)\n- [Computation of Multiple Correspondence Analysis, with code in R](https://core.ac.uk/download/pdf/6591520.pdf)\n- [Singular Value Decomposition Tutorial](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf)\n- [Multiple Factor Analysis](https://www.utdallas.edu/~herve/Abdi-MFA2007-pretty.pdf)\n\n### Principal component analysis (PCA)\n\nIf you\'re using PCA it is assumed you have a dataframe consisting of numerical continuous variables. In this example we\'re going to be using the [Iris flower dataset](https://www.wikiwand.com/en/Iris_flower_data_set).\n\n```python\n>>> import pandas as pd\n>>> import prince\n>>> from sklearn import datasets\n\n>>> X, y = datasets.load_iris(return_X_y=True)\n>>> X = pd.DataFrame(data=X, columns=[\'Sepal length\', \'Sepal width\', \'Petal length\', \'Petal width\'])\n>>> y = pd.Series(y).map({0: \'Setosa\', 1: \'Versicolor\', 2: \'Virginica\'})\n>>> X.head()\n   Sepal length  Sepal width  Petal length  Petal width\n0           5.1          3.5           1.4          0.2\n1           4.9          3.0           1.4          0.2\n2           4.7          3.2           1.3          0.2\n3           4.6          3.1           1.5          0.2\n4           5.0          3.6           1.4          0.2\n\n```\n\nThe `PCA` class implements scikit-learn\'s `fit`/`transform` API. It\'s parameters have to passed at initialisation before calling the `fit` method.\n\n```python\n>>> pca = prince.PCA(\n...     n_components=2,\n...     n_iter=3,\n...     rescale_with_mean=True,\n...     rescale_with_std=True,\n...     copy=True,\n...     check_input=True,\n...     engine=\'auto\',\n...     random_state=42\n... )\n>>> pca = pca.fit(X)\n\n```\n\nThe available parameters are:\n\n- `n_components`: the number of components that are computed. You only need two if your intention is to make a chart.\n- `n_iter`: the number of iterations used for computing the SVD\n- `rescale_with_mean`: whether to substract each column\'s mean\n- `rescale_with_std`: whether to divide each column by it\'s standard deviation\n- `copy`: if `False` then the computations will be done inplace which can have possible side-effects on the input data\n- `engine`: what SVD engine to use (should be one of `[\'auto\', \'fbpca\', \'sklearn\']`)\n- `random_state`: controls the randomness of the SVD results.\n\nOnce the `PCA` has been fitted, it can be used to extract the row principal coordinates as so:\n\n```python\n>>> pca.transform(X).head()  # Same as pca.row_coordinates(X).head()\n          0         1\n0 -2.264703  0.480027\n1 -2.080961 -0.674134\n2 -2.364229 -0.341908\n3 -2.299384 -0.597395\n4 -2.389842  0.646835\n\n```\n\nEach column stands for a principal component whilst each row stands a row in the original dataset. You can display these projections with the `plot_row_coordinates` method:\n\n```python\n>>> ax = pca.plot_row_coordinates(\n...     X,\n...     ax=None,\n...     figsize=(6, 6),\n...     x_component=0,\n...     y_component=1,\n...     labels=None,\n...     color_labels=y,\n...     ellipse_outline=False,\n...     ellipse_fill=True,\n...     show_points=True\n... )\n>>> ax.get_figure().savefig(\'images/pca_row_coordinates.svg\')\n\n```\n\n<div align="center">\n  <img src="images/pca_row_coordinates.svg" />\n</div>\n\nEach principal component explains part of the underlying of the distribution. You can see by how much by using the accessing the `explained_inertia_` property:\n\n```python\n>>> pca.explained_inertia_  # doctest: +ELLIPSIS\n[0.729624..., 0.228507...]\n\n```\n\nThe explained inertia represents the percentage of the inertia each principal component contributes. It sums up to 1 if the `n_components` property is equal to the number of columns in the original dataset. you The explained inertia is obtained by dividing the eigenvalues obtained with the SVD by the total inertia, both of which are also accessible.\n\n```python\n>>> pca.eigenvalues_  # doctest: +ELLIPSIS\n[437.774672..., 137.104570...]\n\n>>> pca.total_inertia_  # doctest: +ELLIPSIS\n600.0...\n\n>>> pca.explained_inertia_\n[0.729624..., 0.228507...]\n\n```\n\nYou can also obtain the correlations between the original variables and the principal components.\n\n```python\n>>> pca.column_correlations(X)\n                     0         1\nPetal length  0.991555  0.023415\nPetal width   0.964979  0.064000\nSepal length  0.890169  0.360830\nSepal width  -0.460143  0.882716\n\n```\n\nYou may also want to know how much each observation contributes to each principal component. This can be done with the `row_contributions` method.\n\n```python\n>>> pca.row_contributions(X).head()\n          0         1\n0  0.011716  0.001681\n1  0.009892  0.003315\n2  0.012768  0.000853\n3  0.012077  0.002603\n4  0.013046  0.003052\n\n```\n\nYou can also transform row projections back into their original space by using the `inverse_transform` method.\n\n```python\n>>> pca.inverse_transform(pca.transform(X)).head()\n          0         1         2         3\n0  5.018949  3.514854  1.466013  0.251922\n1  4.738463  3.030433  1.603913  0.272074\n2  4.720130  3.196830  1.328961  0.167414\n3  4.668436  3.086770  1.384170  0.182247\n4  5.017093  3.596402  1.345411  0.206706\n\n```\n\n### Correspondence analysis (CA)\n\nYou should be using correspondence analysis when you want to analyse a contingency table. In other words you want to analyse the dependencies between two categorical variables. The following example comes from section 17.2.3 of [this textbook](http://ce.aut.ac.ir/~shiry/lecture/Advanced%20Machine%20Learning/Manifold_Modern_Multivariate%20Statistical%20Techniques%20-%20Regres.pdf). It shows the number of occurrences between different hair and eye colors.\n\n```python\n>>> import pandas as pd\n\n>>> pd.set_option(\'display.float_format\', lambda x: \'{:.6f}\'.format(x))\n>>> X = pd.DataFrame(\n...    data=[\n...        [326, 38, 241, 110, 3],\n...        [688, 116, 584, 188, 4],\n...        [343, 84, 909, 412, 26],\n...        [98, 48, 403, 681, 85]\n...    ],\n...    columns=pd.Series([\'Fair\', \'Red\', \'Medium\', \'Dark\', \'Black\']),\n...    index=pd.Series([\'Blue\', \'Light\', \'Medium\', \'Dark\'])\n... )\n>>> X\n        Fair  Red  Medium  Dark  Black\nBlue     326   38     241   110      3\nLight    688  116     584   188      4\nMedium   343   84     909   412     26\nDark      98   48     403   681     85\n\n```\n\nUnlike the `PCA` class, the `CA` only exposes scikit-learn\'s `fit` method.\n\n```python\n>>> import prince\n>>> ca = prince.CA(\n...     n_components=2,\n...     n_iter=3,\n...     copy=True,\n...     check_input=True,\n...     engine=\'auto\',\n...     random_state=42\n... )\n>>> X.columns.rename(\'Hair color\', inplace=True)\n>>> X.index.rename(\'Eye color\', inplace=True)\n>>> ca = ca.fit(X)\n\n```\n\nThe parameters and methods overlap with those proposed by the `PCA` class.\n\n```python\n>>> ca.row_coordinates(X)\n               0         1\nBlue   -0.400300 -0.165411\nLight  -0.440708 -0.088463\nMedium  0.033614  0.245002\nDark    0.702739 -0.133914\n\n>>> ca.column_coordinates(X)\n               0         1\nFair   -0.543995 -0.173844\nRed    -0.233261 -0.048279\nMedium -0.042024  0.208304\nDark    0.588709 -0.103950\nBlack   1.094388 -0.286437\n\n```\n\nYou can plot both sets of principal coordinates with the `plot_coordinates` method.\n\n```python\n>>> ax = ca.plot_coordinates(\n...     X=X,\n...     ax=None,\n...     figsize=(6, 6),\n...     x_component=0,\n...     y_component=1,\n...     show_row_labels=True,\n...     show_col_labels=True\n... )\n>>> ax.get_figure().savefig(\'images/ca_coordinates.svg\')\n\n```\n\n<div align="center">\n  <img src="images/ca_coordinates.svg" />\n</div>\n\nLike for the `PCA` you can access the inertia contribution of each principal component as well as the eigenvalues and the total inertia.\n\n```python\n>>> ca.eigenvalues_  # doctest: +ELLIPSIS\n[0.199244..., 0.030086...]\n\n>>> ca.total_inertia_  # doctest: +ELLIPSIS\n0.230191...\n\n>>> ca.explained_inertia_  # doctest: +ELLIPSIS\n[0.865562..., 0.130703...]\n\n```\n\n### Multiple correspondence analysis (MCA)\n\nMultiple correspondence analysis (MCA) is an extension of correspondence analysis (CA). It should be used when you have more than two categorical variables. The idea is simply to compute the one-hot encoded version of a dataset and apply CA on it. As an example we\'re going to use the [balloons dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/) taken from the [UCI datasets website](https://archive.ics.uci.edu/ml/datasets.html).\n\n```python\n>>> import pandas as pd\n\n>>> X = pd.read_csv(\'https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data\')\n>>> X.columns = [\'Color\', \'Size\', \'Action\', \'Age\', \'Inflated\']\n>>> X.head()\n    Color   Size   Action    Age Inflated\n0  YELLOW  SMALL  STRETCH  ADULT        T\n1  YELLOW  SMALL  STRETCH  CHILD        F\n2  YELLOW  SMALL      DIP  ADULT        F\n3  YELLOW  SMALL      DIP  CHILD        F\n4  YELLOW  LARGE  STRETCH  ADULT        T\n\n```\n\nThe `MCA` also implements the `fit` and `transform` methods.\n\n```python\n>>> import prince\n>>> mca = prince.MCA(\n...     n_components=2,\n...     n_iter=3,\n...     copy=True,\n...     check_input=True,\n...     engine=\'auto\',\n...     random_state=42\n... )\n>>> mca = mca.fit(X)\n\n```\n\nLike the `CA` class, the `MCA` class also has `plot_coordinates` method.\n\n```python\n>>> ax = mca.plot_coordinates(\n...     X=X,\n...     ax=None,\n...     figsize=(6, 6),\n...     show_row_points=True,\n...     row_points_size=10,\n...     show_row_labels=False,\n...     show_column_points=True,\n...     column_points_size=30,\n...     show_column_labels=False,\n...     legend_n_cols=1\n... )\n>>> ax.get_figure().savefig(\'images/mca_coordinates.svg\')\n\n```\n\n<div align="center">\n  <img src="images/mca_coordinates.svg" />\n</div>\n\nThe eigenvalues and inertia values are also accessible.\n\n```python\n>>> mca.eigenvalues_  # doctest: +ELLIPSIS\n[0.401656..., 0.211111...]\n\n>>> mca.total_inertia_\n1.0\n\n>>> mca.explained_inertia_  # doctest: +ELLIPSIS\n[0.401656..., 0.211111...]\n\n```\n\n\n### Multiple factor analysis (MFA)\n\nMultiple factor analysis (MFA) is meant to be used when you have groups of variables. In practice it builds a PCA on each group -- or an MCA, depending on the types of the group\'s variables. It then constructs a global PCA on the results of the so-called partial PCAs -- or MCAs. The dataset used in the following examples come from [this paper](https://www.utdallas.edu/~herve/Abdi-MFA2007-pretty.pdf). In the dataset, three experts give their opinion on six different wines. Each opinion for each wine is recorded as a variable. We thus want to consider the separate opinions of each expert whilst also having a global overview of each wine. MFA is the perfect fit for this kind of situation.\n\nFirst of all let\'s copy the data used in the paper.\n\n```python\n>>> import pandas as pd\n\n>>> X = pd.DataFrame(\n...     data=[\n...         [1, 6, 7, 2, 5, 7, 6, 3, 6, 7],\n...         [5, 3, 2, 4, 4, 4, 2, 4, 4, 3],\n...         [6, 1, 1, 5, 2, 1, 1, 7, 1, 1],\n...         [7, 1, 2, 7, 2, 1, 2, 2, 2, 2],\n...         [2, 5, 4, 3, 5, 6, 5, 2, 6, 6],\n...         [3, 4, 4, 3, 5, 4, 5, 1, 7, 5]\n...     ],\n...     columns=[\'E1 fruity\', \'E1 woody\', \'E1 coffee\',\n...              \'E2 red fruit\', \'E2 roasted\', \'E2 vanillin\', \'E2 woody\',\n...              \'E3 fruity\', \'E3 butter\', \'E3 woody\'],\n...     index=[\'Wine {}\'.format(i+1) for i in range(6)]\n... )\n>>> X[\'Oak type\'] = [1, 2, 2, 2, 1, 1]\n\n```\n\nThe groups are passed as a dictionary to the `MFA` class.\n\n```python\n>>> groups = {\n...    \'Expert #{}\'.format(no+1): [c for c in X.columns if c.startswith(\'E{}\'.format(no+1))]\n...    for no in range(3)\n... }\n>>> import pprint\n>>> pprint.PrettyPrinter().pprint(groups)\n{\'Expert #1\': [\'E1 fruity\', \'E1 woody\', \'E1 coffee\'],\n \'Expert #2\': [\'E2 red fruit\', \'E2 roasted\', \'E2 vanillin\', \'E2 woody\'],\n \'Expert #3\': [\'E3 fruity\', \'E3 butter\', \'E3 woody\']}\n\n```\n\nNow we can fit an `MFA`.\n\n```python\n>>> import prince\n>>> mfa = prince.MFA(\n...     groups=groups,\n...     n_components=2,\n...     n_iter=3,\n...     copy=True,\n...     check_input=True,\n...     engine=\'auto\',\n...     random_state=42\n... )\n>>> mfa = mfa.fit(X)\n\n```\n\nThe `MFA` inherits from the `PCA` class, which entails that you have access to all it\'s methods and properties. The `row_coordinates` method will return the global coordinates of each wine.\n\n```python\n>>> mfa.row_coordinates(X)\n               0         1\nWine 1 -2.172155 -0.508596\nWine 2  0.557017 -0.197408\nWine 3  2.317663 -0.830259\nWine 4  1.832557  0.905046\nWine 5 -1.403787  0.054977\nWine 6 -1.131296  0.576241\n\n```\n\nJust like for the `PCA` you can plot the row coordinates with the `plot_row_coordinates` method.\n\n```python\n>>> ax = mfa.plot_row_coordinates(\n...     X,\n...     ax=None,\n...     figsize=(6, 6),\n...     x_component=0,\n...     y_component=1,\n...     labels=X.index,\n...     color_labels=[\'Oak type {}\'.format(t) for t in X[\'Oak type\']],\n...     ellipse_outline=False,\n...     ellipse_fill=True,\n...     show_points=True\n... )\n>>> ax.get_figure().savefig(\'images/mfa_row_coordinates.svg\')\n\n```\n\n<div align="center">\n  <img src="images/mfa_row_coordinates.svg" />\n</div>\n\nYou can also obtain the row coordinates inside each group. The `partial_row_coordinates` method returns a `pandas.DataFrame` where the set of columns is a `pandas.MultiIndex`. The first level of indexing corresponds to each specified group whilst the nested level indicates the coordinates inside each group.\n\n```python\n>>> mfa.partial_row_coordinates(X)  # doctest: +NORMALIZE_WHITESPACE\n  Expert #1           Expert #2           Expert #3\n               0         1         0         1         0         1\nWine 1 -2.764432 -1.104812 -2.213928 -0.863519 -1.538106  0.442545\nWine 2  0.773034  0.298919  0.284247 -0.132135  0.613771 -0.759009\nWine 3  1.991398  0.805893  2.111508  0.499718  2.850084 -3.796390\nWine 4  1.981456  0.927187  2.393009  1.227146  1.123206  0.560803\nWine 5 -1.292834 -0.620661 -1.492114 -0.488088 -1.426414  1.273679\nWine 6 -0.688623 -0.306527 -1.082723 -0.243122 -1.622541  2.278372\n\n```\n\nLikewhise you can visualize the partial row coordinates with the `plot_partial_row_coordinates` method.\n\n```python\n>>> ax = mfa.plot_partial_row_coordinates(\n...     X,\n...     ax=None,\n...     figsize=(6, 6),\n...     x_component=0,\n...     y_component=1,\n...     color_labels=[\'Oak type {}\'.format(t) for t in X[\'Oak type\']]\n... )\n>>> ax.get_figure().savefig(\'images/mfa_partial_row_coordinates.svg\')\n\n```\n\n<div align="center">\n  <img src="images/mfa_partial_row_coordinates.svg" />\n</div>\n\nAs usual you have access to inertia information.\n\n```python\n>>> mfa.eigenvalues_  # doctest: +ELLIPSIS\n[2.834800..., 0.356859...]\n\n>>> mfa.total_inertia_\n3.353004...\n\n>>> mfa.explained_inertia_  # doctest: +ELLIPSIS\n[0.845450..., 0.106429...]\n\n```\n\nYou can also access information concerning each partial factor analysis via the `partial_factor_analysis_` attribute.\n\n```python\n>>> for name, fa in sorted(mfa.partial_factor_analysis_.items()):  # doctest: +ELLIPSIS\n...     print(\'{} eigenvalues: {}\'.format(name, fa.eigenvalues_))\nExpert #1 eigenvalues: [2.862595..., 0.119836...]\nExpert #2 eigenvalues: [3.651083..., 0.194159...]\nExpert #3 eigenvalues: [2.480488..., 0.441195...]\n\n```\n\nThe `row_contributions` method will provide you with the inertia contribution of each row with respect to each component.\n\n```python\n>>> mfa.row_contributions(X)\n              0        1\nWine 1 1.664406 0.724851\nWine 2 0.109450 0.109203\nWine 3 1.894865 1.931661\nWine 4 1.184657 2.295325\nWine 5 0.695152 0.008470\nWine 6 0.451471 0.930490\n\n```\n\nThe `column_correlations` method will return the correlation between the original variables and the components.\n\n```python\n>>> mfa.column_correlations(X)\n                     0         1\nE1 coffee    -0.918449 -0.043444\nE1 fruity     0.968449  0.192294\nE1 woody     -0.984442 -0.120198\nE2 red fruit  0.887263  0.357632\nE2 roasted   -0.955795  0.026039\nE2 vanillin  -0.950629 -0.177883\nE2 woody     -0.974649  0.127239\nE3 butter    -0.945767  0.221441\nE3 fruity     0.594649 -0.820777\nE3 woody     -0.992337  0.029747\n\n```\n\n\n### Factor analysis of mixed data (FAMD)\n\nA description is on it\'s way. This section is empty because I have to refactor the documentation a bit.\n\n```python\n>>> import pandas as pd\n\n>>> X = pd.DataFrame(\n...     data=[\n...         [\'A\', \'A\', \'A\', 2, 5, 7, 6, 3, 6, 7],\n...         [\'A\', \'A\', \'A\', 4, 4, 4, 2, 4, 4, 3],\n...         [\'B\', \'A\', \'B\', 5, 2, 1, 1, 7, 1, 1],\n...         [\'B\', \'A\', \'B\', 7, 2, 1, 2, 2, 2, 2],\n...         [\'B\', \'B\', \'B\', 3, 5, 6, 5, 2, 6, 6],\n...         [\'B\', \'B\', \'A\', 3, 5, 4, 5, 1, 7, 5]\n...     ],\n...     columns=[\'E1 fruity\', \'E1 woody\', \'E1 coffee\',\n...              \'E2 red fruit\', \'E2 roasted\', \'E2 vanillin\', \'E2 woody\',\n...              \'E3 fruity\', \'E3 butter\', \'E3 woody\'],\n...     index=[\'Wine {}\'.format(i+1) for i in range(6)]\n... )\n>>> X[\'Oak type\'] = [1, 2, 2, 2, 1, 1]\n\n```\n\nNow we can fit an `FAMD`.\n\n```python\n>>> import prince\n>>> famd = prince.FAMD(\n...     n_components=2,\n...     n_iter=3,\n...     copy=True,\n...     check_input=True,\n...     engine=\'auto\',\n...     random_state=42\n... )\n>>> famd = famd.fit(X.drop(\'Oak type\', axis=\'columns\'))  # No need for \'Oak type\'\n\n```\n\nThe `FAMD` inherits from the `MFA` class, which entails that you have access to all it\'s methods and properties. The `row_coordinates` method will return the global coordinates of each wine.\n\n```python\n>>> famd.row_coordinates(X)\n              0         1\nWine 1 3.351475  4.278852\nWine 2 3.396873  4.135743\nWine 3 4.777638 -1.643254\nWine 4 4.769714 -1.665251\nWine 5 3.779385 -3.053543\nWine 6 3.465413 -0.304409\n\n```\n\nJust like for the `MFA` you can plot the row coordinates with the `plot_row_coordinates` method.\n\n```python\n>>> ax = famd.plot_row_coordinates(\n...     X,\n...     ax=None,\n...     figsize=(6, 6),\n...     x_component=0,\n...     y_component=1,\n...     labels=X.index,\n...     color_labels=[\'Oak type {}\'.format(t) for t in X[\'Oak type\']],\n...     ellipse_outline=False,\n...     ellipse_fill=True,\n...     show_points=True\n... )\n>>> ax.get_figure().savefig(\'images/famd_row_coordinates.svg\')\n\n```\n\n<div align="center">\n  <img src="images/famd_row_coordinates.svg" />\n</div>\n\n\n## Going faster\n\nBy default `prince` uses `sklearn`\'s randomized SVD implementation (the one used under the hood for [`TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)). One of the goals of Prince is to make it possible to use a different SVD backend. For the while the only other supported backend is [Facebook\'s randomized SVD implementation](https://research.facebook.com/blog/fast-randomized-svd/) called [fbpca](http://fbpca.readthedocs.org/en/latest/). You can use it by setting the `engine` parameter to `\'fbpca\'`:\n\n```python\n>>> import prince\n>>> pca = prince.PCA(engine=\'fbpca\')\n\n```\n\nIf you are using Anaconda then you should be able to install `fbpca` without any pain by running `pip install fbpca`.\n\n\n## License\n\nThe MIT License (MIT). Please see the [license file](LICENSE) for more information.\n'