b'# Deploy machine learning models in production\n\nCortex is an open source platform for deploying machine learning models\xe2\x80\x94trained with nearly any framework\xe2\x80\x94as production web services.\n\n<br>\n\n<!-- Delete on release branches -->\n<!-- CORTEX_VERSION_README_MINOR -->\n[install](https://www.cortex.dev/install) \xe2\x80\xa2 [tutorial](https://www.cortex.dev/v/master/iris-classifier) \xe2\x80\xa2 [docs](https://www.cortex.dev) \xe2\x80\xa2 [examples](https://github.com/cortexlabs/cortex/tree/0.10/examples) \xe2\x80\xa2 [we\'re hiring](https://angel.co/cortex-labs-inc/jobs) \xe2\x80\xa2 [email us](mailto:hello@cortex.dev) \xe2\x80\xa2 [chat with us](https://gitter.im/cortexlabs/cortex)<br><br>\n\n<!-- Set header Cache-Control=no-cache on the S3 object metadata (see https://help.github.com/en/articles/about-anonymized-image-urls) -->\n![Demo](https://d1zqebknpdh033.cloudfront.net/demo/gif/v0.8.gif)\n\n<br>\n\n## Key features\n\n- **Autoscaling:** Cortex automatically scales APIs to handle production workloads.\n\n- **Multi framework:** Cortex supports TensorFlow, PyTorch, scikit-learn, XGBoost, and more.\n\n- **CPU / GPU support:** Cortex can run inference on CPU or GPU infrastructure.\n\n- **Spot instances:** Cortex supports EC2 spot instances.\n\n- **Rolling updates:** Cortex updates deployed APIs without any downtime.\n\n- **Log streaming:** Cortex streams logs from deployed models to your CLI.\n\n- **Prediction monitoring:** Cortex monitors network metrics and tracks predictions.\n\n- **Minimal configuration:** Deployments are defined in a single `cortex.yaml` file.\n\n<br>\n\n## Usage\n\n### Implement your predictor\n\n```python\n# predictor.py\n\nmodel = download_model()\n\ndef predict(sample, metadata):\n    return model.predict(sample["text"])\n```\n\n### Configure your deployment\n\n```yaml\n# cortex.yaml\n\n- kind: deployment\n  name: sentiment\n\n- kind: api\n  name: classifier\n  predictor:\n    path: predictor.py\n  tracker:\n    model_type: classification\n  compute:\n    gpu: 1\n    mem: 4G\n```\n\n### Deploy to AWS\n\n```bash\n$ cortex deploy\n\ncreating classifier (http://***.amazonaws.com/sentiment/classifier)\n```\n\n### Serve real-time predictions\n\n```bash\n$ curl http://***.amazonaws.com/sentiment/classifier \\\n    -X POST -H "Content-Type: application/json" \\\n    -d \'{"text": "the movie was amazing!"}\'\n\npositive\n```\n\n### Monitor your deployment\n\n```bash\n$ cortex get classifier --watch\n\nstatus   up-to-date   available   requested   last update   avg latency\nlive     1            1           1           8s            24ms\n\nclass     count\npositive  8\nnegative  4\n```\n\n<br>\n\n## How it works\n\nThe CLI sends configuration and code to the cluster every time you run `cortex deploy`. Each model is loaded into a Docker container, along with any Python packages and request handling code. The model is exposed as a web service using Elastic Load Balancing (ELB), TensorFlow Serving, and ONNX Runtime. The containers are orchestrated on Elastic Kubernetes Service (EKS) while logs and metrics are streamed to CloudWatch.\n\n<br>\n\n## Examples\n\n<!-- CORTEX_VERSION_README_MINOR x5 -->\n- [Sentiment analysis](https://github.com/cortexlabs/cortex/tree/0.10/examples/tensorflow/sentiment-analyzer) in TensorFlow with BERT\n- [Image classification](https://github.com/cortexlabs/cortex/tree/0.10/examples/tensorflow/image-classifier) in TensorFlow with Inception\n- [Text generation](https://github.com/cortexlabs/cortex/tree/0.10/examples/pytorch/text-generator) in PyTorch with DistilGPT2\n- [Reading comprehension](https://github.com/cortexlabs/cortex/tree/0.10/examples/pytorch/text-generator) in PyTorch with ELMo-BiDAF\n- [Iris classification](https://github.com/cortexlabs/cortex/tree/0.10/examples/sklearn/iris-classifier) in scikit-learn\n'