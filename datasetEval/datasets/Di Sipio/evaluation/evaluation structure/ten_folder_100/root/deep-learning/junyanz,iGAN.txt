b'## iGAN: Interactive Image Generation via Generative Adversarial Networks\n[Project](http://efrosgans.eecs.berkeley.edu/iGAN/) |  [Youtube](https://youtu.be/9c4z6YsBGQ0) |  [Paper](https://arxiv.org/abs/1609.03552)  \n\nRecent projects:  \n[[pix2pix]](https://github.com/phillipi/pix2pix): Torch implementation for learning a mapping from input images to output images.  \n[[CycleGAN]](https://github.com/junyanz/CycleGAN): Torch implementation for learning an image-to-image translation (i.e., pix2pix) without input-output pairs.  \n[[pytorch-CycleGAN-and-pix2pix]](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix): PyTorch implementation for both unpaired and paired image-to-image translation.\n\n\n<img src=\'pics/demo.gif\' width=320>\n\n## Overview\niGAN (aka. interactive GAN) is the author\'s implementation of interactive image generation interface described in:  \n"Generative Visual Manipulation on the Natural Image Manifold"   \n[Jun-Yan Zhu](https://people.eecs.berkeley.edu/~junyanz/), [Philipp Kr\xc3\xa4henb\xc3\xbchl](http://www.philkr.net/), [Eli Shechtman](https://research.adobe.com/person/eli-shechtman/), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros/)    \nIn European Conference on Computer Vision (ECCV) 2016\n\n<img src=\'pics/demo_teaser.jpg\' width=800>\n\n\nGiven a few user strokes, our system could produce photo-realistic samples that best satisfy the user edits in real-time. Our system is based on deep generative models such as Generative Adversarial Networks ([GAN](https://arxiv.org/abs/1406.2661)) and [DCGAN](https://github.com/Newmu/dcgan_code). The system serves the following two purposes:\n* An intelligent drawing interface for automatically generating images inspired by the color and shape of the brush strokes.\n* An interactive visual debugging tool for understanding and visualizing deep generative models. By interacting with the generative model, a developer can understand what visual content the model can produce, as well as the limitation of the model.\n\nPlease cite our paper if you find this code useful in your research. (Contact: Jun-Yan Zhu, junyanz at mit dot edu)\n\n## Getting started\n* Install the python libraries. (See [Requirements](https://github.com/junyanz/iGAN#requirements)).\n* Download the code from GitHub:\n```bash\ngit clone https://github.com/junyanz/iGAN\ncd iGAN\n```\n* Download the model. (See `Model Zoo` for details):\n``` bash\nbash ./models/scripts/download_dcgan_model.sh outdoor_64\n```\n\n* Run the python script:\n``` bash\nTHEANO_FLAGS=\'device=gpu0, floatX=float32, nvcc.fastmath=True\' python iGAN_main.py --model_name outdoor_64\n```\n\n## Requirements\nThe code is written in Python2 and requires the following 3rd party libraries:\n* numpy\n* [OpenCV](http://opencv.org/)\n```bash\nsudo apt-get install python-opencv\n```\n* [Theano](https://github.com/Theano/Theano)\n```bash\nsudo pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n```\n* [PyQt4](https://wiki.python.org/moin/PyQt4): more details on Qt installation can be found [here](http://www.saltycrane.com/blog/2008/01/how-to-install-pyqt4-on-ubuntu-linux/)\n```bash\nsudo apt-get install python-qt4\n```\n* [Qdarkstyle](https://github.com/ColinDuquesnoy/QDarkStyleSheet)\n```bash\nsudo pip install qdarkstyle\n```\n* [dominate](https://github.com/Knio/dominate)\n```bash\nsudo pip install dominate\n```\n* GPU + CUDA + cuDNN:\nThe code is tested on GTX Titan X + CUDA 7.5 + cuDNN 5.  Here are the tutorials on how to install [CUDA](http://www.r-tutor.com/gpu-computing/cuda-installation/cuda7.5-ubuntu) and [cuDNN](http://askubuntu.com/questions/767269/how-can-i-install-cudnn-on-ubuntu-16-04). A decent GPU is required to run the system in real-time. [**Warning**] If you run the program on a GPU server, you need to use remote desktop software (e.g., VNC), which may introduce display artifacts and latency problem.\n\n## Python3\nFor `Python3` users, you need to replace `pip` with `pip3`:\n* PyQt4 with Python3:\n``` bash\nsudo apt-get install python3-pyqt4\n```\n* OpenCV3 with Python3: see the installation [instruction](http://www.pyimagesearch.com/2015/07/20/install-opencv-3-0-and-python-3-4-on-ubuntu/).\n\n\n## Interface:\nSee [[Youtube]](https://youtu.be/9c4z6YsBGQ0?t=2m18s) at 2:18s for the interactive image generation demos.  \n\n<img src=\'pics/ui_intro.jpg\' width=800>  \n\n#### Layout\n* Drawing Pad: This is the main window of our interface. A user can apply different edits via our brush tools, and the system will display the generated image. Check/Uncheck `Edits` button to display/hide user edits.  \n* Candidate Results: a display showing thumbnails of all the candidate results (e.g., different modes) that fits the user edits. A user can click a mode (highlighted by a green rectangle), and the drawing pad will show this result.\n* Brush Tools:  `Coloring Brush` for changing the color of a specific region; `Sketching brush` for outlining the shape. `Warping brush` for modifying the shape more explicitly.\n* Slider Bar: drag the slider bar to explore the interpolation sequence between the initial result (i.e., randomly generated image) and the current result (e.g., image that satisfies the user edits).\n* Control Panel: `Play`: play the interpolation sequence; `Fix`: use the current result as additional constraints for further editing  `Restart`: restart the system; `Save`: save the result to a webpage. `Edits`: Check the box if you would like to show the edits on top of the generated image.\n\n\n#### User interaction\n* `Coloring Brush`:  right-click to select a color; hold left click to paint; scroll the mouse wheel to adjust the width of the brush.\n* `Sketching Brush`: hold left-click to sketch the shape.\n* `Warping Brush`: We recommend you first use coloring and sketching before the warping brush. Right-click to select a square region; hold left click to drag the region; scroll the mouse wheel to adjust the size of the square region.\n* Shortcuts: P for `Play`, F for `Fix`, R for `Restart`; S for `Save`; E for `Edits`; Q for quitting the program.\n* Tooltips: when you move the cursor over a button, the system will display the tooltip of the button.\n\n\n## Model Zoo:\nDownload the Theano DCGAN model (e.g., outdoor_64). Before using our system, please check out the random real images vs. DCGAN generated samples to see which kind of images that a model can produce.\n\n``` bash\nbash ./models/scripts/download_dcgan_model.sh outdoor_64\n```\n* [ourdoor_64.dcgan_theano](http://efrosgans.eecs.berkeley.edu/iGAN/models/theano_dcgan/outdoor_64.dcgan_theano) (64x64): trained on 150K landscape images from MIT [Places](http://places.csail.mit.edu/) dataset [[Real](http://efrosgans.eecs.berkeley.edu/iGAN/samples/outdoor_64_real.png) vs. [DCGAN](http://efrosgans.eecs.berkeley.edu/iGAN/samples/outdoor_64_dcgan.png)].\n* [church_64.dcgan_theano](http://efrosgans.eecs.berkeley.edu/iGAN/models/theano_dcgan/church_64.dcgan_theano) (64x64): trained on 126k church images from the [LSUN](http://lsun.cs.princeton.edu/2016/) challenge [[Real](http://efrosgans.eecs.berkeley.edu/iGAN/samples/church_64_real.png) vs. [DCGAN](http://efrosgans.eecs.berkeley.edu/iGAN/samples/church_64_dcgan.png)].\n* [handbag_64.dcgan_theano](http://efrosgans.eecs.berkeley.edu/iGAN/models/theano_dcgan/handbag_64.dcgan_theano) (64x64): trained on 137K handbag images downloaded from Amazon [[Real](http://efrosgans.eecs.berkeley.edu/iGAN/samples/handbag_64_real.png) vs. [DCGAN](http://efrosgans.eecs.berkeley.edu/iGAN/samples/handbag_64_dcgan.png)].\n* [shoes_64.dcgan_theano](http://efrosgans.eecs.berkeley.edu/iGAN/models/theano_dcgan/shoes_64.dcgan_theano) (64x64): trained on 50K shoes images collected by [Yu and Grauman](http://vision.cs.utexas.edu/projects/finegrained/utzap50k/) [[Real](http://efrosgans.eecs.berkeley.edu/iGAN/samples/shoes_64_real.png) vs. [DCGAN](http://efrosgans.eecs.berkeley.edu/iGAN/samples/shoes_64_dcgan.png)].\n* [hed_shoes_64.dcgan_theano](http://efrosgans.eecs.berkeley.edu/iGAN/models/theano_dcgan/hed_shoes_64.dcgan_theano) (64x64): trained on 50K shoes sketches (computed by [HED](https://github.com/s9xie/hed)) [[Real](http://efrosgans.eecs.berkeley.edu/iGAN/samples/hed_shoes_64_real.png) vs. [DCGAN](http://efrosgans.eecs.berkeley.edu/iGAN/samples/hed_shoes_64_dcgan.png)]. (Use this model with `--shadow` flag)\n\nWe provide a simple script to generate samples from a pre-trained DCGAN model. You can run this script to test if Theano, CUDA, cuDNN are configured properly before running our interface.\n```bash\nTHEANO_FLAGS=\'device=gpu0, floatX=float32, nvcc.fastmath=True\' python generate_samples.py --model_name outdoor_64 --output_image outdoor_64_dcgan.png\n```\n\n\n## Command line arguments:\nType `python iGAN_main.py --help` for a complete list of the arguments. Here we discuss some important arguments:\n* `--model_name`: the name of the model (e.g., outdoor_64, shoes_64, etc.)\n* `--model_type`: currently only supports dcgan_theano.\n* `--model_file`: the file that stores the generative model; If not specified, `model_file=\'./models/%s.%s\' % (model_name, model_type)`\n* `--top_k`: the number of the candidate results being displayed\n* `--average`: show an average image in the main window. Inspired by [AverageExplorer](https://people.eecs.berkeley.edu/~junyanz/projects/averageExplorer/), average image is a weighted average of multiple generated results, with the weights reflecting user-indicated importance. You can switch between average mode and normal mode by press `A`.\n* `--shadow`: We build a sketching assistance system for guiding the freeform drawing of objects inspired by [ShadowDraw](http://vision.cs.utexas.edu/projects/shadowdraw/shadowdraw.html)\nTo use the interface, download the model `hed_shoes_64` and run the following script\n```bash\nTHEANO_FLAGS=\'device=gpu0, floatX=float32, nvcc.fastmath=True\' python iGAN_main.py --model_name hed_shoes_64 --shadow --average\n```\n\n## Dataset and Training\nSee more details [here](./train_dcgan/README.md)\n\n## Projecting an Image onto Latent Space\n<img src=\'pics/predict.jpg\' width=800>\n\nWe provide a script to project an image into latent space (i.e., `x->z`):\n* Download the pre-trained AlexNet model (`conv4`):\n```bash\nbash models/scripts/download_alexnet.sh conv4\n```\n* Run the following script with a model and an input image. (e.g., model: `shoes_64.dcgan_theano`, and input image `./pics/shoes_test.png`)\n```bash\nTHEANO_FLAGS=\'device=gpu0, floatX=float32, nvcc.fastmath=True\' python iGAN_predict.py --model_name shoes_64 --input_image ./pics/shoes_test.png --solver cnn_opt\n```\n* Check the result saved in `./pics/shoes_test_cnn_opt.png`\n* We provide three methods: `opt` for optimization method; `cnn` for feed-forward network method (fastest); `cnn_opt` hybrid of the previous methods (default and best). Type `python iGAN_predict.py --help` for a complete list of the arguments.\n\n## Script without UI\n<img src=\'pics/script_result.png\' width=1000>\n\nWe also provide a standalone script that should work without UI. Given user constraints (i.e., a color map, a color mask, and an edge map), the script generates multiple images that mostly satisfy the user constraints. See `python iGAN_script.py --help` for more details.\n```bash\nTHEANO_FLAGS=\'device=gpu0, floatX=float32, nvcc.fastmath=True\' python iGAN_script.py --model_name outdoor_64\n```\n\n\n## Citation\n```\n@inproceedings{zhu2016generative,\n  title={Generative Visual Manipulation on the Natural Image Manifold},\n  author={Zhu, Jun-Yan and Kr{\\"a}henb{\\"u}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},\n  booktitle={Proceedings of European Conference on Computer Vision (ECCV)},\n  year={2016}\n}\n```\n\n## Cat Paper Collection\nIf you love cats, and love reading cool graphics, vision, and learning papers, please check out our Cat Paper Collection:  \n[[Github]](https://github.com/junyanz/CatPapers) [[Webpage]](http://people.eecs.berkeley.edu/~junyanz/cat/cat_papers.html)\n\n## Acknowledgement\n* We modified the DCGAN [code](https://github.com/Newmu/dcgan_code) in our package. Please cite the original [DCGAN](https://arxiv.org/abs/1511.06434) paper if you use their models.\n* This work was supported, in part, by funding from Adobe, eBay, and Intel, as well as a hardware grant from NVIDIA. J.-Y. Zhu is supported by Facebook Graduate Fellowship.\n'