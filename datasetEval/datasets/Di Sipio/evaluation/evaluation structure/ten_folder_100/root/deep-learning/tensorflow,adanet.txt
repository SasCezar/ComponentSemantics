b'# AdaNet\n\n<div align="center">\n  <img src="https://tensorflow.github.io/adanet/images/adanet_tangram_logo.png" alt="adanet_tangram_logo"><br><br>\n</div>\n\n[![Documentation Status](https://readthedocs.org/projects/adanet/badge)](https://adanet.readthedocs.io)\n[![PyPI version](https://badge.fury.io/py/adanet.svg)](https://badge.fury.io/py/adanet)\n[![Travis](https://travis-ci.org/tensorflow/adanet.svg?branch=master)](https://travis-ci.org/tensorflow/adanet)\n[![codecov](https://codecov.io/gh/tensorflow/adanet/branch/master/graph/badge.svg)](https://codecov.io/gh/tensorflow/adanet)\n[![Gitter](https://badges.gitter.im/tensorflow/adanet.svg)](https://gitter.im/tensorflow/adanet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![Downloads](https://pepy.tech/badge/adanet)](https://pepy.tech/project/adanet)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/tensorflow/adanet/blob/master/LICENSE)\n\n**AdaNet** is a lightweight TensorFlow-based framework for automatically learning high-quality models with minimal expert intervention. AdaNet builds on recent AutoML efforts to be fast and flexible while providing learning guarantees. Importantly, AdaNet provides a general framework for not only learning a neural network architecture, but also for learning to ensemble to obtain even better models.\n\nThis project is based on the _AdaNet algorithm_, presented in \xe2\x80\x9c[AdaNet: Adaptive Structural Learning of Artificial Neural Networks](http://proceedings.mlr.press/v70/cortes17a.html)\xe2\x80\x9d at [ICML 2017](https://icml.cc/Conferences/2017), for learning the structure of a neural network as an ensemble of subnetworks.\n\nAdaNet has the following goals:\n\n* _Ease of use_: Provide familiar APIs (e.g. Keras, Estimator) for training, evaluating, and serving models.\n* _Speed_: Scale with available compute and quickly produce high quality models.\n* _Flexibility_: Allow researchers and practitioners to extend AdaNet to novel subnetwork architectures, search spaces, and tasks.\n* _Learning guarantees_: Optimize an objective that offers theoretical learning guarantees.\n\nThe following animation shows AdaNet adaptively growing an ensemble of neural networks. At each iteration, it measures the ensemble loss for each candidate, and selects the best one to move onto the next iteration. At subsequent iterations, the blue subnetworks are frozen, and only yellow subnetworks are trained:\n\n<div align="center" style="max-width: 450px; display: block; margin: 0 auto;">\n  <img src="https://tensorflow.github.io/adanet/images/adanet_animation.gif" alt="adanet_tangram_logo"><br><br>\n</div>\n\nAdaNet was first announced on the Google AI research blog: "[Introducing AdaNet: Fast and Flexible AutoML with Learning Guarantees](https://ai.googleblog.com/2018/10/introducing-adanet-fast-and-flexible.html)".\n\nThis is not an official Google product.\n\n## Features\n\nAdaNet provides the following AutoML features:\n\n * Adaptive neural architecture search and ensemble learning in a single train call.\n * Regression, binary and multi-class classification, and multi-head task support.\n * A [`tf.estimator.Estimator`](https://www.tensorflow.org/guide/estimators) API for training, evaluation, prediction, and serving models.\n * The [`adanet.AutoEnsembleEstimator`](https://github.com/tensorflow/adanet/blob/master/adanet/autoensemble/estimator.py) for learning to ensemble user-defined `tf.estimator.Estimators`.\n * The ability to define subnetworks that change structure over time using [`tf.layers`](https://www.tensorflow.org/api_docs/python/tf/layers) via the [`adanet.subnetwork` API](https://github.com/tensorflow/adanet/blob/master/adanet/subnetwork/generator.py).\n * CPU, GPU, and TPU support.\n * [Distributed multi-server training](https://cloud.google.com/blog/products/gcp/easy-distributed-training-with-tensorflow-using-tfestimatortrain-and-evaluate-on-cloud-ml-engine).\n * TensorBoard integration.\n\n## Example\n\nA simple example of learning to ensemble linear and neural network models:\n\n```python\nimport adanet\nimport tensorflow as tf\n\n# Define the model head for computing loss and evaluation metrics.\nhead = MultiClassHead(n_classes=10)\n\n# Feature columns define how to process examples.\nfeature_columns = ...\n\n# Learn to ensemble linear and neural network models.\nestimator = adanet.AutoEnsembleEstimator(\n    head=head,\n    candidate_pool={\n        "linear":\n            tf.estimator.LinearEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=...),\n        "dnn":\n            tf.estimator.DNNEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=...,\n                hidden_units=[1000, 500, 100])},\n    max_iteration_steps=50)\n\nestimator.train(input_fn=train_input_fn, steps=100)\nmetrics = estimator.evaluate(input_fn=eval_input_fn)\npredictions = estimator.predict(input_fn=predict_input_fn)\n```\n\n## Getting Started\n\nTo get you started:\n\n- [API Documentation](https://adanet.readthedocs.io)\n- [Tutorials: for understanding the AdaNet algorithm and learning to use this package](./adanet/examples/tutorials)\n\n## Requirements\n\nRequires [Python](https://www.python.org/) 2.7, 3.4, 3.5, 3.6, or 3.7.\n\n`adanet` supports both TensorFlow 2.0 and TensorFlow >=1.15. It depends on bug fixes and enhancements not present in TensorFlow releases prior to 1.15. You must install or upgrade your TensorFlow package to at least 1.15:\n\n```shell\n$ pip install "tensorflow==2.0.0" # Or pip install "tensorflow==1.15.*"\n```\n\n## Installing with Pip\n\nYou can use the [pip package manager](https://pip.pypa.io/en/stable/installing/) to install the official `adanet` package from [PyPi](https://pypi.org/project/adanet/):\n\n```shell\n$ pip install adanet\n```\n\n## Installing from Source\n\nTo install from source first you\'ll need to install `bazel` following their [installation instructions](https://docs.bazel.build/versions/master/install.html).\n\nNext clone the `adanet` repository:\n\n```shell\n$ git clone https://github.com/tensorflow/adanet\n$ cd adanet\n```\n\nFrom the `adanet` root directory run the tests:\n\n```shell\n$ bazel build -c opt //...\n# Run tests with nosetests, but skip example tests.\n$ NOSE_EXCLUDE=\'.*nasnet.*.py.*\' python3 -m nose\n```\n\nOnce you have verified that the tests have passed, install `adanet` from source as a [ pip package ](./adanet/pip_package/PIP.md).\n\nYou are now ready to experiment with `adanet`.\n\n```python\nimport adanet\n```\n\n## Citing this Work\n\nIf you use this AdaNet library for academic research, you are encouraged to cite the following paper from the [ICML 2019 AutoML Workshop](https://arxiv.org/abs/1905.00080):\n\n    @misc{weill2019adanet,\n        title={AdaNet: A Scalable and Flexible Framework for Automatically Learning Ensembles},\n        author={Charles Weill and Javier Gonzalvo and Vitaly Kuznetsov and Scott Yang and Scott Yak and Hanna Mazzawi and Eugen Hotaj and Ghassen Jerfel and Vladimir Macko and Ben Adlam and Mehryar Mohri and Corinna Cortes},\n        year={2019},\n        eprint={1905.00080},\n        archivePrefix={arXiv},\n        primaryClass={cs.LG}\n    }\n\n## License\n\nAdaNet is released under the [Apache License 2.0](LICENSE).\n'