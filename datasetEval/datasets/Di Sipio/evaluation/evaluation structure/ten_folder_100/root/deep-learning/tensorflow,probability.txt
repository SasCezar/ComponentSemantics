b'# TensorFlow Probability\n\nTensorFlow Probability is a library for probabilistic reasoning and statistical\nanalysis in TensorFlow. As part of the TensorFlow ecosystem, TensorFlow\nProbability provides integration of probabilistic methods with deep networks,\ngradient-based inference via automatic differentiation, and scalability to\nlarge datasets and models via hardware acceleration (e.g., GPUs) and distributed\ncomputation.\n\nOur probabilistic machine learning tools are structured as follows.\n\n__Layer 0: TensorFlow.__ Numerical operations. In particular, the LinearOperator\nclass enables matrix-free implementations that can exploit special structure\n(diagonal, low-rank, etc.) for efficient computation. It is built and maintained\nby the TensorFlow Probability team and is now part of\n[`tf.linalg`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops/linalg)\nin core TF.\n\n__Layer 1: Statistical Building Blocks__\n\n* Distributions ([`tfp.distributions`](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/distributions)):\n  A large collection of probability\n  distributions and related statistics with batch and\n  [broadcasting](https://docs.scipy.org/doc/numpy-1.14.0/user/basics.broadcasting.html)\n  semantics. See the\n  [Distributions Tutorial](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Distributions_Tutorial.ipynb).\n* Bijectors ([`tfp.bijectors`](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/bijectors)):\n  Reversible and composable transformations of random variables. Bijectors\n  provide a rich class of transformed distributions, from classical examples\n  like the\n  [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution)\n  to sophisticated deep learning models such as\n  [masked autoregressive flows](https://arxiv.org/abs/1705.07057).\n\n__Layer 2: Model Building__\n\n* Edward2 ([`tfp.edward2`](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/experimental/edward2)):\n  A probabilistic programming language for specifying flexible probabilistic\n  models as programs. See the\n  [Edward2 `README.md`](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/experimental/edward2/README.md).\n* Probabilistic Layers ([`tfp.layers`](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/layers)):\n  Neural network layers with uncertainty over the functions they represent,\n  extending TensorFlow Layers.\n\n__Layer 3: Probabilistic Inference__\n\n* Markov chain Monte Carlo ([`tfp.mcmc`](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/mcmc)):\n  Algorithms for approximating integrals via sampling. Includes\n  [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo),\n  random-walk Metropolis-Hastings, and the ability to build custom transition\n  kernels.\n* Variational Inference ([`tfp.vi`](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/vi)):\n  Algorithms for approximating integrals via optimization.\n* Optimizers ([`tfp.optimizer`](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/optimizer)):\n  Stochastic optimization methods, extending TensorFlow Optimizers. Includes\n  [Stochastic Gradient Langevin Dynamics](http://www.icml-2011.org/papers/398_icmlpaper.pdf).\n* Monte Carlo ([`tfp.monte_carlo`](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/monte_carlo)):\n  Tools for computing Monte Carlo expectations.\n\nTensorFlow Probability is under active development. Interfaces may change at any\ntime.\n\n## Examples\n\nSee [`tensorflow_probability/examples/`](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/)\nfor end-to-end examples. It includes tutorial notebooks such as:\n\n* [Linear Mixed Effects Models](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Linear_Mixed_Effects_Models.ipynb).\n  A hierarchical linear model for sharing statistical strength across examples.\n* [Eight Schools](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Eight_Schools.ipynb).\n  A hierarchical normal model for exchangeable treatment effects.\n* [Hierarchical Linear Models](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/HLM_TFP_R_Stan.ipynb).\n  Hierarchical linear models compared among TensorFlow Probability, R, and Stan.\n* [Bayesian Gaussian Mixture Models](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb).\n  Clustering with a probabilistic generative model.\n* [Probabilistic Principal Components Analysis](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_PCA.ipynb).\n  Dimensionality reduction with latent variables.\n* [Gaussian Copulas](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Gaussian_Copula.ipynb).\n  Probability distributions for capturing dependence across random variables.\n* [TensorFlow Distributions: A Gentle Introduction](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Distributions_Tutorial.ipynb).\n  Introduction to TensorFlow Distributions.\n* [Understanding TensorFlow Distributions Shapes](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Understanding_TensorFlow_Distributions_Shapes.ipynb).\n  How to distinguish between samples, batches, and events for arbitrarily shaped\n  probabilistic computations.\n* [TensorFlow Probability Case Study: Covariance Estimation](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Probability_Case_Study_Covariance_Estimation.ipynb).\n  A user\'s case study in applying TensorFlow Probability to estimate covariances.\n\nIt also includes example scripts such as:\n\n* [Variational Autoencoders](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/vae.py).\n  Representation learning with a latent code and variational inference.\n* [Vector-Quantized Autoencoder](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/vq_vae.py).\n  Discrete representation learning with vector quantization.\n* [Disentangled Sequential Variational Autoencoder](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/disentangled_vae.py)\n  Disentangled representation learning over sequences with variational inference.\n* [Grammar Variational Autoencoder](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/grammar_vae.py).\n  Representation learning over productions in a context-free grammar.\n* Latent Dirichlet Allocation\n  ([Distributions version](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/latent_dirichlet_allocation_distributions.py),\n  [Edward2 version](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/latent_dirichlet_allocation_edward2.py)).\n  Mixed membership modeling for capturing topics in a document.\n+ [Deep Exponential Family](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/deep_exponential_family.py).\n  A deep, sparse generative model for discovering a hierarchy of topics.\n* [Bayesian Neural Networks](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/bayesian_neural_network.py).\n  Neural networks with uncertainty over their weights.\n* [Bayesian Logistic Regression](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/logistic_regression.py).\n  Bayesian inference for binary classification.\n\n## Installation\n\n### Stable Builds\n\nTo install the latest version, run the following:\n\n```shell\n# Notes:\n# - We recommend that users move towards using TensorFlow 2.x as soon as\n#   possible. Until the TF2 stable package is released (due in Sep. 2019),\n#   the best way to use TFP with TF2 is to use nightly\xc2\xa0TFP and TF2 packages:\n#     - Nightly TFP: [tfp-nightly](http://pypi.python.org/pypi/tfp-nightly)\n#     - Nightly TF2: [tf-nightly-2.0-preview](http://pypi.python.org/pypi/tf-nightly-2.0-preview)\n#   Once the TF2 stable release comes out, TFP will issue its 0.8.0 release,\n#   which will be tested and stable against TF 2.0.0.\n# - You need the latest version of `pip` in order to get the latest version of\n#   `tf-nightly-2.0-preview`.\n# - For GPU TF, use `tf-nightly-2.0-preview-gpu`.\n# - The `--upgrade` flag ensures you\'ll get the latest version.\n# - The `--user` flag ensures the packages are installed to your user directory\n#   rather than the system directory.\npython -m pip install pip --upgrade --user\npython -m pip install tf-nightly-2.0-preview tfp-nightly --upgrade --user\nTFVERSION=$(python -c \'import tensorflow; print(tensorflow.__version__)\')\n# If you have an older pip, you might get this older version of\n# tf-nightly-2.0-preview, so check to be sure.\n[[ $TFVERSION == \'2.0.0-dev20190731\' ]] &&\n  echo >&2 "Failed to install the most recent TF. Found: ${TFVERSION}."\n```\n\nTensorFlow Probability depends on a recent stable release of\n[TensorFlow](https://www.tensorflow.org/install) (pip package `tensorflow`). See\nthe [TFP release notes](https://github.com/tensorflow/probability/releases) for\ndetails about dependencies between TensorFlow and TensorFlow Probability.\n\nNote: Since TensorFlow is *not* included as a dependency of the TensorFlow\nProbability package (in `setup.py`), you must explicitly install the TensorFlow\npackage (`tensorflow` or `tensorflow-gpu`). This allows us to maintain one\npackage instead of separate packages for CPU and GPU-enabled TensorFlow.\n\nTo force a Python 3-specific install, replace `pip` with `pip3` in the above\ncommands. For additional installation help, guidance installing prerequisites,\nand (optionally) setting up virtual environments, see the [TensorFlow\ninstallation guide](https://www.tensorflow.org/install).\n\n### Nightly Builds\n\nThere are also nightly builds of TensorFlow Probability under the pip package\n`tfp-nightly`, which depends on one of `tf-nightly`, `tf-nightly-gpu`,\n`tf-nightly-2.0-preview` or `tf-nightly-gpu-2.0-preview`. Nightly builds include\nnewer features, but may be less stable than the versioned releases. Docs are\nperiodically refreshed [here](\nhttps://github.com/tensorflow/probability/blob/master/tensorflow_probability/g3doc/api_docs/python/tfp.md).\n\n### Installing from Source\n\nYou can also install from source. This requires the [Bazel](\nhttps://bazel.build/) build system.\n\n```shell\n# sudo apt-get install bazel git python-pip  # Ubuntu; others, see above links.\ngit clone https://github.com/tensorflow/probability.git\ncd probability\nbazel build --copt=-O3 --copt=-march=native :pip_pkg\nPKGDIR=$(mktemp -d)\n./bazel-bin/pip_pkg $PKGDIR\npip install --user --upgrade $PKGDIR/*.whl\n```\n\n## Community\n\nAs part of TensorFlow, we\'re committed to fostering an open and welcoming\nenvironment.\n\n* [Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow): Ask\n  or answer technical questions.\n* [GitHub](https://github.com/tensorflow/probability/issues): Report bugs or\n  make feature requests.\n* [TensorFlow Blog](https://medium.com/tensorflow): Stay up to date on content\n  from the TensorFlow team and best articles from the community.\n* [Youtube Channel](http://youtube.com/tensorflow/): Follow TensorFlow shows.\n* [tfprobability@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfprobability):\n  Open mailing list for discussion and questions.\n\nSee the [TensorFlow Community](https://www.tensorflow.org/community/) page for\nmore details. Check out our latest publicity here:\n\n+ [Coffee with a Googler: Probabilistic Machine Learning in TensorFlow](\n  https://www.youtube.com/watch?v=BjUkL8DFH5Q)\n+ [Introducing TensorFlow Probability](\n  https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245)\n\n## Contributing\n\nWe\'re eager to collaborate with you! See [`CONTRIBUTING.md`](CONTRIBUTING.md)\nfor a guide on how to contribute. This project adheres to TensorFlow\'s\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.\n\n## References\n\nIf you use TensorFlow Probability in a paper, please cite: \n\n+ _TensorFlow Distributions._ Joshua V. Dillon, Ian Langmore, Dustin Tran,\nEugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt\nHoffman, Rif A. Saurous.\n[arXiv preprint arXiv:1711.10604, 2017](https://arxiv.org/abs/1711.10604).\n\n(We\'re aware there\'s a lot more to TensorFlow Probability than Distributions, but the Distributions paper lays out our vision and is a fine thing to cite for now.)\n'