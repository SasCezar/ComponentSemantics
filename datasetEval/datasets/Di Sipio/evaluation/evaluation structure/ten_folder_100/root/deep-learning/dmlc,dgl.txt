b"# Deep Graph Library (DGL)\n[![Build Status](http://ci.dgl.ai:80/buildStatus/icon?job=DGL/master)](http://ci.dgl.ai:80/job/DGL/job/master/)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](./LICENSE)\n\n[Documentation](https://docs.dgl.ai) | [DGL at a glance](https://docs.dgl.ai/tutorials/basics/1_first.html#sphx-glr-tutorials-basics-1-first-py) |\n[Model Tutorials](https://docs.dgl.ai/tutorials/models/index.html) | [Discussion Forum](https://discuss.dgl.ai)\n\nModel Zoos: [Chemistry](https://github.com/dmlc/dgl/tree/master/examples/pytorch/model_zoo) | [Citation Networks](https://github.com/dmlc/dgl/tree/master/examples/pytorch/model_zoo/citation_network)\n\nDGL is a Python package that interfaces between existing tensor libraries and data being expressed as\ngraphs.\n\nIt makes implementing graph neural networks (including Graph Convolution Networks, TreeLSTM, and many others) easy while\nmaintaining high computation efficiency.\n\nAll model examples can be found [here](https://github.com/dmlc/dgl/tree/master/examples).\n\nA summary of part of the model accuracy and training speed with the Pytorch backend (on Amazon EC2 p3.2x instance (w/ V100 GPU)), as compared with the best open-source implementations:\n\n| Model                                                            | Reported <br> Accuracy | DGL <br> Accuracy | Author's training speed (epoch time)                                          | DGL speed (epoch time) | Improvement |\n| -----                                                            | -----------------      | ------------      | ------------------------------------                                          | ---------------------- | ----------- |\n| [GCN](https://arxiv.org/abs/1609.02907)                          | 81.5%                  | 81.0%             | [0.0051s (TF)](https://github.com/tkipf/gcn)                                  | 0.0031s                | 1.64x       |\n| [GAT](https://arxiv.org/abs/1710.10903)                          | 83.0%                  | 83.9%             | [0.0982s (TF)](https://github.com/PetarV-/GAT)                                | 0.0113s                | 8.69x       |\n| [SGC](https://arxiv.org/abs/1902.07153)                          | 81.0%                  | 81.9%             | n/a                                                                           | 0.0008s                | n/a         |\n| [TreeLSTM](http://arxiv.org/abs/1503.00075)                      | 51.0%                  | 51.72%            | [14.02s (DyNet)](https://github.com/clab/dynet/tree/master/examples/treelstm) | 3.18s                  | 4.3x        |\n| [R-GCN <br> (classification)](https://arxiv.org/abs/1703.06103)  | 73.23%                 | 73.53%            | [0.2853s (Theano)](https://github.com/tkipf/relational-gcn)                   | 0.0075s                | 38.2x       |\n| [R-GCN <br> (link prediction)](https://arxiv.org/abs/1703.06103) | 0.158                  | 0.151             | [2.204s (TF)](https://github.com/MichSchli/RelationPrediction)                | 0.453s                 | 4.86x       |\n| [JTNN](https://arxiv.org/abs/1802.04364)                         | 96.44%                 | 96.44%            | [1826s (Pytorch)](https://github.com/wengong-jin/icml18-jtnn)                 | 743s                   | 2.5x        |\n| [LGNN](https://arxiv.org/abs/1705.08415)                         | 94%                    | 94%               | n/a                                                                           | 1.45s                  | n/a         |\n| [DGMG](https://arxiv.org/pdf/1803.03324.pdf)                     | 84%                    | 90%               | n/a                                                                           | 238s                   | n/a         |\n\nWith the MXNet/Gluon backend , we scaled a graph of 50M nodes and 150M edges on a P3.8xlarge instance, \nwith 160s per epoch, on SSE ([Stochastic Steady-state Embedding](https://www.cc.gatech.edu/~hdai8/pdf/equilibrium_embedding.pdf)), \na model similar to GCN. \n\n\nWe are currently in Beta stage.  More features and improvements are coming.\n\n## News\n\nv0.4 has just been released! DGL now support **heterogeneous graphs**, and comes\nwith a subpackage **DGL-KE** that computes embeddings for large knowledge graphs\nsuch as Freebase (1.9 billion triplets).\nSee release note [here](https://github.com/dmlc/dgl/releases/tag/v0.4).\n\nWe presented DGL at [GTC 2019](https://www.nvidia.com/en-us/gtc/) as an\ninstructor-led training session. Check out our slides and tutorial materials\n[here](https://github.com/dglai/DGL-GTC2019)!!!\n\n## System requirements\n\nDGL should work on\n\n* all Linux distributions no earlier than Ubuntu 16.04\n* macOS X\n* Windows 10\n\nDGL also requires Python 3.5 or later.  Python 2 support is coming.\n\nRight now, DGL works on [PyTorch](https://pytorch.org) 0.4.1+ and [MXNet](https://mxnet.apache.org) nightly\nbuild.\n\n## Installation\n\n### Using anaconda\n\n```\nconda install -c dglteam dgl           # cpu version\nconda install -c dglteam dgl-cuda9.0   # CUDA 9.0\nconda install -c dglteam dgl-cuda9.2   # CUDA 9.2\nconda install -c dglteam dgl-cuda10.0  # CUDA 10.0\nconda install -c dglteam dgl-cuda10.1  # CUDA 10.1\n```\n\n### Using pip\n\n```\npip install dgl       # cpu version\npip install dgl-cu90  # CUDA 9.0\npip install dgl-cu92  # CUDA 9.2\npip install dgl-cu100 # CUDA 10.0\npip install dgl-cu101 # CUDA 10.1\n```\n\n### From source\n\nRefer to the guide [here](https://docs.dgl.ai/install/index.html#install-from-source).\n\n## How DGL looks like\n\nA graph can be constructed with feature tensors like this:\n\n```python\nimport dgl\nimport torch as th\n\ng = dgl.DGLGraph()\ng.add_nodes(5)                          # add 5 nodes\ng.add_edges([0, 0, 0, 0], [1, 2, 3, 4]) # add 4 edges 0->1, 0->2, 0->3, 0->4\ng.ndata['h'] = th.randn(5, 3)           # assign one 3D vector to each node\ng.edata['h'] = th.randn(4, 4)           # assign one 4D vector to each edge\n```\n\nThis is *everything* to implement a single layer for Graph Convolutional Network on PyTorch:\n\n```python\nimport dgl.function as fn\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom dgl import DGLGraph\n\nmsg_func = fn.copy_src(src='h', out='m')\nreduce_func = fn.sum(msg='m', out='h')\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_feats, out_feats):\n        super(GCNLayer, self).__init__()\n        self.linear = nn.Linear(in_feats, out_feats)\n\n    def apply(self, nodes):\n        return {'h': F.relu(self.linear(nodes.data['h']))}\n\n    def forward(self, g, feature):\n        g.ndata['h'] = feature\n        g.update_all(msg_func, reduce_func)\n        g.apply_nodes(func=self.apply)\n        return g.ndata.pop('h')\n```\n\nOne can also customize how message and reduce function works.  The following code\ndemonstrates a (simplified version of) Graph Attention Network (GAT) layer:\n\n```python\ndef msg_func(edges):\n    return {'k': edges.src['k'], 'v': edges.src['v']}\n\ndef reduce_func(nodes):\n    # nodes.data['q'] has the shape\n    #     (number_of_nodes, feature_dims)\n    # nodes.data['k'] and nodes.data['v'] have the shape\n    #     (number_of_nodes, number_of_incoming_messages, feature_dims)\n    # You only need to deal with the case where all nodes have the same number\n    # of incoming messages.\n    q = nodes.data['q'][:, None]\n    k = nodes.mailbox['k']\n    v = nodes.mailbox['v']\n    s = F.softmax((q * k).sum(-1), 1)[:, :, None]\n    return {'v': th.sum(s * v, 1)}\n\nclass GATLayer(nn.Module):\n    def __init__(self, in_feats, out_feats):\n        super(GATLayer, self).__init__()\n        self.Q = nn.Linear(in_feats, out_feats)\n        self.K = nn.Linear(in_feats, out_feats)\n        self.V = nn.Linear(in_feats, out_feats)\n\n    def apply(self, nodes):\n        return {'v': F.relu(self.linear(nodes.data['v']))}\n\n    def forward(self, g, feature):\n        g.ndata['v'] = self.V(feature)\n        g.ndata['q'] = self.Q(feature)\n        g.ndata['k'] = self.K(feature)\n        g.update_all(msg_func, reduce_func)\n        g.apply_nodes(func=self.apply)\n        return g.ndata['v']\n```\n\nFor the basics of coding with DGL, please see [DGL basics](https://docs.dgl.ai/tutorials/basics/index.html).\n\nFor more realistic, end-to-end examples, please see [model tutorials](https://docs.dgl.ai/tutorials/models/index.html).\n\n\n## New to Deep Learning?\n\nCheck out the open source book [*Dive into Deep Learning*](http://gluon.ai/).\n\n\n## Contributing\n\nPlease let us know if you encounter a bug or have any suggestions by [filing an issue](https://github.com/dmlc/dgl/issues).\n\nWe welcome all contributions from bug fixes to new features and extensions.\nWe expect all contributions discussed in the issue tracker and going through PRs.  Please refer to our [contribution guide](https://docs.dgl.ai/contribute.html).\n\n## Cite\n\nIf you use DGL in a scientific publication, we would appreciate citations to the following paper:\n```\n@article{wang2019dgl,\n    title={Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs},\n    url={https://arxiv.org/abs/1909.01315},\n    author={{Wang, Minjie and Yu, Lingfan and Zheng, Da and Gan, Quan and Gai, Yu and Ye, Zihao and Li, Mufei and Zhou, Jinjing and Huang, Qi and Ma, Chao and Huang, Ziyue and Guo, Qipeng and Zhang, Hao and Lin, Haibin and Zhao, Junbo and Li, Jinyang and Smola, Alexander J and Zhang, Zheng},\n    journal={ICLR Workshop on Representation Learning on Graphs and Manifolds},\n    year={2019}\n}\n```\n\n## The Team\n\nDGL is developed and maintained by [NYU, NYU Shanghai, AWS Shanghai AI Lab, and AWS MXNet Science Team](https://www.dgl.ai/pages/about.html).\n\n\n## License\n\nDGL uses Apache License 2.0.\n"