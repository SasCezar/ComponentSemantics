b'# Attention is all you need: A Pytorch Implementation\n\nThis is a PyTorch implementation of the Transformer model in "[Attention is All You Need](https://arxiv.org/abs/1706.03762)" (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017). \n\n\nA novel sequence to sequence framework utilizes the **self-attention mechanism**, instead of Convolution operation or Recurrent structure, and achieve the state-of-the-art performance on **WMT 2014 English-to-German translation task**. (2017/06/12)\n\n> The official Tensorflow Implementation can be found in: [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py).\n\n> To learn more about self-attention mechanism, you could read "[A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)".\n\n<p align="center">\n<img src="http://imgur.com/1krF2R6.png" width="250">\n</p>\n\n\nThe project support training and translation with trained model now.\n\nNote that this project is still a work in progress.\n\n\nIf there is any suggestion or error, feel free to fire an issue to let me know. :)\n\n\n# Requirement\n- python 3.4+\n- pytorch 0.4.1+\n- tqdm\n- numpy\n\n\n# Usage\n\n## Some useful tools:\n\nThe example below uses the Moses tokenizer (http://www.statmt.org/moses/) to prepare the data and the moses BLEU script for evaluation.\n\n```bash\nwget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/tokenizer/tokenizer.perl\nwget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.de\nwget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.en\nsed -i "s/$RealBin\\/..\\/share\\/nonbreaking_prefixes//" tokenizer.perl\nwget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl\n```\n\n## WMT\'16 Multimodal Translation: Multi30k (de-en)\n\nAn example of training for the WMT\'16 Multimodal Translation task (http://www.statmt.org/wmt16/multimodal-task.html).\n\n### 0) Download the data.\n\n```bash\nmkdir -p data/multi30k\nwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz &&  tar -xf training.tar.gz -C data/multi30k && rm training.tar.gz\nwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz && tar -xf validation.tar.gz -C data/multi30k && rm validation.tar.gz\nwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt16_task1_test.tar.gz && tar -xf mmt16_task1_test.tar.gz -C data/multi30k && rm mmt16_task1_test.tar.gz\n```\n\n### 1) Preprocess the data.\n```bash\nfor l in en de; do for f in data/multi30k/*.$l; do if [[ "$f" != *"test"* ]]; then sed -i "$ d" $f; fi;  done; done\nfor l in en de; do for f in data/multi30k/*.$l; do perl tokenizer.perl -a -no-escape -l $l -q  < $f > $f.atok; done; done\npython preprocess.py -train_src data/multi30k/train.en.atok -train_tgt data/multi30k/train.de.atok -valid_src data/multi30k/val.en.atok -valid_tgt data/multi30k/val.de.atok -save_data data/multi30k.atok.low.pt\n```\n\n### 2) Train the model\n```bash\npython train.py -data data/multi30k.atok.low.pt -save_model trained -save_mode best -proj_share_weight -label_smoothing\n```\n> If your source and target language share one common vocabulary, use the `-embs_share_weight` flag to enable the model to share source/target word embedding. \n\n### 3) Test the model\n```bash\npython translate.py -model trained.chkpt -vocab data/multi30k.atok.low.pt -src data/multi30k/test.en.atok -no_cuda\n```\n---\n# Performance\n## Training\n\n<p align="center">\n<img src="https://imgur.com/rKeP1bb.png" width="400">\n<img src="https://imgur.com/9je3X6U.png" width="400">\n</p>\n\n- Parameter settings:\n  - default parameter and optimizer settings\n  - label smoothing \n  - target embedding / pre-softmax linear layer weight sharing. \n\n- Elapse per epoch (on NVIDIA Titan X):\n  - Training set: 0.888 minutes\n  - Validation set: 0.011 minutes\n  \n## Testing \n- coming soon.\n---\n# TODO\n  - Evaluation on the generated text.\n  - Attention weight plot.\n---\n# Acknowledgement\n- The project structure, some scripts and the dataset preprocessing steps are heavily borrowed from [OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py).\n- Thanks for the suggestions from @srush, @iamalbert and @ZiJianZhao.\n'