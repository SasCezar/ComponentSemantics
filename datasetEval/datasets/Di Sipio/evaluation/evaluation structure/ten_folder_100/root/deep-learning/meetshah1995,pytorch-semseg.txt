b'# pytorch-semseg\n\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://github.com/meetshah1995/pytorch-semseg/blob/master/LICENSE)\n[![pypi](https://img.shields.io/pypi/v/pytorch_semseg.svg)](https://pypi.python.org/pypi/pytorch-semseg/0.1.2)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1185075.svg)](https://doi.org/10.5281/zenodo.1185075)\n\n\n\n## Semantic Segmentation Algorithms Implemented in PyTorch\n\nThis repository aims at mirroring popular semantic segmentation architectures in PyTorch. \n\n\n<p align="center">\n<a href="https://www.youtube.com/watch?v=iXh9aCK3ubs" target="_blank"><img src="https://i.imgur.com/agvJOPF.gif" width="364"/></a>\n<img src="https://meetshah1995.github.io/images/blog/ss/ptsemseg.png" width="49%"/>\n</p>\n\n\n### Networks implemented\n\n* [PSPNet](https://arxiv.org/abs/1612.01105) - With support for loading pretrained models w/o caffe dependency\n* [ICNet](https://arxiv.org/pdf/1704.08545.pdf) - With optional batchnorm and pretrained models\n* [FRRN](https://arxiv.org/abs/1611.08323) - Model A and B\n* [FCN](https://arxiv.org/abs/1411.4038) - All 1 (FCN32s), 2 (FCN16s) and 3 (FCN8s) stream variants\n* [U-Net](https://arxiv.org/abs/1505.04597) - With optional deconvolution and batchnorm\n* [Link-Net](https://codeac29.github.io/projects/linknet/) - With multiple resnet backends\n* [Segnet](https://arxiv.org/abs/1511.00561) - With Unpooling using Maxpool indices\n\n\n#### Upcoming \n\n* [E-Net](https://arxiv.org/abs/1606.02147)\n* [RefineNet](https://arxiv.org/abs/1611.06612)\n\n### DataLoaders implemented\n\n* [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)\n* [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html)\n* [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/)\n* [MIT Scene Parsing Benchmark](http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip)\n* [Cityscapes](https://www.cityscapes-dataset.com/)\n* [NYUDv2](http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)\n* [Sun-RGBD](http://rgbd.cs.princeton.edu/)\n\n\n### Requirements\n\n* pytorch >=0.4.0\n* torchvision ==0.2.0\n* scipy\n* tqdm\n* tensorboardX\n\n#### One-line installation\n    \n`pip install -r requirements.txt`\n\n### Data\n\n* Download data for desired dataset(s) from list of URLs [here](https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html#sec_datasets).\n* Extract the zip / tar and modify the path appropriately in your `config.yaml`\n\n\n### Usage\n\n**Setup config file**\n\n```yaml\n# Model Configuration\nmodel:\n    arch: <name> [options: \'fcn[8,16,32]s, unet, segnet, pspnet, icnet, icnetBN, linknet, frrn[A,B]\'\n    <model_keyarg_1>:<value>\n\n# Data Configuration\ndata:\n    dataset: <name> [options: \'pascal, camvid, ade20k, mit_sceneparsing_benchmark, cityscapes, nyuv2, sunrgbd, vistas\'] \n    train_split: <split_to_train_on>\n    val_split: <spit_to_validate_on>\n    img_rows: 512\n    img_cols: 1024\n    path: <path/to/data>\n    <dataset_keyarg1>:<value>\n\n# Training Configuration\ntraining:\n    n_workers: 64\n    train_iters: 35000\n    batch_size: 16\n    val_interval: 500\n    print_interval: 25\n    loss:\n        name: <loss_type> [options: \'cross_entropy, bootstrapped_cross_entropy, multi_scale_crossentropy\']\n        <loss_keyarg1>:<value>\n\n    # Optmizer Configuration\n    optimizer:\n        name: <optimizer_name> [options: \'sgd, adam, adamax, asgd, adadelta, adagrad, rmsprop\']\n        lr: 1.0e-3\n        <optimizer_keyarg1>:<value>\n\n        # Warmup LR Configuration\n        warmup_iters: <iters for lr warmup>\n        mode: <\'constant\' or \'linear\' for warmup\'>\n        gamma: <gamma for warm up>\n       \n    # Augmentations Configuration\n    augmentations:\n        gamma: x                                     #[gamma varied in 1 to 1+x]\n        hue: x                                       #[hue varied in -x to x]\n        brightness: x                                #[brightness varied in 1-x to 1+x]\n        saturation: x                                #[saturation varied in 1-x to 1+x]\n        contrast: x                                  #[contrast varied in 1-x to 1+x]\n        rcrop: [h, w]                                #[crop of size (h,w)]\n        translate: [dh, dw]                          #[reflective translation by (dh, dw)]\n        rotate: d                                    #[rotate -d to d degrees]\n        scale: [h,w]                                 #[scale to size (h,w)]\n        ccrop: [h,w]                                 #[center crop of (h,w)]\n        hflip: p                                     #[flip horizontally with chance p]\n        vflip: p                                     #[flip vertically with chance p]\n\n    # LR Schedule Configuration\n    lr_schedule:\n        name: <schedule_type> [options: \'constant_lr, poly_lr, multi_step, cosine_annealing, exp_lr\']\n        <scheduler_keyarg1>:<value>\n\n    # Resume from checkpoint  \n    resume: <path_to_checkpoint>\n```\n\n**To train the model :**\n\n```\npython train.py [-h] [--config [CONFIG]] \n\n--config                Configuration file to use\n```\n\n**To validate the model :**\n\n```\nusage: validate.py [-h] [--config [CONFIG]] [--model_path [MODEL_PATH]]\n                       [--eval_flip] [--measure_time]\n\n  --config              Config file to be used\n  --model_path          Path to the saved model\n  --eval_flip           Enable evaluation with flipped image | True by default\n  --measure_time        Enable evaluation with time (fps) measurement | True\n                        by default\n```\n\n**To test the model w.r.t. a dataset on custom images(s):**\n\n```\npython test.py [-h] [--model_path [MODEL_PATH]] [--dataset [DATASET]]\n               [--dcrf [DCRF]] [--img_path [IMG_PATH]] [--out_path [OUT_PATH]]\n \n  --model_path          Path to the saved model\n  --dataset             Dataset to use [\'pascal, camvid, ade20k etc\']\n  --dcrf                Enable DenseCRF based post-processing\n  --img_path            Path of the input image\n  --out_path            Path of the output segmap\n```\n\n\n**If you find this code useful in your research, please consider citing:**\n\n```\n@article{mshahsemseg,\n    Author = {Meet P Shah},\n    Title = {Semantic Segmentation Architectures Implemented in PyTorch.},\n    Journal = {https://github.com/meetshah1995/pytorch-semseg},\n    Year = {2017}\n}\n```\n\n'