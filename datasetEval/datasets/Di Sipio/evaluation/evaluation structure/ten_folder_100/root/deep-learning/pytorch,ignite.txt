b'Ignite\n======\n\n.. image:: https://travis-ci.org/pytorch/ignite.svg?branch=master\n    :target: https://travis-ci.org/pytorch/ignite\n\n.. image:: https://codecov.io/gh/pytorch/ignite/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/pytorch/ignite\n\n.. image:: https://pepy.tech/badge/pytorch-ignite\n    :target: https://pepy.tech/project/pytorch-ignite\n\n.. image:: https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fpytorch-ignite%2Fjson&query=%24.info.version&colorB=brightgreen&prefix=v\n    :target: https://pytorch.org/ignite/index.html\n    \nIgnite is a high-level library to help with training neural networks in PyTorch.\n\n- ignite helps you write compact but full-featured training loops in a few lines of code\n- you get a training loop with metrics, early-stopping, model checkpointing and other features without the boilerplate\n\nBelow we show a side-by-side comparison of using pure pytorch and using ignite to create a training loop\nto train and validate your model with occasional checkpointing:\n\n.. image:: assets/ignite_vs_bare_pytorch.png\n    :target: https://raw.githubusercontent.com/pytorch/ignite/master/assets/ignite_vs_bare_pytorch.png\n\nAs you can see, the code is more concise and readable with ignite. Furthermore, adding additional metrics, or\nthings like early stopping is a breeze in ignite, but can start to rapidly increase the complexity of\nyour code when "rolling your own" training loop.\n\n\nInstallation\n============\n\nFrom `pip <https://pypi.org/project/pytorch-ignite/>`_:\n\n.. code:: bash\n\n    pip install pytorch-ignite\n\n\nFrom `conda <https://anaconda.org/pytorch/ignite>`_:\n\n.. code:: bash\n\n    conda install ignite -c pytorch\n\n\nFrom source:\n\n.. code:: bash\n\n    pip install git+https://github.com/pytorch/ignite\n\n\n\nNightly releases\n----------------\n\nFrom pip:\n\n.. code:: bash\n\n    pip install --pre pytorch-ignite\n\n\nFrom conda (this suggests to install `pytorch nightly release <https://anaconda.org/pytorch-nightly/pytorch>`_ instead\nof stable version as dependency):\n\n.. code:: bash\n\n    conda install ignite -c pytorch-nightly\n\n\nWhy Ignite?\n===========\nIgnite\'s high level of abstraction assumes less about the type of network (or networks) that you are training, and we require the user to define the closure to be run in the training and validation loop. This level of abstraction allows for a great deal more of flexibility, such as co-training multiple models (i.e. GANs) and computing/tracking multiple losses and metrics in your training loop.\n\nIgnite also allows for multiple handlers to be attached to events, and a finer granularity of events in the engine loop.\n\n\nDocumentation\n=============\nAPI documentation and an overview of the library can be found `here <https://pytorch.org/ignite/index.html>`_.\n\n\nStructure\n=========\n- **ignite**: Core of the library, contains an engine for training and evaluating, all of the classic machine learning metrics and a variety of handlers to ease the pain of training and validation of neural networks! \n\n- **ignite.contrib**: The Contrib directory contains additional modules contributed by Ignite users. Modules vary from TBPTT engine, various optimisation parameter schedulers, logging handlers and a metrics module containing many regression metrics (`ignite.contrib.metrics.regression <https://github.com/pytorch/ignite/tree/master/ignite/contrib/metrics/regression>`_)! \n\nThe code in **ignite.contrib** is not as fully maintained as the core part of the library. It may change or be removed at any time without notice.\n\n\nExamples\n========\n\nWe provide several examples ported from `pytorch/examples <https://github.com/pytorch/examples>`_ using `ignite`\nto display how it helps to write compact and full-featured training loops in a few lines of code:\n\nMNIST example\n--------------\n\nBasic neural network training on MNIST dataset with/without `ignite.contrib` module:\n\n- `MNIST with ignite.contrib TQDM/Tensorboard/Visdom loggers <https://github.com/pytorch/ignite/tree/master/examples/contrib/mnist>`_\n- `MNIST with native TQDM/Tensorboard/Visdom logging <https://github.com/pytorch/ignite/tree/master/examples/mnist>`_\n\nDistributed CIFAR10 example\n---------------------------\n\nTraining a small variant of ResNet on CIFAR10 in various configurations: 1) single gpu, 2) single node multiple gpus, 3) multiple nodes and multilple gpus.\n\n- `CIFAR10 <https://github.com/pytorch/ignite/tree/master/examples/contrib/cifar10>`_\n\n\nOther examples\n--------------\n\n- `DCGAN <https://github.com/pytorch/ignite/tree/master/examples/gan>`_\n- `Reinforcement Learning <https://github.com/pytorch/ignite/tree/master/examples/reinforcement_learning>`_\n- `Fast Neural Style <https://github.com/pytorch/ignite/tree/master/examples/fast_neural_style>`_\n\n\nNotebooks\n---------\n\n- `Text Classification using Convolutional Neural Networks <https://github.com/pytorch/ignite/blob/master/examples/notebooks/TextCNN.ipynb>`_\n- `Variational Auto Encoders <https://github.com/pytorch/ignite/blob/master/examples/notebooks/VAE.ipynb>`_\n- `Training Cycle-GAN on Horses to Zebras <https://github.com/pytorch/ignite/blob/master/examples/notebooks/CycleGAN.ipynb>`_\n- `Finetuning EfficientNet-B0 on CIFAR100 <https://github.com/pytorch/ignite/blob/master/examples/notebooks/EfficientNet_Cifar100_finetuning.ipynb>`_\n- `Convolutional Neural Networks for Classifying Fashion-MNIST Dataset <https://github.com/pytorch/ignite/blob/master/examples/notebooks/FashionMNIST.ipynb>`_\n- `Hyperparameters tuning with Ax <https://github.com/pytorch/ignite/blob/master/examples/notebooks/Cifar10_Ax_hyperparam_tuning.ipynb>`_\n\nContributing\n============\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us.\n\nPlease see the `contribution guidelines <https://github.com/pytorch/ignite/blob/master/CONTRIBUTING.md>`_ for more information.\n\nAs always, PRs are welcome :)\n\n\nThey use Ignite\n===============\n\n- `State-of-the-Art Conversational AI with Transfer Learning <https://github.com/huggingface/transfer-learning-conv-ai>`_\n- `Tutorial on Transfer Learning in NLP held at NAACL 2019 <https://github.com/huggingface/naacl_transfer_learning_tutorial>`_\n- `Implementation of "Attention is All You Need" paper <https://github.com/akurniawan/pytorch-transformer>`_\n- `Implementation of DropBlock: A regularization method for convolutional networks in PyTorch <https://github.com/miguelvr/dropblock>`_\n- `Deep-Reinforcement-Learning-Hands-On-Second-Edition, published by Packt <https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition>`_\n- `Kaggle Kuzushiji Recognition: 2nd place solution <https://github.com/lopuhin/kaggle-kuzushiji-2019>`_\n- `Unsupervised Data Augmentation experiments in PyTorch <https://github.com/vfdev-5/UDA-pytorch>`_\n- `Hyperparameters tuning with Optuna <https://github.com/pfnet/optuna/blob/master/examples/pytorch_ignite_simple.py>`_\n\nSee other projects at `"Used by" <https://github.com/pytorch/ignite/network/dependents?package_id=UGFja2FnZS02NzI5ODEwNA%3D%3D>`_\n\nIf your project implements a paper, represents other use-cases not covered in our official tutorials,\nKaggle competition\'s code or just your code presents interesting results and uses Ignite. We would like to add your project\nin this list, so please send a PR with brief description of the project.\n'