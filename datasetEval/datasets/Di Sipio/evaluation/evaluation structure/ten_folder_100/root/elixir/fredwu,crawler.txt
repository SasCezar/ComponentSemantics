b'# Crawler\n\n[![Travis](https://img.shields.io/travis/fredwu/crawler.svg)](https://travis-ci.org/fredwu/crawler)\n[![CodeBeat](https://codebeat.co/badges/76916047-5b66-466d-91d3-7131a269899a)](https://codebeat.co/projects/github-com-fredwu-crawler-master)\n[![Coverage](https://img.shields.io/coveralls/fredwu/crawler.svg)](https://coveralls.io/github/fredwu/crawler?branch=master)\n[![Hex.pm](https://img.shields.io/hexpm/v/crawler.svg)](https://hex.pm/packages/crawler)\n\nA high performance web crawler in Elixir, with worker pooling and rate limiting via [OPQ](https://github.com/fredwu/opq).\n\n## Features\n\n- Crawl assets (javascript, css and images).\n- Save to disk.\n- Hook for scraping content.\n- Restrict crawlable domains, paths or content types.\n- Limit concurrent crawlers.\n- Limit rate of crawling.\n- Set the maximum crawl depth.\n- Set timeouts.\n- Set retries strategy.\n- Set crawler\'s user agent.\n- Manually pause/resume/stop the crawler.\n\n## Architecture\n\nBelow is a very high level architecture diagram demonstrating how Crawler works.\n\n![](https://rawgit.com/fredwu/crawler/master/architecture.svg)\n\n## Usage\n\n```elixir\nCrawler.crawl("http://elixir-lang.org", max_depths: 2)\n```\n\nThere are several ways to access the crawled page data:\n\n1. Use [`Crawler.Store`](https://hexdocs.pm/crawler/Crawler.Store.html)\n2. Tap into the registry([?](https://hexdocs.pm/elixir/Registry.html)) [`Crawler.Store.DB`](lib/crawler/store.ex)\n3. Use your own [scraper](#custom-modules)\n4. If the `:save_to` option is set, pages will be saved to disk in addition to the above mentioned places\n5. Provide your own [custom parser](#custom-modules) and manage how data is stored and accessed yourself\n\n## Configurations\n\n| Option          | Type    | Default Value               | Description |\n|-----------------|---------|-----------------------------|-------------|\n| `:assets`       | list    | `[]`                        | Whether to fetch any asset files, available options: `"css"`, `"js"`, `"images"`.\n| `:save_to`      | string  | `nil`                       | When provided, the path for saving crawled pages.\n| `:workers`      | integer | `10`                        | Maximum number of concurrent workers for crawling.\n| `:interval`     | integer | `0`                         | Rate limit control - number of milliseconds before crawling more pages, defaults to `0` which is effectively no rate limit.\n| `:max_depths`   | integer | `3`                         | Maximum nested depth of pages to crawl.\n| `:timeout`      | integer | `5000`                      | Timeout value for fetching a page, in ms. Can also be set to `:infinity`, useful when combined with `Crawler.pause/1`.\n| `:user_agent`   | string  | `Crawler/x.x.x (...)`       | User-Agent value sent by the fetch requests.\n| `:url_filter`   | module  | `Crawler.Fetcher.UrlFilter` | Custom URL filter, useful for restricting crawlable domains, paths or content types.\n| `:retrier`      | module  | `Crawler.Fetcher.Retrier`   | Custom fetch retrier, useful for retrying failed crawls.\n| `:modifier`     | module  | `Crawler.Fetcher.Modifier`  | Custom modifier, useful for adding custom request headers or options.\n| `:scraper`      | module  | `Crawler.Scraper`           | Custom scraper, useful for scraping content as soon as the parser parses it.\n| `:parser`       | module  | `Crawler.Parser`            | Custom parser, useful for handling parsing differently or to add extra functionalities.\n| `:encode_uri`   | boolean | `false`                     | When set to `true` apply the `URI.encode` to the URL to be crawled.\n\n## Custom Modules\n\nIt is possible to swap in your custom logic as shown in the configurations section. Your custom modules need to conform to their respective behaviours:\n\n### Retrier\n\nSee [`Crawler.Fetcher.Retrier`](lib/crawler/fetcher/retrier.ex).\n\nCrawler uses [ElixirRetry](https://github.com/safwank/ElixirRetry)\'s exponential backoff strategy by default.\n\n```elixir\ndefmodule CustomRetrier do\n  @behaviour Crawler.Fetcher.Retrier.Spec\nend\n```\n\n### URL Filter\n\nSee [`Crawler.Fetcher.UrlFilter`](lib/crawler/fetcher/url_filter.ex).\n\n```elixir\ndefmodule CustomUrlFilter do\n  @behaviour Crawler.Fetcher.UrlFilter.Spec\nend\n```\n\n### Scraper\n\nSee [`Crawler.Scraper`](lib/crawler/scraper.ex).\n\n```elixir\ndefmodule CustomScraper do\n  @behaviour Crawler.Scraper.Spec\nend\n```\n\n### Parser\n\nSee [`Crawler.Parser`](lib/crawler/parser.ex).\n\n```elixir\ndefmodule CustomParser do\n  @behaviour Crawler.Parser.Spec\nend\n```\n\n### Modifier\n\nSee [`Crawler.Fetcher.Modifier`](lib/crawler/fetcher/modifier.ex).\n\n```elixir\ndefmodule CustomModifier do\n  @behaviour Crawler.Fetcher.Modifier.Spec\nend\n```\n\n## Pause / Resume / Stop Crawler\n\nCrawler provides `pause/1`, `resume/1` and `stop/1`, see below.\n\n```elixir\n{:ok, opts} = Crawler.crawl("http://elixir-lang.org")\n\nCrawler.pause(opts)\n\nCrawler.resume(opts)\n\nCrawler.stop(opts)\n```\n\nPlease note that when pausing Crawler, you would need to set a large enough `:timeout` (or even set it to `:infinity`) otherwise parser would timeout due to unprocessed links.\n\n## API Reference\n\nPlease see https://hexdocs.pm/crawler.\n\n## Changelog\n\nPlease see [CHANGELOG.md](CHANGELOG.md).\n\n## License\n\nLicensed under [MIT](http://fredwu.mit-license.org/).\n'